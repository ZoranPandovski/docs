{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","social":[{"link":"https://github.com/mindsdb/mindsdb","type":"github"},{"link":"https://twitter.com/mindsdb","type":"twitter"},{"link":"https://www.mindsdb.com","type":"link"}]},"docs":[{"location":"","text":"Getting started Installation There are a few options to install MindsDB on different operating systems. To find the one that works the best for you, check out the below links. Docker Installation Follow the Docker installation instructions. Windows Installation Follow the Windows installation instructions. Linux Installation Follow the Linux installation instructions. macOS Installation Follow the macOS installation instructions. Source Installation Follow the from source installation instructions. Tutorials AiTables in MySQL AiTables in MariaDB AiTables in PostgreSQL AiTables in ClickHouse Contribute to MindsDB Became a contributor to MindsDB Join MindsDB community Useful links MindsDB APIs documentation Get in touch Book a demo MindsDB Benchmarks TODO","title":"Install MindsDB"},{"location":"#getting-started","text":"","title":"Getting started"},{"location":"#installation","text":"There are a few options to install MindsDB on different operating systems. To find the one that works the best for you, check out the below links. Docker Installation Follow the Docker installation instructions. Windows Installation Follow the Windows installation instructions. Linux Installation Follow the Linux installation instructions. macOS Installation Follow the macOS installation instructions. Source Installation Follow the from source installation instructions.","title":"Installation"},{"location":"#tutorials","text":"AiTables in MySQL AiTables in MariaDB AiTables in PostgreSQL AiTables in ClickHouse","title":"Tutorials"},{"location":"#contribute-to-mindsdb","text":"Became a contributor to MindsDB Join MindsDB community","title":"Contribute to MindsDB"},{"location":"#useful-links","text":"MindsDB APIs documentation Get in touch Book a demo MindsDB Benchmarks TODO","title":"Useful links"},{"location":"Config/","text":"MindsDB has config variables that can be set by as environment variables or on a script. Here are some of the variables of general interest: MINDSDB_STORAGE_PATH Where MindsDB stores its data, by default it is the path where pip installs packages + mindsdb/mindsdb_storage DEFAULT_MARGIN_OF_ERROR The reverse of how much of the data you feed in mindsdb will sample in order to generate insights about the data. For example, if this is set to 0.4, mindsdb will sample 0.6 of the data. DEFAULT_LOG_LEVEL What logs mindsdb will display (By default this is set to: CONST.INFO_LOG_LEVEL ) How to set config variables? You can either set it as environment variables or you can do it in your script. import os os . environ [ '<varname>' ] = < value > For example, if you want to specify a different storage directory: import os os . environ [ 'MINDSDB_STORAGE_PATH' ] = '/home/my_wonderful_username/place_where_i_store_big_files/' # now we import mindsdb from mindsdb import MindsDB Alternatively, we can se the variable after importing mindsdb: mindsdb . CONFIG . MINDSDB_STORAGE_PATH = '/home/my_wonderful_username/place_where_i_store_big_files/' You can see all the config variables available here","title":"Configuration Settings"},{"location":"Config/#how-to-set-config-variables","text":"You can either set it as environment variables or you can do it in your script. import os os . environ [ '<varname>' ] = < value > For example, if you want to specify a different storage directory: import os os . environ [ 'MINDSDB_STORAGE_PATH' ] = '/home/my_wonderful_username/place_where_i_store_big_files/' # now we import mindsdb from mindsdb import MindsDB Alternatively, we can se the variable after importing mindsdb: mindsdb . CONFIG . MINDSDB_STORAGE_PATH = '/home/my_wonderful_username/place_where_i_store_big_files/' You can see all the config variables available here","title":"How to set config variables?"},{"location":"FunctionalInterface/","text":"This section goes into detail about each of the methods exposed by Functional and each of the arguments they work with. All of the Functional methods act as utilities for Predictors. Get Models F.get_models() Takes no argument, returns a list with all the models and some information about them. Note: this is akin to a static method, it acts the same way no matter what predictor object you call it on, but due to various consideration it hasn't been swtiched to a static method yet Get Model Data F.get_model_data(model_name='model_name') Returns all the data we have about a given model. This is a rather complex python dictionary meant to be interpreted by the Studio GUI. We recommend looking at the training logs mindsdb_native gives to see some of these insights in an easy to read format. model_name -- Required argument, the name of the model to return data about. Note: this is akin to a static method, it acts the same way no matter what predictor object you call it on, but due to various consideration it hasn't been swtiched to a static method yet Export Model F.export_predictor(model_name='model_name') Exports this predictor's data (or the data of another predictor) to a zip file inside the CONFIG.MINDSDB_STORAGE_PATH directory. model_name -- The name of the model to export (defaults to the name of the current Predictor). Rename Model F.rename_model(old_model_name='old_name', new_model_name='new_name') Renames the created model. old_model_name: the name of the model you want to rename new_model_name: the new name of the model Export Storage F.export_storage(mindsdb_storage_dir='mindsdb_storage') Exports mindsdb's storage directory to a zip file. mindsdb_storage_dir -- The location where you want to save the mindsdb storage directory. Load Model F.import_model(model_archive_path='path/to/predictor.zip') Loads a predictor that was previously exported into the current mindsdb_native storage path so you can use it later. model_archive_path -- full_path that contains your mindsdb_native predictor zip file. Delete Model F.delete_model(model_name='blah') Deletes a given predictor from the storage path mindsdb_native is currently operating with. model_name -- The name of the model to delete (defaults to the name of the current Predictor). Analyse dataset F.analyse_dataset(from_data=the_data_source, sample_settings={}) Analyse the dataset inside the data source, file, ulr or pandas dataframe. This runs all the steps prior to actually training a predictive model. from_data -- the data that you want to analyse, this can be either a file, a pandas data frame, a url or a mindsdb data source. sample_settings -- A dictionary of options for sampling from the dataset. Includes sample_for_analysis . sample_for_training , sample_margin_of_error , sample_confidence_level , sample_percentage , sample_function .","title":"Functional Interface"},{"location":"FunctionalInterface/#get-models","text":"F.get_models() Takes no argument, returns a list with all the models and some information about them. Note: this is akin to a static method, it acts the same way no matter what predictor object you call it on, but due to various consideration it hasn't been swtiched to a static method yet","title":"Get Models"},{"location":"FunctionalInterface/#get-model-data","text":"F.get_model_data(model_name='model_name') Returns all the data we have about a given model. This is a rather complex python dictionary meant to be interpreted by the Studio GUI. We recommend looking at the training logs mindsdb_native gives to see some of these insights in an easy to read format. model_name -- Required argument, the name of the model to return data about. Note: this is akin to a static method, it acts the same way no matter what predictor object you call it on, but due to various consideration it hasn't been swtiched to a static method yet","title":"Get Model Data"},{"location":"FunctionalInterface/#export-model","text":"F.export_predictor(model_name='model_name') Exports this predictor's data (or the data of another predictor) to a zip file inside the CONFIG.MINDSDB_STORAGE_PATH directory. model_name -- The name of the model to export (defaults to the name of the current Predictor).","title":"Export Model"},{"location":"FunctionalInterface/#rename-model","text":"F.rename_model(old_model_name='old_name', new_model_name='new_name') Renames the created model. old_model_name: the name of the model you want to rename new_model_name: the new name of the model","title":"Rename Model"},{"location":"FunctionalInterface/#export-storage","text":"F.export_storage(mindsdb_storage_dir='mindsdb_storage') Exports mindsdb's storage directory to a zip file. mindsdb_storage_dir -- The location where you want to save the mindsdb storage directory.","title":"Export Storage"},{"location":"FunctionalInterface/#load-model","text":"F.import_model(model_archive_path='path/to/predictor.zip') Loads a predictor that was previously exported into the current mindsdb_native storage path so you can use it later. model_archive_path -- full_path that contains your mindsdb_native predictor zip file.","title":"Load Model"},{"location":"FunctionalInterface/#delete-model","text":"F.delete_model(model_name='blah') Deletes a given predictor from the storage path mindsdb_native is currently operating with. model_name -- The name of the model to delete (defaults to the name of the current Predictor).","title":"Delete Model"},{"location":"FunctionalInterface/#analyse-dataset","text":"F.analyse_dataset(from_data=the_data_source, sample_settings={}) Analyse the dataset inside the data source, file, ulr or pandas dataframe. This runs all the steps prior to actually training a predictive model. from_data -- the data that you want to analyse, this can be either a file, a pandas data frame, a url or a mindsdb data source. sample_settings -- A dictionary of options for sampling from the dataset. Includes sample_for_analysis . sample_for_training , sample_margin_of_error , sample_confidence_level , sample_percentage , sample_function .","title":"Analyse dataset"},{"location":"GoogleColab/","text":"Using MindsDB with Google Colab Google Colab is a free cloud service that supports free GPU! You can use MindsDB there. Fortunately, this is really easy. Inside Google Colab, start a new python 3 notebook and in a cell, insert the following: !pip install mindsdb Let's Build an Example First we'll import mindsdb from mindsdb import Predictor This is where it gets interesting. It's now up to you to install any dataset you want, so long as its a Excel or CSV file (or some other from of separator, doesn't necessarily have to be a \",\"). We'll be linking it to colab next. In this example we'll be using a students dataset from kaggle. You can get it here if you want to follow along. Once you have your CSV dataset, download it and put it in a new folder on your Google Drive. We'll call ours Datasets . We'll import it into colab using the following lines from google.colab import drive drive . mount ( '/content/drive' ) Now just follow the instructions and enter your authorization code. Here, we'll create a file variable that stores the path of our dataset. file = \"./drive/My Drive/Datasets/StudentsPerformance.csv\" Training Now let's create a MindsDB object and initialize it with our data from the file. We'll be prediciting the reading_score and we'll call our model 'reading_predictor'. Remember that depending on your dataset, these variables might change. Just remember that predict is the column you want to make your prediction on and that mindsdb will automatically rename all your columns to snake case. mdb = Predictor ( name = 'reading_score_predictor' ) mdb . learn ( from_data = file , # call file from google drive to_predict = 'reading_score' ) Testing mdb.predict needs 1 of 2 arguments to run a prediction: when_data is a file with one or more values or a dictionary of values for the columns we want to use for the prediction. The following example uses a dictionary via the when_data argument: # Load the `Predictor` we just trained via calling `learn` mdb = Predictor ( name = 'reading_score_predictor' ) # Make a prediction using a dictionary of input values predictions = mdb . predict ( when_data = { 'writing_score' : 80 , 'math_score' : 40 , 'lunch' : 'standard' } ) Finally we print out the result: # The dictionary containing the prediction print ( predictions ) # The confidence we have in the prediction (`0` being the lowest confidence and `1` being 100% confident) # Note, the confidence is not equal to the model's overall accuracy print ( predictions [ 'reading_score_confidence' ]) # The actual value predicted for `reading_score` print ( predictions [ 'reading_score' ]) You can find our Google Colab Example here.","title":"Running on Google Colab"},{"location":"GoogleColab/#using-mindsdb-with-google-colab","text":"Google Colab is a free cloud service that supports free GPU! You can use MindsDB there. Fortunately, this is really easy. Inside Google Colab, start a new python 3 notebook and in a cell, insert the following: !pip install mindsdb","title":"Using MindsDB with Google Colab"},{"location":"GoogleColab/#lets-build-an-example","text":"First we'll import mindsdb from mindsdb import Predictor This is where it gets interesting. It's now up to you to install any dataset you want, so long as its a Excel or CSV file (or some other from of separator, doesn't necessarily have to be a \",\"). We'll be linking it to colab next. In this example we'll be using a students dataset from kaggle. You can get it here if you want to follow along. Once you have your CSV dataset, download it and put it in a new folder on your Google Drive. We'll call ours Datasets . We'll import it into colab using the following lines from google.colab import drive drive . mount ( '/content/drive' ) Now just follow the instructions and enter your authorization code. Here, we'll create a file variable that stores the path of our dataset. file = \"./drive/My Drive/Datasets/StudentsPerformance.csv\"","title":"Let's Build an Example"},{"location":"GoogleColab/#training","text":"Now let's create a MindsDB object and initialize it with our data from the file. We'll be prediciting the reading_score and we'll call our model 'reading_predictor'. Remember that depending on your dataset, these variables might change. Just remember that predict is the column you want to make your prediction on and that mindsdb will automatically rename all your columns to snake case. mdb = Predictor ( name = 'reading_score_predictor' ) mdb . learn ( from_data = file , # call file from google drive to_predict = 'reading_score' )","title":"Training"},{"location":"GoogleColab/#testing","text":"mdb.predict needs 1 of 2 arguments to run a prediction: when_data is a file with one or more values or a dictionary of values for the columns we want to use for the prediction. The following example uses a dictionary via the when_data argument: # Load the `Predictor` we just trained via calling `learn` mdb = Predictor ( name = 'reading_score_predictor' ) # Make a prediction using a dictionary of input values predictions = mdb . predict ( when_data = { 'writing_score' : 80 , 'math_score' : 40 , 'lunch' : 'standard' } ) Finally we print out the result: # The dictionary containing the prediction print ( predictions ) # The confidence we have in the prediction (`0` being the lowest confidence and `1` being 100% confident) # Note, the confidence is not equal to the model's overall accuracy print ( predictions [ 'reading_score_confidence' ]) # The actual value predicted for `reading_score` print ( predictions [ 'reading_score' ]) You can find our Google Colab Example here.","title":"Testing"},{"location":"InsideMindsDB/","text":"Different transactions PREDICT, CREATE MODEL etc, require different steps/phases, however they may share some of these phases, in order to make this process modular we keep the variables in the Transaction controller (the data bus) as the communication interface, as such, the implementation of a given phase can change, so long as the expected variables in the bus prevail. (We will describe in more detail some of the Phase Modules in the next section). The MindsDB Stack DataExtractor It deals with extracting inputs from various data-sources such as files, directories and SQL compatible databases. If input is a query, it builds the joins with all implied tables (if any). StatsLoader : There is some transaction such as PREDICT where it's assumed that the statistical information is already known, all we have to do is make sure we load the right statistics to the transaction BUS. At the moment we don't support loading database from {char}svs that don't have headers or have incomplete headers. NOTE : That as of now mindsDB requires that the full dataset can be loaded into memory, in the future we might look into supporting very large datasets using something like apache drill to query a FS or db for the chunks of data we need in order to train and generate our statistical analysis . StatsGenerator Once the data is pulled and aggregated from the various data sources, MindsDB runs an analysis of each of the columns of the corpus. The purpose of the stats generator is two fold: To provide various data quality scores in order to determine the overall quality of a column (e.g. variance, some correlation metrics between columns, amount of duplicates). To provide properties about the columns which have to be used in the following steps and in order to rain the model. (e.g. histogram, data type) After all stats are computed, we warn the user of any interesting insights we found about his data and (if web logs are enabled), use the generated values to plot some interesting information about the data (e.g. data type distribution, outliers, histogram). Finally, the various stats are passed on as part of the metadata, so that further phases and the model itself can use them. Model Interface Train mode : When calling learn ,the model interface will feed the data to a machine learning framework which does the training in order to build a model. Predict mode : When calling predict , the model interface will feed the data to the model built by learn in order to generate a prediction. Data adaption : The ModelInterface phase is simply a lightweight wrapper over the model backends which handle adapting the data frame used by mindsdb into a format they can work with. During this process additional metadata for the machine learning libraries/frameworks is generated based on the results of the Stats Generator phase. Learning backend : The learning backends as the ensemble learning libraries used by mindsdb to train the model that will generate the predictions. Currently the learning backend we are working on supporting is Lightwood (created by us, based on the pre 1.0 version of mindsdb, work in progress). ModelAnalyzer The model analyzer phase runs after training is done in order to gather insights about the model and gather insights about the data that we can only get post-training. At the moment, it contains the fitting for a probabilistic model which is used to determine the accuracy of future prediction, based on the number of missing features and the bucket in which the predicted value falls.","title":"Inside MindsDB"},{"location":"InsideMindsDB/#the-mindsdb-stack","text":"","title":"The MindsDB Stack"},{"location":"InsideMindsDB/#dataextractor","text":"It deals with extracting inputs from various data-sources such as files, directories and SQL compatible databases. If input is a query, it builds the joins with all implied tables (if any). StatsLoader : There is some transaction such as PREDICT where it's assumed that the statistical information is already known, all we have to do is make sure we load the right statistics to the transaction BUS. At the moment we don't support loading database from {char}svs that don't have headers or have incomplete headers. NOTE : That as of now mindsDB requires that the full dataset can be loaded into memory, in the future we might look into supporting very large datasets using something like apache drill to query a FS or db for the chunks of data we need in order to train and generate our statistical analysis .","title":"DataExtractor"},{"location":"InsideMindsDB/#statsgenerator","text":"Once the data is pulled and aggregated from the various data sources, MindsDB runs an analysis of each of the columns of the corpus. The purpose of the stats generator is two fold: To provide various data quality scores in order to determine the overall quality of a column (e.g. variance, some correlation metrics between columns, amount of duplicates). To provide properties about the columns which have to be used in the following steps and in order to rain the model. (e.g. histogram, data type) After all stats are computed, we warn the user of any interesting insights we found about his data and (if web logs are enabled), use the generated values to plot some interesting information about the data (e.g. data type distribution, outliers, histogram). Finally, the various stats are passed on as part of the metadata, so that further phases and the model itself can use them.","title":"StatsGenerator"},{"location":"InsideMindsDB/#model-interface","text":"Train mode : When calling learn ,the model interface will feed the data to a machine learning framework which does the training in order to build a model. Predict mode : When calling predict , the model interface will feed the data to the model built by learn in order to generate a prediction. Data adaption : The ModelInterface phase is simply a lightweight wrapper over the model backends which handle adapting the data frame used by mindsdb into a format they can work with. During this process additional metadata for the machine learning libraries/frameworks is generated based on the results of the Stats Generator phase. Learning backend : The learning backends as the ensemble learning libraries used by mindsdb to train the model that will generate the predictions. Currently the learning backend we are working on supporting is Lightwood (created by us, based on the pre 1.0 version of mindsdb, work in progress).","title":"Model Interface"},{"location":"InsideMindsDB/#modelanalyzer","text":"The model analyzer phase runs after training is done in order to gather insights about the model and gather insights about the data that we can only get post-training. At the moment, it contains the fitting for a probabilistic model which is used to determine the accuracy of future prediction, based on the number of missing features and the bucket in which the predicted value falls.","title":"ModelAnalyzer"},{"location":"JupyterNotebook/","text":"Create a notebook Click on the Link to Try it in your browser with Classic Notebook : You\u2019ll see a screen load with \u2018Binder\u2019 at the top. This should resolve to a screen, with a file menu near the top. On the far left to the file menu, select file, then drag down \u2018New Notebook\u2019 and from there select \u2018Python 3\u2019. You will then see Python command line Installing mindsdb and running In the command line type: !pip install git+https://github.com/mindsdb/mindsdb.git@master --user --no-cache-dir --upgrade --force-reinstall; then press the Run button in the top bar and wait for the install to finish. Now we can run one of our mindsdb examples, first by training a model: import mindsdb # Instantiate a mindsdb Predictor mdb = mindsdb . Predictor ( name = 'real_estate_model' ) # We tell the Predictor what column or key we want to learn and from what data mdb . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , # the path to the file where we can learn from, (note: can be url) to_predict = 'rental_price' , # the column we want to learn to predict given all the data in the file ) Then generating some predictions: mdb = mindsdb . Predictor ( name = 'real_estate_model' ) # use the model to make predictions result = mdb . predict ( when_data = { 'number_of_rooms' : 2 , 'number_of_bathrooms' : 1 , 'sqft' : 1190 }) # The result will be an array containing predictions for each data point (in this case only one), a confidence for said prediction and a few other extra informations print ( 'The predicted price is $ {price} with {conf} confidence' . format ( price = result [ 0 ][ 'rental_price' ], conf = result [ 0 ][ 'rental_price_confidence' ]))","title":"Running on Jupyter Notebook"},{"location":"JupyterNotebook/#create-a-notebook","text":"Click on the Link to Try it in your browser with Classic Notebook : You\u2019ll see a screen load with \u2018Binder\u2019 at the top. This should resolve to a screen, with a file menu near the top. On the far left to the file menu, select file, then drag down \u2018New Notebook\u2019 and from there select \u2018Python 3\u2019. You will then see Python command line","title":"Create a notebook"},{"location":"JupyterNotebook/#installing-mindsdb-and-running","text":"In the command line type: !pip install git+https://github.com/mindsdb/mindsdb.git@master --user --no-cache-dir --upgrade --force-reinstall; then press the Run button in the top bar and wait for the install to finish. Now we can run one of our mindsdb examples, first by training a model: import mindsdb # Instantiate a mindsdb Predictor mdb = mindsdb . Predictor ( name = 'real_estate_model' ) # We tell the Predictor what column or key we want to learn and from what data mdb . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , # the path to the file where we can learn from, (note: can be url) to_predict = 'rental_price' , # the column we want to learn to predict given all the data in the file ) Then generating some predictions: mdb = mindsdb . Predictor ( name = 'real_estate_model' ) # use the model to make predictions result = mdb . predict ( when_data = { 'number_of_rooms' : 2 , 'number_of_bathrooms' : 1 , 'sqft' : 1190 }) # The result will be an array containing predictions for each data point (in this case only one), a confidence for said prediction and a few other extra informations print ( 'The predicted price is $ {price} with {conf} confidence' . format ( price = result [ 0 ][ 'rental_price' ], conf = result [ 0 ][ 'rental_price_confidence' ]))","title":"Installing mindsdb and running"},{"location":"PredictorInterface/","text":"This section goes into detail about each of the methods exposed by Predictor and each of the arguments they work with. Predictor Note: The Predictor in MindsDB's words means Machine Learning Model. Constructor predictor = Predictor(name='weather_forecast') Constructs a new mindsdb predictor name -- Required argument, the name of the predictor, used for saving the predictor, creating a new predictor with the same name as a previous one loads the data from the old predictor root_folder -- The directory (also known as folder) where the predictor information should be saved log_level -- The log level that the predictor should use, number from 0 to 50, with 0 meaning log everything and 50 meaning log only fatal errors: DEBUG_LOG_LEVEL = 10 INFO_LOG_LEVEL = 20 WARNING_LOG_LEVEL = 30 ERROR_LOG_LEVEL = 40 NO_LOGS_LOG_LEVEL = 50 Learn predictor.learn(from_data=a_data_source, to_predict='a_column') Teach the predictor to make predictions on a given dataset, extract information about the dataset and extract information about the resulting machine learning model making the predictions. This is the \"main\" functionality of mindsdb_native together with the \"predict\" method. from_data -- the data that you want to use for training, this can be either a file, a pandas data frame, a url or a mindsdb data source. to_predict -- The columns/keys to be predicted (aka the targets, output columns, target variables), can be either a string (when specifying a single column) or a list of strings (when specifying multiple columns). test_from_data -- Specify a different data source on which mindsdb should test the machine learning model, by default mindsdb_native takes testing and validation samples from your dataset to use during training and analysis, and only trains the predictive model on ~80% of the data. This might seem sub-optimal if you're not used to machine learning, but trust us, it allows us to have much better confidence in the model we give you. timeseries specific parameters : group_by, window_size, order_by -- For more information on how to use these, please see the advanced examples section dealing with timeseries . Please note, these are currently subject to change, though they will remain backwards compatible until v2.0. ignore_columns -- Ignore certain columns from your data entirely. stop_training_in_x_seconds -- Stop training the model after this amount of seconds, note, the full amount it takes for mindsdb_native to run might be up to twice the amount you specify in this value. Thought, usually, the model training constitutes ~70-90% of the total mindsdb_native runtime. stop_training_in_accuracy -- Deprecated argument, left for backwards compatibility, to be removed or revamped in v2.0, refrain from using, it has no effects. backend -- The machine learning backend to use in order to train the model. This can be a string equal to lightwood (default) or ludwig , this can also be a custom model object. rebuild_model -- Defaults to True , if this is set to False the model will be loaded and the model analysis and data analysis will be re-run, but a new model won't be trained. use_gpu -- Defaults to None (autodetect), set to True if you have a GPU and want to make sure it's used or to False if you want to train the model on the CPU, this will speed up model training a lot in most situations. Note, that the default learning backend (lightwood) only work with relatively new (2016+) GPUs. equal_accuracy_for_all_output_categories -- When you have unbalanced target variable values, this will treat all of them as equally important when training the model. To see more information about this and an example, please see advanced section . output_categories_importance_dictionary -- A dictionary containing a number representing the importance for each (or some) values from the column to be predicted. An example of how his can be used (assume the column we are predicting is called is_true and takes two falues): {'is_true': {'True':0.6, 'False':1}} . The bigger the number (maximum value is one), the more important will it be for the model to predict that specific value correctly (usually at the cost of predicting other values correctly and getting more false positives for that value). advanced_args -- A dictionary of advanced arguments. Includes force_disable_cache , force_categorical_encoding , handle_foreign_keys , use_selfaware_model , deduplicate_data . sample_settings -- A dictionary of options for sampling from the dataset. Includes sample_for_analysis . sample_for_training , sample_margin_of_error , sample_confidence_level , sample_percentage , sample_function . If you are interested in using advanced_args or sample_settings but you are unsure of how they work, please shot us an email or create a github issue and we will help you. Predict predict(self, when_data = None, update_cached_model = False, use_gpu=False, advanced_args={}, backend=None, run_confidence_variation_analysis=False): predictor.predict(from_data=the_data_source) Make a prediction about a given dataset. when_data -- the data that you want to make the predictions for, this can be either a file, a pandas data frame, a url, a dictionary used for single prediction(column name: value) or a mindsdb data source. update_cached_model -- Deprecated argument, left for backwards compatibility, to be removed or revamped in v2.0, refrain from using, it has no effects. use_gpu -- Defaults to None (autodetect), set to True if you have a GPU and want to make sure it's used or to False if you want to train the model on the CPU, this will speed up model training a lot in most situations. Note, that the default learning backend (lightwood) only work with relatively new (2016+) GPUs. advanced_args -- A dictionary of advanced arguments. Includes force_disable_cache . backend -- The machine learning backend to use in order to train the model. This can be a string equal to lightwood (default) or ludwig , this can also be a custom model object. Note, you shouldn't use a different backend than the one you used to train the model, this will result in undefined behavior in the worst case scenario and most likely lead to a weired error. This defaults to whatever backend was last used when calling learn on this predictor. run_confidence_variation_analysis -- Run a confidence variation analysis on each of the given input column, currently only works when making single predictions via when_data . It provides some more in-depth analysis of a given prediction, by specifying how the confidence would increase/decrease based on which of the columns in the prediction were not present (had null, None or empty values). Test predictor.test(when_data=data, accuracy_score_functions='r2_score', score_using='predicted_value', predict_args={'use_gpu': True}): Test the overall confidence of the predictor e.g {'rental_price_accuracy': 0.95}. when_data -- Use this when you have data in either a file, a pandas data frame, or url to a file that you want to predict from. accuracy_score_functions -- A single function or a dictionary for the form {f'{target_name}': acc_func} when multiple targets are used. score_using -- What values from the explanation of the target to be used in the score function. predict_args -- Dictionary of arguments to be passed to predict (same arguments that predict accepts), e.g: predict_args={'use_gpu': True} . Predictor Quality Prediction Quality DataSources Mindsdb exposes a number of data sources that can be used with the predictor, you can find more details in the datasources section . Constants and Configuration For the constants and configuration options exposed by mindsdb_native at large please refer to this section .","title":"Predictor Interface"},{"location":"PredictorInterface/#predictor","text":"Note: The Predictor in MindsDB's words means Machine Learning Model.","title":"Predictor"},{"location":"PredictorInterface/#constructor","text":"predictor = Predictor(name='weather_forecast') Constructs a new mindsdb predictor name -- Required argument, the name of the predictor, used for saving the predictor, creating a new predictor with the same name as a previous one loads the data from the old predictor root_folder -- The directory (also known as folder) where the predictor information should be saved log_level -- The log level that the predictor should use, number from 0 to 50, with 0 meaning log everything and 50 meaning log only fatal errors: DEBUG_LOG_LEVEL = 10 INFO_LOG_LEVEL = 20 WARNING_LOG_LEVEL = 30 ERROR_LOG_LEVEL = 40 NO_LOGS_LOG_LEVEL = 50","title":"Constructor"},{"location":"PredictorInterface/#learn","text":"predictor.learn(from_data=a_data_source, to_predict='a_column') Teach the predictor to make predictions on a given dataset, extract information about the dataset and extract information about the resulting machine learning model making the predictions. This is the \"main\" functionality of mindsdb_native together with the \"predict\" method. from_data -- the data that you want to use for training, this can be either a file, a pandas data frame, a url or a mindsdb data source. to_predict -- The columns/keys to be predicted (aka the targets, output columns, target variables), can be either a string (when specifying a single column) or a list of strings (when specifying multiple columns). test_from_data -- Specify a different data source on which mindsdb should test the machine learning model, by default mindsdb_native takes testing and validation samples from your dataset to use during training and analysis, and only trains the predictive model on ~80% of the data. This might seem sub-optimal if you're not used to machine learning, but trust us, it allows us to have much better confidence in the model we give you. timeseries specific parameters : group_by, window_size, order_by -- For more information on how to use these, please see the advanced examples section dealing with timeseries . Please note, these are currently subject to change, though they will remain backwards compatible until v2.0. ignore_columns -- Ignore certain columns from your data entirely. stop_training_in_x_seconds -- Stop training the model after this amount of seconds, note, the full amount it takes for mindsdb_native to run might be up to twice the amount you specify in this value. Thought, usually, the model training constitutes ~70-90% of the total mindsdb_native runtime. stop_training_in_accuracy -- Deprecated argument, left for backwards compatibility, to be removed or revamped in v2.0, refrain from using, it has no effects. backend -- The machine learning backend to use in order to train the model. This can be a string equal to lightwood (default) or ludwig , this can also be a custom model object. rebuild_model -- Defaults to True , if this is set to False the model will be loaded and the model analysis and data analysis will be re-run, but a new model won't be trained. use_gpu -- Defaults to None (autodetect), set to True if you have a GPU and want to make sure it's used or to False if you want to train the model on the CPU, this will speed up model training a lot in most situations. Note, that the default learning backend (lightwood) only work with relatively new (2016+) GPUs. equal_accuracy_for_all_output_categories -- When you have unbalanced target variable values, this will treat all of them as equally important when training the model. To see more information about this and an example, please see advanced section . output_categories_importance_dictionary -- A dictionary containing a number representing the importance for each (or some) values from the column to be predicted. An example of how his can be used (assume the column we are predicting is called is_true and takes two falues): {'is_true': {'True':0.6, 'False':1}} . The bigger the number (maximum value is one), the more important will it be for the model to predict that specific value correctly (usually at the cost of predicting other values correctly and getting more false positives for that value). advanced_args -- A dictionary of advanced arguments. Includes force_disable_cache , force_categorical_encoding , handle_foreign_keys , use_selfaware_model , deduplicate_data . sample_settings -- A dictionary of options for sampling from the dataset. Includes sample_for_analysis . sample_for_training , sample_margin_of_error , sample_confidence_level , sample_percentage , sample_function . If you are interested in using advanced_args or sample_settings but you are unsure of how they work, please shot us an email or create a github issue and we will help you.","title":"Learn"},{"location":"PredictorInterface/#predict","text":"predict(self, when_data = None, update_cached_model = False, use_gpu=False, advanced_args={}, backend=None, run_confidence_variation_analysis=False): predictor.predict(from_data=the_data_source) Make a prediction about a given dataset. when_data -- the data that you want to make the predictions for, this can be either a file, a pandas data frame, a url, a dictionary used for single prediction(column name: value) or a mindsdb data source. update_cached_model -- Deprecated argument, left for backwards compatibility, to be removed or revamped in v2.0, refrain from using, it has no effects. use_gpu -- Defaults to None (autodetect), set to True if you have a GPU and want to make sure it's used or to False if you want to train the model on the CPU, this will speed up model training a lot in most situations. Note, that the default learning backend (lightwood) only work with relatively new (2016+) GPUs. advanced_args -- A dictionary of advanced arguments. Includes force_disable_cache . backend -- The machine learning backend to use in order to train the model. This can be a string equal to lightwood (default) or ludwig , this can also be a custom model object. Note, you shouldn't use a different backend than the one you used to train the model, this will result in undefined behavior in the worst case scenario and most likely lead to a weired error. This defaults to whatever backend was last used when calling learn on this predictor. run_confidence_variation_analysis -- Run a confidence variation analysis on each of the given input column, currently only works when making single predictions via when_data . It provides some more in-depth analysis of a given prediction, by specifying how the confidence would increase/decrease based on which of the columns in the prediction were not present (had null, None or empty values).","title":"Predict"},{"location":"PredictorInterface/#test","text":"predictor.test(when_data=data, accuracy_score_functions='r2_score', score_using='predicted_value', predict_args={'use_gpu': True}): Test the overall confidence of the predictor e.g {'rental_price_accuracy': 0.95}. when_data -- Use this when you have data in either a file, a pandas data frame, or url to a file that you want to predict from. accuracy_score_functions -- A single function or a dictionary for the form {f'{target_name}': acc_func} when multiple targets are used. score_using -- What values from the explanation of the target to be used in the score function. predict_args -- Dictionary of arguments to be passed to predict (same arguments that predict accepts), e.g: predict_args={'use_gpu': True} .","title":"Test"},{"location":"PredictorInterface/#predictor-quality","text":"","title":"Predictor Quality"},{"location":"PredictorInterface/#prediction-quality","text":"","title":"Prediction Quality"},{"location":"PredictorInterface/#datasources","text":"Mindsdb exposes a number of data sources that can be used with the predictor, you can find more details in the datasources section .","title":"DataSources"},{"location":"PredictorInterface/#constants-and-configuration","text":"For the constants and configuration options exposed by mindsdb_native at large please refer to this section .","title":"Constants and Configuration"},{"location":"community/","text":"Join our community If you have additional questions or you want to chat with MindsDB core team, you can join our MindsDB community forum . MindsDB newsletter To get updates on MindsDB\u2019s latest announcements, releases, and events, sign up for our newsletter . Become a MindsDB Beta tester If you want to become a part of our product and get first access to all of our latest updates, join our Beta testers community . Talk to our engineers If you want to use MindsDB or you have more questions you can book a demo with one of our engineers. Get in touch for collaboration Contact us by sumbiting this form .","title":"Join our community"},{"location":"community/#join-our-community","text":"If you have additional questions or you want to chat with MindsDB core team, you can join our MindsDB community forum .","title":"Join our community"},{"location":"community/#mindsdb-newsletter","text":"To get updates on MindsDB\u2019s latest announcements, releases, and events, sign up for our newsletter .","title":"MindsDB newsletter"},{"location":"community/#become-a-mindsdb-beta-tester","text":"If you want to become a part of our product and get first access to all of our latest updates, join our Beta testers community .","title":"Become a MindsDB Beta tester"},{"location":"community/#talk-to-our-engineers","text":"If you want to use MindsDB or you have more questions you can book a demo with one of our engineers.","title":"Talk to our engineers"},{"location":"community/#get-in-touch-for-collaboration","text":"Contact us by sumbiting this form .","title":"Get in touch for collaboration"},{"location":"contribute/","text":"Contribute to MindsDB Thank you for your interest to contribute to MindsDB. MindsDB is free open source software and all type of contributions are welcome, whether they\u2019re documentation changes, bug reports, bug fixes or new source code changes. Report issue We use GitHub issues to track bugs and features. Report them by opening a new issue and fill out all of the required inputs as Your Python version , MindsDB version , Describe the bug , Steps to reproduce . Documentation We always try to improve our documentation. All of the Pull Requests that improve our grammar, docs structure or fix typo are welcomed. Check MindsDB Docs repository and help us. Easy for contribution issue Most of the issues that are open for contributions will be tagged with good-first-issue or help-wanted . After you find the issue that you want to contribute to, follow the fork-and-pull workflow: Fork the Mindsdb repository Clone the repository locally Make changes and commit them Push your local branch to your fork Submit a Pull request so that we can review your changes Write a commit message Make sure that the CI tests are GREEN NOTE: Be sure to merge the latest from \"upstream\" before making a pull request! The Pull Request reviews are done on a regular basis. Please, make sure you respond to our feedback/questions and sign our CLA. Write for us You find MindsDB usefull and you want to share your story? Make a PR to this repo with your writing in markdown file or just post it on Medium, Dev or your own blog post. We would love to hear that .","title":"How to contribute"},{"location":"contribute/#contribute-to-mindsdb","text":"Thank you for your interest to contribute to MindsDB. MindsDB is free open source software and all type of contributions are welcome, whether they\u2019re documentation changes, bug reports, bug fixes or new source code changes.","title":"Contribute to MindsDB"},{"location":"contribute/#report-issue","text":"We use GitHub issues to track bugs and features. Report them by opening a new issue and fill out all of the required inputs as Your Python version , MindsDB version , Describe the bug , Steps to reproduce .","title":"Report issue"},{"location":"contribute/#documentation","text":"We always try to improve our documentation. All of the Pull Requests that improve our grammar, docs structure or fix typo are welcomed. Check MindsDB Docs repository and help us.","title":"Documentation"},{"location":"contribute/#easy-for-contribution-issue","text":"Most of the issues that are open for contributions will be tagged with good-first-issue or help-wanted . After you find the issue that you want to contribute to, follow the fork-and-pull workflow: Fork the Mindsdb repository Clone the repository locally Make changes and commit them Push your local branch to your fork Submit a Pull request so that we can review your changes Write a commit message Make sure that the CI tests are GREEN NOTE: Be sure to merge the latest from \"upstream\" before making a pull request! The Pull Request reviews are done on a regular basis. Please, make sure you respond to our feedback/questions and sign our CLA.","title":"Easy for contribution issue"},{"location":"contribute/#write-for-us","text":"You find MindsDB usefull and you want to share your story? Make a PR to this repo with your writing in markdown file or just post it on Medium, Dev or your own blog post. We would love to hear that .","title":"Write for us"},{"location":"faq/","text":"Why MindsDB? We are building MindsDB, because we want to Democratize Machine Learning . If you want to learn more about why this is important and how we aim to do this, you can check out our presentation here . Who are we building MindsDB for? We are building MindsDB for all of those that can get their hands in data, and can type a few lines of code. What are MindsDB's current Goals? MindsDB has 3 simple goals. Provide a way to learn and predict from data with a line of code. Explainable, by answering the following questions: When learning: What is interesting in my data and why? When can I trust this model and why? When I should not trust this model and why? How can I improve this model? When predicting: Why this prediction? Why not something else? Remain state of the art. Since MindsDB users are delegating the ML machinery, MindsDB should try to always generate state of the art models for the users. What is the roadmap? MindsDB roadmap is aimed to be aligned with our goals: Success versions 1.0 MindsDB GUI where you can visualize explainability goals Support for images and complex text MindsDB REST API's Success versions 2.0 Run Machine Learning Models as Tables Important versions 3.0 Auto ETL and Data Preparation What type of data can MindsDB learn and predict from? We support tabular data, this is CSV, Excel, JSON, text files, pandas data frame, URLs, s3 files or MySQL, Mongo, ClickHouse, PostgreSQL data stores. For more information please see the data sources documentation . How does it work? In very simple terms, MindsDB follows the following steps: to learn : break data source intro train, test, validate transform data source into tensors build and train encoders (if necessary) produce a neural network based model that can take in the input tensor and produce the target tensor break train data into batches and try learning a model that can fit the target test and validate until model convergence store metadata about the most fit model to predict : transform question data into input tensor load most fit model run input tensor into model transform output tensor into readable output You can learn more about the internals of mindsdb here . How can I help? You can help in the following ways: Trying MindsDB and reporting issues . If you know python, you can also help us debug open issues. Issues labels with the good first issue tag should be the easiest to start with . You can help us with documentation and examples. Tell your friends, write a blog post about MindsDB. Join our team, we are growing fast so we should have a few open positions . Why is it called MindsDB? Well, as most names, we needed one, we like science fiction and the culture series , where there are these AI super smart entities called Minds. How about the DB part?. Although in the future we will support all kinds of data, currently our objective is to add intelligence to existing data stores/databases, hence the term DB. As to becoming a Mind to your DB . Why the bear? Who doesn't like bears! Anyway, a bear for UC Berkeley where this all was initially coded. What is the difference between AI and Machine Learning? What is XAI?","title":"FAQ"},{"location":"faq/#why-mindsdb","text":"We are building MindsDB, because we want to Democratize Machine Learning . If you want to learn more about why this is important and how we aim to do this, you can check out our presentation here .","title":"Why MindsDB?"},{"location":"faq/#who-are-we-building-mindsdb-for","text":"We are building MindsDB for all of those that can get their hands in data, and can type a few lines of code.","title":"Who are we building MindsDB for?"},{"location":"faq/#what-are-mindsdbs-current-goals","text":"MindsDB has 3 simple goals. Provide a way to learn and predict from data with a line of code. Explainable, by answering the following questions: When learning: What is interesting in my data and why? When can I trust this model and why? When I should not trust this model and why? How can I improve this model? When predicting: Why this prediction? Why not something else? Remain state of the art. Since MindsDB users are delegating the ML machinery, MindsDB should try to always generate state of the art models for the users.","title":"What are MindsDB's current Goals?"},{"location":"faq/#what-is-the-roadmap","text":"MindsDB roadmap is aimed to be aligned with our goals: Success versions 1.0 MindsDB GUI where you can visualize explainability goals Support for images and complex text MindsDB REST API's Success versions 2.0 Run Machine Learning Models as Tables Important versions 3.0 Auto ETL and Data Preparation","title":"What is the roadmap?"},{"location":"faq/#what-type-of-data-can-mindsdb-learn-and-predict-from","text":"We support tabular data, this is CSV, Excel, JSON, text files, pandas data frame, URLs, s3 files or MySQL, Mongo, ClickHouse, PostgreSQL data stores. For more information please see the data sources documentation .","title":"What type of data can MindsDB learn and predict from?"},{"location":"faq/#how-does-it-work","text":"In very simple terms, MindsDB follows the following steps: to learn : break data source intro train, test, validate transform data source into tensors build and train encoders (if necessary) produce a neural network based model that can take in the input tensor and produce the target tensor break train data into batches and try learning a model that can fit the target test and validate until model convergence store metadata about the most fit model to predict : transform question data into input tensor load most fit model run input tensor into model transform output tensor into readable output You can learn more about the internals of mindsdb here .","title":"How does it work?"},{"location":"faq/#how-can-i-help","text":"You can help in the following ways: Trying MindsDB and reporting issues . If you know python, you can also help us debug open issues. Issues labels with the good first issue tag should be the easiest to start with . You can help us with documentation and examples. Tell your friends, write a blog post about MindsDB. Join our team, we are growing fast so we should have a few open positions .","title":"How can I help?"},{"location":"faq/#why-is-it-called-mindsdb","text":"Well, as most names, we needed one, we like science fiction and the culture series , where there are these AI super smart entities called Minds. How about the DB part?. Although in the future we will support all kinds of data, currently our objective is to add intelligence to existing data stores/databases, hence the term DB. As to becoming a Mind to your DB . Why the bear? Who doesn't like bears! Anyway, a bear for UC Berkeley where this all was initially coded.","title":"Why is it called MindsDB?"},{"location":"faq/#what-is-the-difference-between-ai-and-machine-learning","text":"","title":"What is the difference between AI and Machine Learning?"},{"location":"faq/#what-is-xai","text":"","title":"What is XAI?"},{"location":"comparisons/ComparsionWithOtherLibraries/","text":"Let's compared Mindsdb with some popular deep learning and machine learning libraries to show what makes it different. Models With libraries such as Tensorflow, Sklearn, Pytorch you must have the expertise to build models from scratch. Your models are also black boxes, you can't be sure how or why they work and you have to pre-process your data in a format that's suitable for the model and look for any errors in the data yourself. With Mindsdb anyone can build state of the art models without any machine learning knowledge. Mindsdb also provides data extraction, analyses your input data and analyses the resulting model to try and understand what makes it work and what types of situations it works best in. Data Preprocessing Building models require time for cleaning the data, normalizing the data, converting it into the format your library uses, determining the type of data in each column and a proper encoding for it Mindsdb can read data from csv, json, excel, file urls, s3 objects , dataframes and relational database tables or queries (currently there's native support for maraiadb, mysql and postgres), you just need to tell it which colum(s) it should predict. It will automatically process the data for you and give insights about the data. Code Samples We are going to use home_rentals.csv dataset for comparison purpose. Inside the dataset dir, you can find the dataset split into train and test data. Our goal is to predict the rental_price of the house given the information we have in home_rental.csv . We will look at doing this with Sklearn, Tensorflow, Ludwig and Mindsdb. Sklearn is a generic easy-to-use machine learning library. Tensorflow is the state of the art deep learning model building library from google. Ludwig is a library from Uber that aims to help people build machine learning models without knowledge of machine learning (similar to mindsdb) Building the model Now we will build the actual models to train on the training dataset and run some predictions on the testing dataset. For the purpose of this example, we'll build a simple linear regression with both Tensorflow and Sklearn, in order to keep the code to a minimum. Tensorflow import tensorflow as tf import pandas as pd import numpy as np from tensorflow import keras from tensorflow.keras import layers from sklearn.preprocessing import MinMaxScaler # process data df = pd . read_csv ( \"home_rentals.csv\" ) labels = df . pop ( 'rental_price' ) . values . reshape ( - 1 , 1 ) features = df . _get_numeric_data () . values xscaler = MinMaxScaler () features = xscaler . fit_transform ( features ) yscaler = MinMaxScaler () labels = yscaler . fit_transform ( labels ) input_dim = 4 # only numerical features for this example output_dim = 1 # predict rental_price # neural network definition inputs = keras . Input ( shape = ( 4 )) x = layers . Dense ( 100 )( inputs ) outputs = layers . Dense ( 1 )( x ) model = keras . Model ( inputs = inputs , outputs = outputs ) optimizer = keras . optimizers . SGD ( learning_rate = 1e-4 ) # transform data to TensorFlow format dataset = tf . data . Dataset . from_tensor_slices (( features . astype ( np . float32 ), labels . astype ( np . float32 ))) dataset = dataset . shuffle ( buffer_size = 64 ) . batch ( 32 ) def compute_loss ( labels , predictions ): return tf . reduce_mean ( tf . square ( labels - predictions )) # mean squared error def train_on_batch ( x , y ): with tf . GradientTape () as tape : predictions = model ( x ) loss = compute_loss ( y , predictions ) gradients = tape . gradient ( loss , model . trainable_weights ) optimizer . apply_gradients ( zip ( gradients , model . trainable_weights )) return loss epochs = 50 for epoch in range ( epochs ): for step , ( x , y ) in enumerate ( dataset ): loss = train_on_batch ( x , y ) if epoch % 10 == 0 : print ( f 'Epoch { epoch } : last batch loss = { float ( loss ) } ' ) # predict for test sample feat = [[ 2 , # rooms 1 , # bathrooms 1190 , # square feet 2000 ]] # initial price print ( \"The predicted price is %f \" % yscaler . inverse_transform ( model . predict ( xscaler . transform ( feat )))) Sklearn from sklearn import datasets , linear_model from sklearn.metrics import mean_squared_error , r2_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder import pandas as pd #load data data = pd . read_csv ( \"home_rentals.csv\" ) #target value labels = data [ 'rental_price' ] train1 = data . drop ([ 'rental_price' ], axis = 1 ) #train test split x_train , x_test , y_train , y_test = train_test_split ( train1 , labels ) # label encode values le = LabelEncoder () le . fit ( x_train [ 'location' ] . astype ( str )) x_train [ 'location' ] = le . transform ( x_train [ 'location' ] . astype ( str )) x_test [ 'location' ] = le . transform ( x_test [ 'location' ] . astype ( str )) le . fit ( x_train [ 'neighborhood' ] . astype ( str )) x_train [ 'neighborhood' ] = le . transform ( x_train [ 'neighborhood' ] . astype ( str )) x_test [ 'neighborhood' ] = le . transform ( x_test [ 'neighborhood' ] . astype ( str )) # Create linear regression object regr = linear_model . LinearRegression () # Train the model using the training sets regr . fit ( x_train , y_train ) # Make predictions using the testing set prediction = regr . predict ( x_test ) # The coefficients print ( 'Prediction ' , prediction ) print ( 'Coefficients: \\n ' , regr . coef_ ) # The mean squared error print ( 'Mean squared error: %.2f ' % mean_squared_error ( y_test , prediction )) # The coefficient of determination: 1 is perfect prediction print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , prediction )) Ludwig from ludwig.api import LudwigModel import pandas as pd # read data train_dataf = pd . read_csv ( \"train.csv\" ) # defining the data model_definition = { 'input_features' :[ { 'name' : 'number_of_rooms' , 'type' : 'numerical' }, { 'name' : 'number_of_bathrooms' , 'type' : 'numerical' }, { 'name' : 'sqft' , 'type' : 'numerical' }, { 'name' : 'location' , 'type' : 'text' }, { 'name' : 'days_on_market' , 'type' : 'numerical' }, { 'name' : 'initial_price' , 'type' : 'numerical' }, { 'name' : 'neighborhood' , 'type' : 'text' }, ], 'output_features' : [ { 'name' : 'rental_price' , 'type' : 'numerical' } ] } # creating and training the model model = LudwigModel ( model_definition ) train_stats = model . train ( data_df = train_dataf ) # read test data test_dataf = pd . read_csv ( \"test.csv\" ) #predict data predictions = model . predict ( data_df = test_dataf ) print ( predictions ) model . close () Note: If the data is inconsistent or of erroneous, null value it may throw an error. So you may want to preprocess/clean the data first. Mindsdb from mindsdb import Predictor # tell mindsDB what we want to learn and from what data Predictor ( name = 'home_rentals_price' ) . learn ( to_predict = 'rental_price' , # the column we want to learn to predict given all the data in the file from_data = 'train.csv' # the path to the file where we can learn from, (note: can be url) ) # use the model to make predictions result = Predictor ( name = 'home_rentals_price' ) . predict ( when = { 'number_of_rooms' : 2 , 'initial_price' : 2000 , 'number_of_bathrooms' : 1 , 'sqft' : 1190 }) # now print the results print ( 'The predicted price is between $ {price} with {conf} confidence' . format ( price = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence_interval' ], conf = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence' ])) Generally speaking, Mindsdb differentiates itself from other libraries by its simplicity . Lastly, Mindsdb Studio provides you with an easy way to visiualize more insights about the model. This can also be done by calling mdb.get_model_data('model_name') , but it's easier to use Mindsdb Studio to visualize the data, rather than looking at the raw json. Comparing Mindsdb accuracy with state-of-the-art models We have a few example datasets where we try to compare the accuracy obtained by Mindsdb with that of the best models we could find. It should be noted, Mindsdb accuracy doesn't always beat or match stat-of-the-art models, but the main goal of Mindsdb is to learn quickly, be very easy to use and be adaptable on any dataset. If you have the time and know-how to build a model that performs better than Mindsdb, but you still want the insights into the model and the data, as well as the pre-processing that Mindsdb provides, you can always plug in a custom machine learning model into Mindsdb. We are currently creating a new Benchmarks repository where you can find detailed list with up to date examples. Untill we release that you can check the old list of accuracy comparisons on our examples repo . Each directory containes different examples, datasets and README.md . To see the accuracies and the models, simply run mindsdb_acc.py to run mindsdb on the dataset. At some point we might keep a more easy to read list of these comparisons, but for now the results change to often and there are too many models to make this practical to maintain. We invite anyone with an interesting dataset and a well performing models to send it to us, or contribute to this repository, so that we can see how mindsdb stands up to it (or try it themselves and tell us the results they got).","title":"Comparison with other Libraries"},{"location":"comparisons/ComparsionWithOtherLibraries/#models","text":"With libraries such as Tensorflow, Sklearn, Pytorch you must have the expertise to build models from scratch. Your models are also black boxes, you can't be sure how or why they work and you have to pre-process your data in a format that's suitable for the model and look for any errors in the data yourself. With Mindsdb anyone can build state of the art models without any machine learning knowledge. Mindsdb also provides data extraction, analyses your input data and analyses the resulting model to try and understand what makes it work and what types of situations it works best in.","title":"Models"},{"location":"comparisons/ComparsionWithOtherLibraries/#data-preprocessing","text":"Building models require time for cleaning the data, normalizing the data, converting it into the format your library uses, determining the type of data in each column and a proper encoding for it Mindsdb can read data from csv, json, excel, file urls, s3 objects , dataframes and relational database tables or queries (currently there's native support for maraiadb, mysql and postgres), you just need to tell it which colum(s) it should predict. It will automatically process the data for you and give insights about the data.","title":"Data Preprocessing"},{"location":"comparisons/ComparsionWithOtherLibraries/#code-samples","text":"We are going to use home_rentals.csv dataset for comparison purpose. Inside the dataset dir, you can find the dataset split into train and test data. Our goal is to predict the rental_price of the house given the information we have in home_rental.csv . We will look at doing this with Sklearn, Tensorflow, Ludwig and Mindsdb. Sklearn is a generic easy-to-use machine learning library. Tensorflow is the state of the art deep learning model building library from google. Ludwig is a library from Uber that aims to help people build machine learning models without knowledge of machine learning (similar to mindsdb)","title":"Code Samples"},{"location":"comparisons/ComparsionWithOtherLibraries/#building-the-model","text":"Now we will build the actual models to train on the training dataset and run some predictions on the testing dataset. For the purpose of this example, we'll build a simple linear regression with both Tensorflow and Sklearn, in order to keep the code to a minimum.","title":"Building the model"},{"location":"comparisons/ComparsionWithOtherLibraries/#tensorflow","text":"import tensorflow as tf import pandas as pd import numpy as np from tensorflow import keras from tensorflow.keras import layers from sklearn.preprocessing import MinMaxScaler # process data df = pd . read_csv ( \"home_rentals.csv\" ) labels = df . pop ( 'rental_price' ) . values . reshape ( - 1 , 1 ) features = df . _get_numeric_data () . values xscaler = MinMaxScaler () features = xscaler . fit_transform ( features ) yscaler = MinMaxScaler () labels = yscaler . fit_transform ( labels ) input_dim = 4 # only numerical features for this example output_dim = 1 # predict rental_price # neural network definition inputs = keras . Input ( shape = ( 4 )) x = layers . Dense ( 100 )( inputs ) outputs = layers . Dense ( 1 )( x ) model = keras . Model ( inputs = inputs , outputs = outputs ) optimizer = keras . optimizers . SGD ( learning_rate = 1e-4 ) # transform data to TensorFlow format dataset = tf . data . Dataset . from_tensor_slices (( features . astype ( np . float32 ), labels . astype ( np . float32 ))) dataset = dataset . shuffle ( buffer_size = 64 ) . batch ( 32 ) def compute_loss ( labels , predictions ): return tf . reduce_mean ( tf . square ( labels - predictions )) # mean squared error def train_on_batch ( x , y ): with tf . GradientTape () as tape : predictions = model ( x ) loss = compute_loss ( y , predictions ) gradients = tape . gradient ( loss , model . trainable_weights ) optimizer . apply_gradients ( zip ( gradients , model . trainable_weights )) return loss epochs = 50 for epoch in range ( epochs ): for step , ( x , y ) in enumerate ( dataset ): loss = train_on_batch ( x , y ) if epoch % 10 == 0 : print ( f 'Epoch { epoch } : last batch loss = { float ( loss ) } ' ) # predict for test sample feat = [[ 2 , # rooms 1 , # bathrooms 1190 , # square feet 2000 ]] # initial price print ( \"The predicted price is %f \" % yscaler . inverse_transform ( model . predict ( xscaler . transform ( feat ))))","title":"Tensorflow"},{"location":"comparisons/ComparsionWithOtherLibraries/#sklearn","text":"from sklearn import datasets , linear_model from sklearn.metrics import mean_squared_error , r2_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder import pandas as pd #load data data = pd . read_csv ( \"home_rentals.csv\" ) #target value labels = data [ 'rental_price' ] train1 = data . drop ([ 'rental_price' ], axis = 1 ) #train test split x_train , x_test , y_train , y_test = train_test_split ( train1 , labels ) # label encode values le = LabelEncoder () le . fit ( x_train [ 'location' ] . astype ( str )) x_train [ 'location' ] = le . transform ( x_train [ 'location' ] . astype ( str )) x_test [ 'location' ] = le . transform ( x_test [ 'location' ] . astype ( str )) le . fit ( x_train [ 'neighborhood' ] . astype ( str )) x_train [ 'neighborhood' ] = le . transform ( x_train [ 'neighborhood' ] . astype ( str )) x_test [ 'neighborhood' ] = le . transform ( x_test [ 'neighborhood' ] . astype ( str )) # Create linear regression object regr = linear_model . LinearRegression () # Train the model using the training sets regr . fit ( x_train , y_train ) # Make predictions using the testing set prediction = regr . predict ( x_test ) # The coefficients print ( 'Prediction ' , prediction ) print ( 'Coefficients: \\n ' , regr . coef_ ) # The mean squared error print ( 'Mean squared error: %.2f ' % mean_squared_error ( y_test , prediction )) # The coefficient of determination: 1 is perfect prediction print ( 'Coefficient of determination: %.2f ' % r2_score ( y_test , prediction ))","title":"Sklearn"},{"location":"comparisons/ComparsionWithOtherLibraries/#ludwig","text":"from ludwig.api import LudwigModel import pandas as pd # read data train_dataf = pd . read_csv ( \"train.csv\" ) # defining the data model_definition = { 'input_features' :[ { 'name' : 'number_of_rooms' , 'type' : 'numerical' }, { 'name' : 'number_of_bathrooms' , 'type' : 'numerical' }, { 'name' : 'sqft' , 'type' : 'numerical' }, { 'name' : 'location' , 'type' : 'text' }, { 'name' : 'days_on_market' , 'type' : 'numerical' }, { 'name' : 'initial_price' , 'type' : 'numerical' }, { 'name' : 'neighborhood' , 'type' : 'text' }, ], 'output_features' : [ { 'name' : 'rental_price' , 'type' : 'numerical' } ] } # creating and training the model model = LudwigModel ( model_definition ) train_stats = model . train ( data_df = train_dataf ) # read test data test_dataf = pd . read_csv ( \"test.csv\" ) #predict data predictions = model . predict ( data_df = test_dataf ) print ( predictions ) model . close () Note: If the data is inconsistent or of erroneous, null value it may throw an error. So you may want to preprocess/clean the data first.","title":"Ludwig"},{"location":"comparisons/ComparsionWithOtherLibraries/#mindsdb","text":"from mindsdb import Predictor # tell mindsDB what we want to learn and from what data Predictor ( name = 'home_rentals_price' ) . learn ( to_predict = 'rental_price' , # the column we want to learn to predict given all the data in the file from_data = 'train.csv' # the path to the file where we can learn from, (note: can be url) ) # use the model to make predictions result = Predictor ( name = 'home_rentals_price' ) . predict ( when = { 'number_of_rooms' : 2 , 'initial_price' : 2000 , 'number_of_bathrooms' : 1 , 'sqft' : 1190 }) # now print the results print ( 'The predicted price is between $ {price} with {conf} confidence' . format ( price = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence_interval' ], conf = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence' ])) Generally speaking, Mindsdb differentiates itself from other libraries by its simplicity . Lastly, Mindsdb Studio provides you with an easy way to visiualize more insights about the model. This can also be done by calling mdb.get_model_data('model_name') , but it's easier to use Mindsdb Studio to visualize the data, rather than looking at the raw json.","title":"Mindsdb"},{"location":"comparisons/ComparsionWithOtherLibraries/#comparing-mindsdb-accuracy-with-state-of-the-art-models","text":"We have a few example datasets where we try to compare the accuracy obtained by Mindsdb with that of the best models we could find. It should be noted, Mindsdb accuracy doesn't always beat or match stat-of-the-art models, but the main goal of Mindsdb is to learn quickly, be very easy to use and be adaptable on any dataset. If you have the time and know-how to build a model that performs better than Mindsdb, but you still want the insights into the model and the data, as well as the pre-processing that Mindsdb provides, you can always plug in a custom machine learning model into Mindsdb. We are currently creating a new Benchmarks repository where you can find detailed list with up to date examples. Untill we release that you can check the old list of accuracy comparisons on our examples repo . Each directory containes different examples, datasets and README.md . To see the accuracies and the models, simply run mindsdb_acc.py to run mindsdb on the dataset. At some point we might keep a more easy to read list of these comparisons, but for now the results change to often and there are too many models to make this practical to maintain. We invite anyone with an interesting dataset and a well performing models to send it to us, or contribute to this repository, so that we can see how mindsdb stands up to it (or try it themselves and tell us the results they got).","title":"Comparing Mindsdb accuracy with state-of-the-art models"},{"location":"databases/","text":"AI Tables - Add Automated Machine Learning capabilities into databases You can make Machine Learning predictions directly inside the database by using MindsDB AI-Tables with the most popular database management systems like MySQL, MariaDB, PostgreSQL, ClickHouse. Any database user can now create, train and deploy machine learning models with the same knowledge they have of SQL. AI Tables benefits AI tables help to accelerate development speed and reduce complexity of machine learning workflows: Automated model training and deployment Predictions done at the Data Layer Saves time of moving to production Greater insight into model accuracy Easy to manage and use Open-source based Learn more how AI Tables work from the video below. Try it by installing MindsDB and following the tutorials . AITables example AITables differ from normal tables in that they can generate predictions upon being queried and returning such predictions like if it was data that existed on the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this; SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > To really sink in this idea, let us expand the concept through an example. The used car price example Imagine that you want to solve the problem of estimating the right price for a car on your website that has been selling used cars over the past 2 years. The data is persistend in your database inside a table called used_cars_data where you keep records of every car you have sold so far, storing information such as: price, transmission, mileage, fuel_type, road_tax, mpg (Miles Per Gallon) and engine_size. Since you have historical data, you know that you could use Machine Learning to solve this problem. Wouldn't it be nice if you could simply tell your database server to do and manage the Machine Learning parts for you? At MindsDB we think so too! And AI-Tables baked directly to your database are here to do exactly that. You can for instance with a single INSERT statement, create a machine learning model/predictor trained to predict \u2018price\u2019 using the data that lives in the table sold_cars and publish it as an AI-Table called 'used_cars_model'. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); After that you can get price predictions by querying the generated 'used_cars_model' AI-Table, as follows: SELECT price , confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; As you can see with AI-Tables, we are aiming to simplify Machine Learning mechanics to simple SQL queries, so that you can focus on the important part; which is to think about what predictions you need and what data you want your ML to learn from to make such predictions. Currently, we are supporting the integration with: MySQL PostgreSQL MariaDB ClickHouse","title":"AI Tables - Add Automated Machine Learning capabilities into databases"},{"location":"databases/#ai-tables-add-automated-machine-learning-capabilities-into-databases","text":"You can make Machine Learning predictions directly inside the database by using MindsDB AI-Tables with the most popular database management systems like MySQL, MariaDB, PostgreSQL, ClickHouse. Any database user can now create, train and deploy machine learning models with the same knowledge they have of SQL.","title":"AI Tables - Add Automated Machine Learning capabilities into databases"},{"location":"databases/#ai-tables-benefits","text":"AI tables help to accelerate development speed and reduce complexity of machine learning workflows: Automated model training and deployment Predictions done at the Data Layer Saves time of moving to production Greater insight into model accuracy Easy to manage and use Open-source based Learn more how AI Tables work from the video below. Try it by installing MindsDB and following the tutorials .","title":"AI Tables benefits"},{"location":"databases/#aitables-example","text":"AITables differ from normal tables in that they can generate predictions upon being queried and returning such predictions like if it was data that existed on the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this; SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > To really sink in this idea, let us expand the concept through an example.","title":"AITables example"},{"location":"databases/#the-used-car-price-example","text":"Imagine that you want to solve the problem of estimating the right price for a car on your website that has been selling used cars over the past 2 years. The data is persistend in your database inside a table called used_cars_data where you keep records of every car you have sold so far, storing information such as: price, transmission, mileage, fuel_type, road_tax, mpg (Miles Per Gallon) and engine_size. Since you have historical data, you know that you could use Machine Learning to solve this problem. Wouldn't it be nice if you could simply tell your database server to do and manage the Machine Learning parts for you? At MindsDB we think so too! And AI-Tables baked directly to your database are here to do exactly that. You can for instance with a single INSERT statement, create a machine learning model/predictor trained to predict \u2018price\u2019 using the data that lives in the table sold_cars and publish it as an AI-Table called 'used_cars_model'. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); After that you can get price predictions by querying the generated 'used_cars_model' AI-Table, as follows: SELECT price , confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; As you can see with AI-Tables, we are aiming to simplify Machine Learning mechanics to simple SQL queries, so that you can focus on the important part; which is to think about what predictions you need and what data you want your ML to learn from to make such predictions. Currently, we are supporting the integration with: MySQL PostgreSQL MariaDB ClickHouse","title":"The used car price example"},{"location":"databases/Clickhouse/","text":"AI Tables in ClickHouse Now, you can train machine learning models straight from the database by using MindsDB and ClickHouse . Prerequisite You will need MindsDB version >= 2.0.0 and ClickHouse installed. Install MindsDB Install ClickHouse Configuration Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 0.0.0.0.) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default localhost). port(default 47335). log -- The logging configuration: console_level - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. file_level - \"INFO\", \"DEBUG\", \"ERROR\". folder logs - Directory of log files. format - Format of log message e.g \"%(asctime)s - %(levelname)s - %(message)s\". integrations['default_clickhouse'] -- This key specifies the integration type in this case default_clickhouse . The required keys are: user(default default) - The ClickHouse user name. host(default localhost) - Connect to the ClickHouse server on the given host. password - The password of the ClickHouse user. type - Integration type(mariadb, postgresql, mysql, clickhouse). port(default 8123) - The TCP/IP port number to use for the connection. interface -- This key is used by MindsDB and provides the path to the directory where MindsDB shall save configuration and model files: datastore enabled(default false) - If not provided MindsDB will use default storage inside /var. storage_dir - Path to the storage directory for datastore. mindsdb_native enabled - If not provided mindsdb_native will use default storage inside /var. storage_dir - Path to the storage directory for datastore. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_clickhouse\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 8123 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Start MindsDB To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file. Train new model To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_predictor' , 'SO2' , 'SELECT * FROM data.pollution_measurement' , { \"option\" : \"value\" } ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example SO2. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Query the model To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) You should get a similar response from MindsDB as: price predicted info 0.001156540079952395 0.9869 Check JSON bellow { \"predicted_value\" : 0.001156540079952395 , \"confidence\" : 0.9869 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 0.003184904620383531 , 0.013975553923630717 ], \"important_missing_information\" : [ \"Station code\" , \"Latitude\" , \"O3\" ], \"confidence_composition\" : { \"CO\" : 0.006 }, \"extra_insights\" : { \"if_missing\" : [{ \"NO2\" : 0.007549311956155897 }, { \"CO\" : 0.005459383721227349 }, { \"PM10\" : 0.003870252306568623 }] } } Delete the model If you want to delete the predictor that you have previously created run: INSERT INTO mindsdb . commands values ( 'DELETE predictor airq_predictor' ); To get additional information about the integration check out Machine Learning Models as Tables with ClickHouse tutorial.","title":"AI Tables in ClickHouse"},{"location":"databases/Clickhouse/#ai-tables-in-clickhouse","text":"Now, you can train machine learning models straight from the database by using MindsDB and ClickHouse .","title":"AI Tables in ClickHouse"},{"location":"databases/Clickhouse/#prerequisite","text":"You will need MindsDB version >= 2.0.0 and ClickHouse installed. Install MindsDB Install ClickHouse","title":"Prerequisite"},{"location":"databases/Clickhouse/#configuration","text":"Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 0.0.0.0.) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default localhost). port(default 47335). log -- The logging configuration: console_level - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. file_level - \"INFO\", \"DEBUG\", \"ERROR\". folder logs - Directory of log files. format - Format of log message e.g \"%(asctime)s - %(levelname)s - %(message)s\". integrations['default_clickhouse'] -- This key specifies the integration type in this case default_clickhouse . The required keys are: user(default default) - The ClickHouse user name. host(default localhost) - Connect to the ClickHouse server on the given host. password - The password of the ClickHouse user. type - Integration type(mariadb, postgresql, mysql, clickhouse). port(default 8123) - The TCP/IP port number to use for the connection. interface -- This key is used by MindsDB and provides the path to the directory where MindsDB shall save configuration and model files: datastore enabled(default false) - If not provided MindsDB will use default storage inside /var. storage_dir - Path to the storage directory for datastore. mindsdb_native enabled - If not provided mindsdb_native will use default storage inside /var. storage_dir - Path to the storage directory for datastore. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_clickhouse\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 8123 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" }","title":"Configuration"},{"location":"databases/Clickhouse/#start-mindsdb","text":"To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file.","title":"Start MindsDB"},{"location":"databases/Clickhouse/#train-new-model","text":"To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_predictor' , 'SO2' , 'SELECT * FROM data.pollution_measurement' , { \"option\" : \"value\" } ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example SO2. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"Train new model"},{"location":"databases/Clickhouse/#query-the-model","text":"To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) You should get a similar response from MindsDB as: price predicted info 0.001156540079952395 0.9869 Check JSON bellow { \"predicted_value\" : 0.001156540079952395 , \"confidence\" : 0.9869 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 0.003184904620383531 , 0.013975553923630717 ], \"important_missing_information\" : [ \"Station code\" , \"Latitude\" , \"O3\" ], \"confidence_composition\" : { \"CO\" : 0.006 }, \"extra_insights\" : { \"if_missing\" : [{ \"NO2\" : 0.007549311956155897 }, { \"CO\" : 0.005459383721227349 }, { \"PM10\" : 0.003870252306568623 }] } }","title":"Query the model"},{"location":"databases/Clickhouse/#delete-the-model","text":"If you want to delete the predictor that you have previously created run: INSERT INTO mindsdb . commands values ( 'DELETE predictor airq_predictor' ); To get additional information about the integration check out Machine Learning Models as Tables with ClickHouse tutorial.","title":"Delete the model"},{"location":"databases/MariaDB/","text":"AI Tables in MariaDB Now, you can train machine learning models straight from the database by using MindsDB and MariaDB . Prerequisite You will need MindsDB version >= 2.0.0 and MariaDB installed: Install MindsDB Install MariaDB Configuration Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters creata a new config.json file. The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 0.0.0.0.) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default localhost). port(default 47335). log -- The logging configuration: console_level - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. file_level - \"INFO\", \"DEBUG\", \"ERROR\". folder logs - Directory of log files. format - Format of log message e.g \"%(asctime)s - %(levelname)s - %(message)s\". integrations -- This key specifies the integration type in this case default_mariadb . The required keys are: user(default root) - The MariaDB user name. host(default localhost) - Connect to the MariaDB server on the given host. password - The password of the MariaDB account. type - Integration type(mariadb, postgresql, mysql, clickhouse). port(default 3306) - The TCP/IP port number to use for the connection. interface -- This key is used by MindsDB and provides the path to the directory where MindsDB shall save configuration and model files: datastore enabled(default false) - If not provided MindsDB will use default storage inside /var. storage_dir - Path to the storage directory for datastore. mindsdb_native enabled - If not provided mindsdb_native will use default storage inside /var. storage_dir - Path to the storage directory for datastore. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mariadb\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"password\" , \"port\" : 3306 , \"type\" : \"mariadb\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Install CONNECT Storage Engine Also you need to install the CONNECT Storage Engine to access external local data. Checkout MariaDB docs on how to do that. Start MindsDB To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file. Train new model To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'used_cars_model' , 'price' , 'SELECT * FROM test.UsedCarsData' , \"option,value\" ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example price. To predict multiple featurs include a comma separated string e.g 'price,year'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Query the model To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"A6\" AND mileage = 36203 AND transmission = \"Automatic\" AND fuelType = \"Diesel\" AND mpg = \"64.2\" AND engineSize = 2 AND year = 2016 AND tax = 20 ; You should get a similar response from MindsDB as: price predicted info 13111 0.9921 Check JSON bellow { \"predicted_value\" : 13111 , \"confidence\" : 0.9921 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10792 , 32749 ], \"important_missing_information\" : [], \"confidence_composition\" : { \"Model\" : 0.009 , \"year\" : 0.013 }, \"extra_insights\" : { \"if_missing\" : [{ \"Model\" : 12962 }, { \"year\" : 12137 }, { \"transmission\" : 2136 }, { \"mileage\" : 22706 }, { \"fuelType\" : 7134 }, { \"tax\" : 13210 }, { \"mpg\" : 27409 }, { \"engineSize\" : 13111 }] } } Delete the model To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statment while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'used_cars_model' Train and predict multiple features You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the price and a year : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'used_cars_model' , 'price,year' , 'SELECT * FROM test.UsedCarsData' , \"option,value\" } ); And query it using the select_data_query : SELECT price AS predicted , FROM mindsdb . used_cars_model WHERE select_data_query = 'SELECT year FROM price_data' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. If you want to follow along with a tutorial check out AI Tables in MariaDB tutorial.","title":"AI Tables in MariaDB"},{"location":"databases/MariaDB/#ai-tables-in-mariadb","text":"Now, you can train machine learning models straight from the database by using MindsDB and MariaDB .","title":"AI Tables in MariaDB"},{"location":"databases/MariaDB/#prerequisite","text":"You will need MindsDB version >= 2.0.0 and MariaDB installed: Install MindsDB Install MariaDB","title":"Prerequisite"},{"location":"databases/MariaDB/#configuration","text":"Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters creata a new config.json file. The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 0.0.0.0.) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default localhost). port(default 47335). log -- The logging configuration: console_level - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. file_level - \"INFO\", \"DEBUG\", \"ERROR\". folder logs - Directory of log files. format - Format of log message e.g \"%(asctime)s - %(levelname)s - %(message)s\". integrations -- This key specifies the integration type in this case default_mariadb . The required keys are: user(default root) - The MariaDB user name. host(default localhost) - Connect to the MariaDB server on the given host. password - The password of the MariaDB account. type - Integration type(mariadb, postgresql, mysql, clickhouse). port(default 3306) - The TCP/IP port number to use for the connection. interface -- This key is used by MindsDB and provides the path to the directory where MindsDB shall save configuration and model files: datastore enabled(default false) - If not provided MindsDB will use default storage inside /var. storage_dir - Path to the storage directory for datastore. mindsdb_native enabled - If not provided mindsdb_native will use default storage inside /var. storage_dir - Path to the storage directory for datastore. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mariadb\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"password\" , \"port\" : 3306 , \"type\" : \"mariadb\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Install CONNECT Storage Engine Also you need to install the CONNECT Storage Engine to access external local data. Checkout MariaDB docs on how to do that.","title":"Configuration"},{"location":"databases/MariaDB/#start-mindsdb","text":"To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file.","title":"Start MindsDB"},{"location":"databases/MariaDB/#train-new-model","text":"To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'used_cars_model' , 'price' , 'SELECT * FROM test.UsedCarsData' , \"option,value\" ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example price. To predict multiple featurs include a comma separated string e.g 'price,year'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"Train new model"},{"location":"databases/MariaDB/#query-the-model","text":"To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"A6\" AND mileage = 36203 AND transmission = \"Automatic\" AND fuelType = \"Diesel\" AND mpg = \"64.2\" AND engineSize = 2 AND year = 2016 AND tax = 20 ; You should get a similar response from MindsDB as: price predicted info 13111 0.9921 Check JSON bellow { \"predicted_value\" : 13111 , \"confidence\" : 0.9921 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10792 , 32749 ], \"important_missing_information\" : [], \"confidence_composition\" : { \"Model\" : 0.009 , \"year\" : 0.013 }, \"extra_insights\" : { \"if_missing\" : [{ \"Model\" : 12962 }, { \"year\" : 12137 }, { \"transmission\" : 2136 }, { \"mileage\" : 22706 }, { \"fuelType\" : 7134 }, { \"tax\" : 13210 }, { \"mpg\" : 27409 }, { \"engineSize\" : 13111 }] } }","title":"Query the model"},{"location":"databases/MariaDB/#delete-the-model","text":"To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statment while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'used_cars_model'","title":"Delete the model"},{"location":"databases/MariaDB/#train-and-predict-multiple-features","text":"You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the price and a year : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'used_cars_model' , 'price,year' , 'SELECT * FROM test.UsedCarsData' , \"option,value\" } ); And query it using the select_data_query : SELECT price AS predicted , FROM mindsdb . used_cars_model WHERE select_data_query = 'SELECT year FROM price_data' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. If you want to follow along with a tutorial check out AI Tables in MariaDB tutorial.","title":"Train and predict multiple features"},{"location":"databases/MySQL/","text":"AI Tables in MySQL Now, you can train machine learning models straight from the database by using MindsDB and MySQL . Prerequisite You will need MindsDB version >= 2.3.0 and MySQL installed: Install MindsDB Install MySQL Enable FEDERATED Storage Engine Configuration Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The available configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mysql'] -- This key specifies the integration type in this case default_mysql . The required keys are: user(default root) - The MySQL user name. host(default 127.0.0.1) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. enabled(true|false) - Enable MySQL integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Enable FEDERATED storage engine The FEDERATED storage engine is not enabled by default in the running server; to enable FEDERATED, you must start the MySQL server binary using the --federated option. Check official docs for more info. Start MindsDB To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file. Train new model To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'us_consumption' , 'consumption' , 'SELECT * FROM us_consumption' ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example consumption. To predict multiple featurs include a comma separated string e.g 'consumption,income'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Query the model To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE when_data = '{\"income\": 1.182497938, \"production\": 5.854555956,\"savings\": 3.183292657, \"unemployment\": 0.1, \"t\":\"2020-01-02\"}' ; You should get a similar response from MindsDB as: consumption predicted info 1.252233223 0.923 Check JSON bellow { \"predicted_value\" : 1.252233223 , \"confidence\" : 0.923 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 1.025658879956537 , 1.9702775375019028 ], \"important_missing_information\" : [], \"confidence_composition\" : {}, \"extra_insights\" : { \"if_missing\" : [{ \"income\" : 0.6966906986877563 }, { \"production\" : 2.5382917051924445 }, { \"savings\" : 1.169812868271305 }, { \"unemployment\" : 1.3 443338862946717 }] } } Delete the model To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statment while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'us_consumption' Train and predict multiple features You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the consumption and a income : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption, income' , 'SELECT * FROM us_consumption' , \"option,value\" } ); And query it using the select_data_query : SELECT consumption AS predicted , FROM mindsdb . us_consumption WHERE select_data_query = 'SELECT income FROM us_consumption' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. To get additional information follow the AiTables in MySQL tutorial .","title":"AI Tables in MySQL"},{"location":"databases/MySQL/#ai-tables-in-mysql","text":"Now, you can train machine learning models straight from the database by using MindsDB and MySQL .","title":"AI Tables in MySQL"},{"location":"databases/MySQL/#prerequisite","text":"You will need MindsDB version >= 2.3.0 and MySQL installed: Install MindsDB Install MySQL Enable FEDERATED Storage Engine","title":"Prerequisite"},{"location":"databases/MySQL/#configuration","text":"Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The available configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mysql'] -- This key specifies the integration type in this case default_mysql . The required keys are: user(default root) - The MySQL user name. host(default 127.0.0.1) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. enabled(true|false) - Enable MySQL integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Enable FEDERATED storage engine The FEDERATED storage engine is not enabled by default in the running server; to enable FEDERATED, you must start the MySQL server binary using the --federated option. Check official docs for more info.","title":"Configuration"},{"location":"databases/MySQL/#start-mindsdb","text":"To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file.","title":"Start MindsDB"},{"location":"databases/MySQL/#train-new-model","text":"To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'us_consumption' , 'consumption' , 'SELECT * FROM us_consumption' ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example consumption. To predict multiple featurs include a comma separated string e.g 'consumption,income'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"Train new model"},{"location":"databases/MySQL/#query-the-model","text":"To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE when_data = '{\"income\": 1.182497938, \"production\": 5.854555956,\"savings\": 3.183292657, \"unemployment\": 0.1, \"t\":\"2020-01-02\"}' ; You should get a similar response from MindsDB as: consumption predicted info 1.252233223 0.923 Check JSON bellow { \"predicted_value\" : 1.252233223 , \"confidence\" : 0.923 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 1.025658879956537 , 1.9702775375019028 ], \"important_missing_information\" : [], \"confidence_composition\" : {}, \"extra_insights\" : { \"if_missing\" : [{ \"income\" : 0.6966906986877563 }, { \"production\" : 2.5382917051924445 }, { \"savings\" : 1.169812868271305 }, { \"unemployment\" : 1.3 443338862946717 }] } }","title":"Query the model"},{"location":"databases/MySQL/#delete-the-model","text":"To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statment while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'us_consumption'","title":"Delete the model"},{"location":"databases/MySQL/#train-and-predict-multiple-features","text":"You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the consumption and a income : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption, income' , 'SELECT * FROM us_consumption' , \"option,value\" } ); And query it using the select_data_query : SELECT consumption AS predicted , FROM mindsdb . us_consumption WHERE select_data_query = 'SELECT income FROM us_consumption' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. To get additional information follow the AiTables in MySQL tutorial .","title":"Train and predict multiple features"},{"location":"databases/PostgreSQL/","text":"AI Tables in PostgreSQL Now, you can train machine learning models straight from the database by using MindsDB and PostgreSQL . Prerequisite You will need MindsDB version >= 2.3.0 and PostgreSQL installed: Install MindsDB Install PostgreSQL Install PostgreSQL foreign data wrapper for MySQL Configuration Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type in this case default_postgres . The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Install PostgreSQL foreign data wrapper for MySQL The Foreign Data Wrapper (mysql_fwd) can be installed from the EnterpriseDB repo . Start MindsDB To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file. Train new model To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption' , 'SELECT * FROM us_consumption' , '{\"timeseries_settings\":{\"order_by\": [\"t\"], \"window\":20}}' ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example consumption. To predict multiple featurs include a comma separated string e.g 'consumption,income'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . If you are using timeseries data check the Timeseries settings . Query the model To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE income = 1 . 182497938 AND production = 5 . 854555956 AND savings = 3 . 183292657 AND unemployment = 0 . 1 You should get a similar response from MindsDB as: consumption predicted info 1.4979682087292199 0.9475 Check JSON bellow { \"predicted_value\" : 1.4979682087292199 , \"confidence\" : 0.9475 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 1.025658879956537 , 1.9702775375019028 ], \"important_missing_information\" : [], \"confidence_composition\" : {}, \"extra_insights\" : { \"if_missing\" : [{ \"income\" : 0.6966906986877563 }, { \"production\" : 2.5382917051924445 }, { \"savings\" : 1.169812868271305 }, { \"unemployment\" : 1.3 443338862946717 }] } } Delete the model To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statment while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'us_consumption' Train and predict multiple features You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the consumption and a income : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption, income' , 'SELECT * FROM us_consumption' , \"option,value\" } ); And query it using the select_data_query : SELECT consumption AS predicted , FROM mindsdb . used_cars_model WHERE select_data_query = 'SELECT income FROM us_consumption' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. To get additional information follow the AiTables in PostgreSQL tutorail .","title":"AI Tables in PostgreSQL"},{"location":"databases/PostgreSQL/#ai-tables-in-postgresql","text":"Now, you can train machine learning models straight from the database by using MindsDB and PostgreSQL .","title":"AI Tables in PostgreSQL"},{"location":"databases/PostgreSQL/#prerequisite","text":"You will need MindsDB version >= 2.3.0 and PostgreSQL installed: Install MindsDB Install PostgreSQL Install PostgreSQL foreign data wrapper for MySQL","title":"Prerequisite"},{"location":"databases/PostgreSQL/#configuration","text":"Default configuration MindsDB will try to use the default configuration(hosts, ports, usernames) for each of the database integrations. If you want to extend that or you are using different parameters create a new config.json file. The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type in this case default_postgres . The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } Install PostgreSQL foreign data wrapper for MySQL The Foreign Data Wrapper (mysql_fwd) can be installed from the EnterpriseDB repo .","title":"Configuration"},{"location":"databases/PostgreSQL/#start-mindsdb","text":"To start mindsdb run following command: python3 - m mindsdb -- api = mysql -- config = config . json The --api parameter specifies the type of API to use in this case mysql. The --config specifies the location of the configuration file.","title":"Start MindsDB"},{"location":"databases/PostgreSQL/#train-new-model","text":"To train a new model, insert a new record inside the mindsdb.predictors table as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption' , 'SELECT * FROM us_consumption' , '{\"timeseries_settings\":{\"order_by\": [\"t\"], \"window\":20}}' ); name (string) -- The name of the predictor. predict (string) -- The feature you want to predict, in this example consumption. To predict multiple featurs include a comma separated string e.g 'consumption,income'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . If you are using timeseries data check the Timeseries settings .","title":"Train new model"},{"location":"databases/PostgreSQL/#query-the-model","text":"To query the model and get the predictions SELECT the target variable, confidence and explanation for that prediction. SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE income = 1 . 182497938 AND production = 5 . 854555956 AND savings = 3 . 183292657 AND unemployment = 0 . 1 You should get a similar response from MindsDB as: consumption predicted info 1.4979682087292199 0.9475 Check JSON bellow { \"predicted_value\" : 1.4979682087292199 , \"confidence\" : 0.9475 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 1.025658879956537 , 1.9702775375019028 ], \"important_missing_information\" : [], \"confidence_composition\" : {}, \"extra_insights\" : { \"if_missing\" : [{ \"income\" : 0.6966906986877563 }, { \"production\" : 2.5382917051924445 }, { \"savings\" : 1.169812868271305 }, { \"unemployment\" : 1.3 443338862946717 }] } }","title":"Query the model"},{"location":"databases/PostgreSQL/#delete-the-model","text":"To delete the predictor that you have previously created, you need to delete it from mindsdb.predictors table. The name should be equal to name added in the INSERT statment while creating the predictor, e.g: DELETE FROM mindsdb . predictors WHERE name = 'us_consumption'","title":"Delete the model"},{"location":"databases/PostgreSQL/#train-and-predict-multiple-features","text":"You can train a model that will predict multiple features by adding a comma separated features values in the predict column. e.g to predict the consumption and a income : INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'us_consumption' , 'consumption, income' , 'SELECT * FROM us_consumption' , \"option,value\" } ); And query it using the select_data_query : SELECT consumption AS predicted , FROM mindsdb . used_cars_model WHERE select_data_query = 'SELECT income FROM us_consumption' ; The requirements to query with select_data_query are: It must be a valid SQL statement It must return columns with names the same as predictor fields. To get additional information follow the AiTables in PostgreSQL tutorail .","title":"Train and predict multiple features"},{"location":"datasources/clickhouse/","text":"Connect to ClickHouse database Connecting MindsDB to your ClickHouse database could be done in two ways: Using MindsDB Graphical User Interface called Studio . Using clickhouse client . MindsDB Studio Using MindsDB Studio, you can connect to the ClickHouse database with a few clicks. Connect to database From the left navigation menu select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal: Select ClickHouse as Supported Database. Add the Database name. Add the Hostname. Add Port. Add ClickHouse user. Add Password for ClickHouse user. Click on CONNECT . Create new Datasource Click on the NEW DATASOURCE button. In the Datasource from DB integration modal: Add Datasource Name. Add Database name. Add SELECT Query e.g (SELECT * FROM my_database) Click on CREATE . That's all You have succesfully connected to ClickHouse from MindsDB Studio. Next step is to train the Machine Learning model . ClickHouse client Before using clickhouse client to connect MindsDB and ClickHouse Server you will need to add additional configuration before starting MindsDB Server. Create a new config.json file. Expand the example below to preview the configuration example. Configuration example { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_clickhouse\" : { \"database\" : \"default\" , \"published\" : true , \"type\" : \"clickhouse\" , \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 8123 , \"user\" : \"default\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_clickhouse'] -- This key specifies the integration type in this case default_clickhouse . The required keys are: user(default is default user) - The ClickHouse user name. host(default 127.0.0.1) - Connect to the ClickHouse server on the given host. password - The password of the ClickHouse user. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 8123) - The TCP/IP port number to use for the connection. publish(true|false) - Enable ClickHouse integration. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. After creating the config.json file, you will need to start MindsDB and provide the path to the newly created config.json as: python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use in this case HTTP and MySQL. The --config specifies the location of the configuration file. If MindsDB is succesfully connected to your ClickHouse database, it will create a new database mindsdb and new table predictors . After starting the server, from your clickhouse-client you can run SELECT query from it to make sure integration is succesfull. SELECT * FROM mindsdb . predictors ; That's all You have succesfully connected MindsDB Server and ClickHouse. Next step is to train the Machine Learning model .","title":"ClickHouse"},{"location":"datasources/clickhouse/#connect-to-clickhouse-database","text":"Connecting MindsDB to your ClickHouse database could be done in two ways: Using MindsDB Graphical User Interface called Studio . Using clickhouse client .","title":"Connect to ClickHouse database"},{"location":"datasources/clickhouse/#mindsdb-studio","text":"Using MindsDB Studio, you can connect to the ClickHouse database with a few clicks.","title":"MindsDB Studio"},{"location":"datasources/clickhouse/#connect-to-database","text":"From the left navigation menu select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal: Select ClickHouse as Supported Database. Add the Database name. Add the Hostname. Add Port. Add ClickHouse user. Add Password for ClickHouse user. Click on CONNECT .","title":"Connect to database"},{"location":"datasources/clickhouse/#create-new-datasource","text":"Click on the NEW DATASOURCE button. In the Datasource from DB integration modal: Add Datasource Name. Add Database name. Add SELECT Query e.g (SELECT * FROM my_database) Click on CREATE . That's all You have succesfully connected to ClickHouse from MindsDB Studio. Next step is to train the Machine Learning model .","title":"Create new Datasource"},{"location":"datasources/clickhouse/#clickhouse-client","text":"Before using clickhouse client to connect MindsDB and ClickHouse Server you will need to add additional configuration before starting MindsDB Server. Create a new config.json file. Expand the example below to preview the configuration example. Configuration example { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_clickhouse\" : { \"database\" : \"default\" , \"published\" : true , \"type\" : \"clickhouse\" , \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 8123 , \"user\" : \"default\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_clickhouse'] -- This key specifies the integration type in this case default_clickhouse . The required keys are: user(default is default user) - The ClickHouse user name. host(default 127.0.0.1) - Connect to the ClickHouse server on the given host. password - The password of the ClickHouse user. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 8123) - The TCP/IP port number to use for the connection. publish(true|false) - Enable ClickHouse integration. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. After creating the config.json file, you will need to start MindsDB and provide the path to the newly created config.json as: python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use in this case HTTP and MySQL. The --config specifies the location of the configuration file. If MindsDB is succesfully connected to your ClickHouse database, it will create a new database mindsdb and new table predictors . After starting the server, from your clickhouse-client you can run SELECT query from it to make sure integration is succesfull. SELECT * FROM mindsdb . predictors ; That's all You have succesfully connected MindsDB Server and ClickHouse. Next step is to train the Machine Learning model .","title":"ClickHouse client"},{"location":"datasources/local/","text":"Connect to local dataset Uploading local dataset to MindsDB could be easly done with a few clicks. You can upload any tabular data format as: CSV, TSV XLS/XLSX/ODC TXT JSON XML Upload local dataset From the left navigation menu select Data dashboard. Click on the UPLOAD button. In the New data source modal: Click on the File explorer. Choose the dataset you want to upload. Add name for the datasource. Click on UPLOAD . That's all You have succesfully uploaded your local dataset from MindsDB Studio. Next step is to train the Machine Learning model .","title":"Local dataset"},{"location":"datasources/local/#connect-to-local-dataset","text":"Uploading local dataset to MindsDB could be easly done with a few clicks. You can upload any tabular data format as: CSV, TSV XLS/XLSX/ODC TXT JSON XML","title":"Connect to local dataset"},{"location":"datasources/local/#upload-local-dataset","text":"From the left navigation menu select Data dashboard. Click on the UPLOAD button. In the New data source modal: Click on the File explorer. Choose the dataset you want to upload. Add name for the datasource. Click on UPLOAD . That's all You have succesfully uploaded your local dataset from MindsDB Studio. Next step is to train the Machine Learning model .","title":"Upload local dataset"},{"location":"datasources/mariadb/","text":"Connect to MariaDB database Connecting MindsDB to your MariaDB database could be done in two ways: Using MindsDB Graphical User Interface called Studio . Using SQL clients . Prerequisite To connect to MariaDB from MindsDB, you will need to install CONNECT Storage Engine. How to install CONNECT Storage Engine Please check the MariaDB documentation . Read more about CONNECT Storage Engine . MindsDB Studio Using MindsDB Studio, you can connect to the MariaDB database with a few clicks. Connect to database From the left navigation menu select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal: Select MariaDB as Supported Database. Add the Database name. Add the Hostname. Add Port. Add MariaDB user. Add Password for MariaDB user. Click on CONNECT . Create new Datasource Click on the NEW DATASOURCE button. In the Datasource from DB integration modal: Add Datasource Name. Add Database name. Add SELECT Query e.g (SELECT * FROM my_database) Click on CREATE . That's all You have succesfully connected to MariaDB from MindsDB Studio. Next step is to train the Machine Learning model . MySQL client Before using mysql-client to connect MindsDB and MariaDB you will need to add additional configuration before starting MindsDB Server. Create a new config.json file. Expand the example below to preview the configuration example. Configuration example { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_mariadb\" : { \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 3306 , \"publish\" : true , \"type\" : \"mariadb\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: api['http'] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mariadb'] -- This key specifies the integration type in this case default_mariadb . The required keys are: user(default root) - The MariaDB user name. host(default 127.0.0.1) - Connect to the MariaDB server on the given host. password - The password of the MariaDB account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. publish(true|false) - Enable MariaDB integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. After creating the config.json file, you will need to start MindsDB and provide the path to the newly created config.json as: python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use in this case HTTP and MySQL. The --config specifies the location of the configuration file. If MindsDB is succesfully connected to your MariaDB database, it will create a new database mindsdb and new table predictors . After starting the server, from your mariadb-client you can run SELECT query from it to make sure integration is succesfull. SELECT * FROM mindsdb . predictors ; That's all You have succesfully connected MindsDB Server and MariaDB. Next step is to train the Machine Learning model .","title":"MariaDB"},{"location":"datasources/mariadb/#connect-to-mariadb-database","text":"Connecting MindsDB to your MariaDB database could be done in two ways: Using MindsDB Graphical User Interface called Studio . Using SQL clients .","title":"Connect to MariaDB database"},{"location":"datasources/mariadb/#prerequisite","text":"To connect to MariaDB from MindsDB, you will need to install CONNECT Storage Engine. How to install CONNECT Storage Engine Please check the MariaDB documentation . Read more about CONNECT Storage Engine .","title":"Prerequisite"},{"location":"datasources/mariadb/#mindsdb-studio","text":"Using MindsDB Studio, you can connect to the MariaDB database with a few clicks.","title":"MindsDB Studio"},{"location":"datasources/mariadb/#connect-to-database","text":"From the left navigation menu select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal: Select MariaDB as Supported Database. Add the Database name. Add the Hostname. Add Port. Add MariaDB user. Add Password for MariaDB user. Click on CONNECT .","title":"Connect to database"},{"location":"datasources/mariadb/#create-new-datasource","text":"Click on the NEW DATASOURCE button. In the Datasource from DB integration modal: Add Datasource Name. Add Database name. Add SELECT Query e.g (SELECT * FROM my_database) Click on CREATE . That's all You have succesfully connected to MariaDB from MindsDB Studio. Next step is to train the Machine Learning model .","title":"Create new Datasource"},{"location":"datasources/mariadb/#mysql-client","text":"Before using mysql-client to connect MindsDB and MariaDB you will need to add additional configuration before starting MindsDB Server. Create a new config.json file. Expand the example below to preview the configuration example. Configuration example { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_mariadb\" : { \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 3306 , \"publish\" : true , \"type\" : \"mariadb\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: api['http'] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mariadb'] -- This key specifies the integration type in this case default_mariadb . The required keys are: user(default root) - The MariaDB user name. host(default 127.0.0.1) - Connect to the MariaDB server on the given host. password - The password of the MariaDB account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. publish(true|false) - Enable MariaDB integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. After creating the config.json file, you will need to start MindsDB and provide the path to the newly created config.json as: python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use in this case HTTP and MySQL. The --config specifies the location of the configuration file. If MindsDB is succesfully connected to your MariaDB database, it will create a new database mindsdb and new table predictors . After starting the server, from your mariadb-client you can run SELECT query from it to make sure integration is succesfull. SELECT * FROM mindsdb . predictors ; That's all You have succesfully connected MindsDB Server and MariaDB. Next step is to train the Machine Learning model .","title":"MySQL client"},{"location":"datasources/mysql/","text":"Connect to MySQL database Connecting MindsDB to your MySQL database could be done in two ways: Using MindsDB Graphical User Interface called Studio . Using MySQL client . Prerequisite To connect to your MySQL Server from MindsDB, you will need to enable FEDERATED Storage Engine. How to enable FEDERATED Storage Engine Please check the MySQL documentation . Stackoverflow answers . Read more about FEDERATED Storage Engine . MindsDB Studio Using MindsDB Studio, you can connect to the MySQL database with a few clicks. Connect to database From the left navigation menu select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal: Select MySQL as Supported Database. Add the Database name. Add the Hostname. Add Port. Add MySQL user. Add Password for MySQL user. Click on CONNECT . Create new Datasource Click on the NEW DATASOURCE button. In the Datasource from DB integration modal: Add Datasource Name. Add Database name. Add SELECT Query e.g (SELECT * FROM my_database) Click on CREATE . That's all You have succesfully connected to MySQL from MindsDB Studio. Next step is to train the Machine Learning model . MySQL client Before using MySQL client to connect MindsDB and MySQL Server you will need to add additional configuration before starting MindsDB Server. Create a new config.json file. Expand the example below to preview the configuration example. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: api['http'] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mysql'] -- This key specifies the integration type in this case default_mysql . The required keys are: user(default root) - The MySQL user name. host(default 127.0.0.1, don't use localhost here) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. publish(true|false) - Enable MySQL integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. After creating the config.json file, you will need to start MindsDB and provide the path to the newly created config.json as: python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use in this case HTTP and MySQL. The --config specifies the location of the configuration file. If MindsDB is succesfully connected to your MySQL database, it will create a new database mindsdb and new table predictors . After starting the server, from your mysql-client you can run SELECT query from it to make sure integration is succesfull. SELECT * FROM mindsdb . predictors ; That's all You have succesfully connected MindsDB Server and MySQL. Next step is to train the Machine Learning model .","title":"MySQL"},{"location":"datasources/mysql/#connect-to-mysql-database","text":"Connecting MindsDB to your MySQL database could be done in two ways: Using MindsDB Graphical User Interface called Studio . Using MySQL client .","title":"Connect to MySQL database"},{"location":"datasources/mysql/#prerequisite","text":"To connect to your MySQL Server from MindsDB, you will need to enable FEDERATED Storage Engine. How to enable FEDERATED Storage Engine Please check the MySQL documentation . Stackoverflow answers . Read more about FEDERATED Storage Engine .","title":"Prerequisite"},{"location":"datasources/mysql/#mindsdb-studio","text":"Using MindsDB Studio, you can connect to the MySQL database with a few clicks.","title":"MindsDB Studio"},{"location":"datasources/mysql/#connect-to-database","text":"From the left navigation menu select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal: Select MySQL as Supported Database. Add the Database name. Add the Hostname. Add Port. Add MySQL user. Add Password for MySQL user. Click on CONNECT .","title":"Connect to database"},{"location":"datasources/mysql/#create-new-datasource","text":"Click on the NEW DATASOURCE button. In the Datasource from DB integration modal: Add Datasource Name. Add Database name. Add SELECT Query e.g (SELECT * FROM my_database) Click on CREATE . That's all You have succesfully connected to MySQL from MindsDB Studio. Next step is to train the Machine Learning model .","title":"Create new Datasource"},{"location":"datasources/mysql/#mysql-client","text":"Before using MySQL client to connect MindsDB and MySQL Server you will need to add additional configuration before starting MindsDB Server. Create a new config.json file. Expand the example below to preview the configuration example. Configuration example { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: api['http'] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_mysql'] -- This key specifies the integration type in this case default_mysql . The required keys are: user(default root) - The MySQL user name. host(default 127.0.0.1, don't use localhost here) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 3306) - The TCP/IP port number to use for the connection. publish(true|false) - Enable MySQL integration. log['level'] -- The logging configuration(optional): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. After creating the config.json file, you will need to start MindsDB and provide the path to the newly created config.json as: python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use in this case HTTP and MySQL. The --config specifies the location of the configuration file. If MindsDB is succesfully connected to your MySQL database, it will create a new database mindsdb and new table predictors . After starting the server, from your mysql-client you can run SELECT query from it to make sure integration is succesfull. SELECT * FROM mindsdb . predictors ; That's all You have succesfully connected MindsDB Server and MySQL. Next step is to train the Machine Learning model .","title":"MySQL client"},{"location":"datasources/postgresql/","text":"Connect to PostgreSQL database Connecting MindsDB to your PostgreSQL database could be done in two ways: Using MindsDB Graphical User Interface called Studio . Using psql client . Prerequisite To connect to your PostgreSQL Server from MindsDB, you will need to install MySQL foreign data wrapper for PostgreSQL. How to install PostgreSQL foreign data wrapper Please check the mysql_fdw documentation . Stackoverflow answers . MindsDB Studio Using MindsDB Studio, you can connect to the PostgreSQL database with a few clicks. Connect to database From the left navigation menu select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal: Select PostgreSQL as Supported Database. Add the Database name. Add the Hostname. Add Port. Add PostgreSQL user. Add Password for PostgreSQL user. Click on CONNECT . Create new Datasource Click on the NEW DATASOURCE button. In the Datasource from DB integration modal: Add Datasource Name. Add Database name. Add SELECT Query e.g (SELECT * FROM my_database) Click on CREATE . That's all You have succesfully connected to PostgreSQL from MindsDB Studio. Next step is to train the Machine Learning model . PSQL client Before using psql client to connect MindsDB and PostgreSQL Server you will need to add additional configuration before starting MindsDB Server. Create a new config.json file. Expand the example below to preview the configuration example. Configuration example { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type in this case default_postgres . The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. publish(true|false) - Enable PostgreSQL integration. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. After creating the config.json file, you will need to start MindsDB and provide the path to the newly created config.json as: python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use in this case HTTP and MySQL. The --config specifies the location of the configuration file. If MindsDB is succesfully connected to your PostgreSQL database, it will create a new schema mindsdb and new table predictors . After starting the server, from your psql-client you can run SELECT query from it to make sure integration is succesfull. SELECT * FROM mindsdb . predictors ; That's all You have succesfully connected MindsDB Server and PostgreSQL. Next step is to train the Machine Learning model .","title":"PostgreSQL"},{"location":"datasources/postgresql/#connect-to-postgresql-database","text":"Connecting MindsDB to your PostgreSQL database could be done in two ways: Using MindsDB Graphical User Interface called Studio . Using psql client .","title":"Connect to PostgreSQL database"},{"location":"datasources/postgresql/#prerequisite","text":"To connect to your PostgreSQL Server from MindsDB, you will need to install MySQL foreign data wrapper for PostgreSQL. How to install PostgreSQL foreign data wrapper Please check the mysql_fdw documentation . Stackoverflow answers .","title":"Prerequisite"},{"location":"datasources/postgresql/#mindsdb-studio","text":"Using MindsDB Studio, you can connect to the PostgreSQL database with a few clicks.","title":"MindsDB Studio"},{"location":"datasources/postgresql/#connect-to-database","text":"From the left navigation menu select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal: Select PostgreSQL as Supported Database. Add the Database name. Add the Hostname. Add Port. Add PostgreSQL user. Add Password for PostgreSQL user. Click on CONNECT .","title":"Connect to database"},{"location":"datasources/postgresql/#create-new-datasource","text":"Click on the NEW DATASOURCE button. In the Datasource from DB integration modal: Add Datasource Name. Add Database name. Add SELECT Query e.g (SELECT * FROM my_database) Click on CREATE . That's all You have succesfully connected to PostgreSQL from MindsDB Studio. Next step is to train the Machine Learning model .","title":"Create new Datasource"},{"location":"datasources/postgresql/#psql-client","text":"Before using psql client to connect MindsDB and PostgreSQL Server you will need to add additional configuration before starting MindsDB Server. Create a new config.json file. Expand the example below to preview the configuration example. Configuration example { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.4\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"publish\" : true , \"host\" : \"localhost\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } All of the options that should be added to the config.json file are: The avaiable configuration options are: api['http] -- This key is used for starting the MindsDB http server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that works through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type in this case default_postgres . The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. publish(true|false) - Enable PostgreSQL integration. log['level'] -- The logging configuration(not required): console - \"INFO\", \"DEBUG\", \"ERROR\". file - Location of the log file. storage_dir -- The directory where mindsdb will store models and configuration. After creating the config.json file, you will need to start MindsDB and provide the path to the newly created config.json as: python3 -m mindsdb --api=http,mysql --config=config.json The --api parameter specifies the type of API to use in this case HTTP and MySQL. The --config specifies the location of the configuration file. If MindsDB is succesfully connected to your PostgreSQL database, it will create a new schema mindsdb and new table predictors . After starting the server, from your psql-client you can run SELECT query from it to make sure integration is succesfull. SELECT * FROM mindsdb . predictors ; That's all You have succesfully connected MindsDB Server and PostgreSQL. Next step is to train the Machine Learning model .","title":"PSQL client"},{"location":"datasources/remote/","text":"Connect to remote URL Connecting MindsDB to your remote data could be easly done with a few clicks. You can connect to your: S3 filestore Raw GitHub files Azure Blob storage files Google cloud storage Or, wherever your files are publicly accessible. You can upload any tabular data format as: CSV, TSV XLS/XLSX/ODC TXT JSON XML Upload from remote URL From the left navigation menu select Data dashboard. Click on the ADD BY URL button. In the New data source from URL modal: Add URL link to your data. Add name for the datasource. Click on UPLOAD . That's all You have succesfully connected to remote dataset from MindsDB Studio. Next step is to train the Machine Learning model .","title":"Remote URL"},{"location":"datasources/remote/#connect-to-remote-url","text":"Connecting MindsDB to your remote data could be easly done with a few clicks. You can connect to your: S3 filestore Raw GitHub files Azure Blob storage files Google cloud storage Or, wherever your files are publicly accessible. You can upload any tabular data format as: CSV, TSV XLS/XLSX/ODC TXT JSON XML","title":"Connect to remote URL"},{"location":"datasources/remote/#upload-from-remote-url","text":"From the left navigation menu select Data dashboard. Click on the ADD BY URL button. In the New data source from URL modal: Add URL link to your data. Add name for the datasource. Click on UPLOAD . That's all You have succesfully connected to remote dataset from MindsDB Studio. Next step is to train the Machine Learning model .","title":"Upload from remote URL"},{"location":"datasources/snowflake/","text":"Connect to Snowflake Data Warehouse Connecting MindsDB to the Snowflake could be easly done with a few clicks. Connect to Snowflake From the left navigation menu select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal: Select Snowflake as Supported Database. Add the Database name. Add the Hostname. Add Port. Add Account. Add Warehouse. Add Schema. Add protocol. Add User. Add Password for the user. Click on CONNECT . Create new Datasource Click on the NEW DATASOURCE button. In the Datasource from DB integration modal: Add Datasource Name. Add Database name. Add SELECT Query e.g (SELECT * FROM my_database). Click on CREATE . That's all You have succesfully connected to Snowflake from MindsDB Studio. Next step is to train the Machine Learning model .","title":"Snowflake"},{"location":"datasources/snowflake/#connect-to-snowflake-data-warehouse","text":"Connecting MindsDB to the Snowflake could be easly done with a few clicks.","title":"Connect to Snowflake Data Warehouse"},{"location":"datasources/snowflake/#connect-to-snowflake","text":"From the left navigation menu select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal: Select Snowflake as Supported Database. Add the Database name. Add the Hostname. Add Port. Add Account. Add Warehouse. Add Schema. Add protocol. Add User. Add Password for the user. Click on CONNECT .","title":"Connect to Snowflake"},{"location":"datasources/snowflake/#create-new-datasource","text":"Click on the NEW DATASOURCE button. In the Datasource from DB integration modal: Add Datasource Name. Add Database name. Add SELECT Query e.g (SELECT * FROM my_database). Click on CREATE . That's all You have succesfully connected to Snowflake from MindsDB Studio. Next step is to train the Machine Learning model .","title":"Create new Datasource"},{"location":"features/ColumnMetrics/","text":"@TODO: Outdated, pending refacotr","title":"ColumnMetrics"},{"location":"features/DataSources/","text":"Our goal is to make it very simple to ingest and prepare data that can be feed into MindsDB as DataSources . Here are the basics: mindsdb.learn from_data argument can be any of the following: file : can be a path in the local system, stringio object or a url to a file, supported types are (csv, json, xlsx, xls). data frame : A pandas DataFrame , pandas is one of the most useful data preparation libraries out there, so it makes sense that we support this. MindsDB data source : MindsDB has a special class for datasources which lends itself for simple data ingestion and preparation, as well as to combine various datasources to learn from. Note: By default, only the FileDS data source is available, to make sure the other data sources work, install mindsdb via pip install mindsdb[extra_data_sources] , or replace mindsdb to mindsdb[extra_data_sources] in whatever install instructions are given for your platform MindsDB data source: MindsDB datasource is an enriched version of a pandas dataframe so all methods in the pandas dataframe also apply to MindsDB, However, the DataSource class provides a way to implement data loading transformations and cleaning of data. Some special implementations of datasources that already do cleaning and various tasks: You can learn about them in mindsdb repository . FileDS An important one is the one MindsDB uses to work with files. from mindsdb import FileDS ds = FileDS ( file , clean_rows = True , custom_parser = None ) # Now you can pass this DataSource to MindsDB mdb . learn ( from_data = ds , predict = '<what column you want to predict>' , # the column we want to learn to predict given all the data in the file model_name = '<the name you want to give to this model>' # the name of this model ) FileDS arguments are: file can be a path in the local system, stringio object or a url to a file, supported types are (csv, json, xlsx, xls). clean_header (default=True) clean column names, so that they don't have special characters or white spaces. clean_rows (default=True) Goes row by row making sure that nulls are nulls and that no corrupt data exists in them, it also cleans empty rows custom_parser (default=None) special function to extract the actual table structure from the file in case you need special transformations. S3DS Use an s3 object as the input. from mindsdb import Predictor , S3DS s3_ds = S3DS ( bucket_name = 'mindsdb-example-data' , file_path = 'home_rentals.csv' ) ` Predictor ( name = 'test' ) . learn ( from_data = s3_ds , to_predict = 'target' ) This data source also takes the optional initialization/constructor arguments: access_key -- The AWS access key [string] secret_key -- The AWS secret key [string] use_default_credentails -- Whether or not to use the default credentials on your machine (e.g. the credentials inside ~/.aws or the role of the machine), defaults to False [boolean] MySqlDS Used to select data from MySQL or MariaDB . from mindsdb import Predictor , MySqlDS mysql_ds = MySqlDS ( query = \"SELECT COUNT(*), SUM(spend), SUM(is_click), website FROM advertising_data\" , user = \"my_user\" , password = \"my very secret password\" , database = \"main_db\" , table = 'advertising_data' , port = 3306 ) Predictor ( name = 'test' ) . learn ( from_data = mysql_ds , to_predict = 'target' ) This data source also takes the optional initialization/constructor arguments: query -- Query which extracts the data, mutually exclusive with table [string] host -- The host of the database (e.g. localhost or some ip) [string] user -- User for the database [string] password -- Password for the user [string] database -- Database to use [string] port -- Port of the database [integer] table -- Table from which to select all the data, mutually exclusive with query [string] PostgresDS Used to select data from PostgreSQL . from mindsdb import Predictor , PostgresDS pg_ds = PostgresDS ( query = \"SELECT COUNT(*), SUM(spend), SUM(is_click), website FROM advertising_data\" , user = \"my_user\" , password = \"my very secret password\" , database = \"main_db\" ) Predictor ( name = 'test' ) . learn ( from_data = pg_ds , to_predict = 'target' ) This data source also takes the optional initialization/constructor arguments: query -- Query which extracts the data, mutually exclusive with table [string] host -- The host of the database (e.g. localhost or some ip) [string] user -- User for the database [string] password -- Password for the user [string] database -- Database to use [string] port -- Port of the database [integer] table -- Table from which to select all the data, mutually exclusive with query [string] ClickhouseDS Used to select data from ClickHouse . from mindsdb import Predictor , ClickhouseDS ch_ds = ClickhouseDS ( query = \"SELECT COUNT(*), SUM(spend), SUM(is_click), website FROM default.advertising_data\" , user = \"my_user\" , password = \"my very secret password\" ) Predictor ( name = 'test' ) . learn ( from_data = ch_ds , to_predict = 'target' ) This data source also take the optional initialization/constructor arguments: query -- Query whith which to extract the data, mutually exclusive with table [string] host -- The host of the database (e.g. localhost or some ip) [string] user -- User for the database [string] password -- Password for the user [string] port -- Port of the database [integer] table -- Table from which to select all the data, mutually exclusive with query [string]","title":"Data sources"},{"location":"features/DataSources/#mindsdb-data-source","text":"MindsDB datasource is an enriched version of a pandas dataframe so all methods in the pandas dataframe also apply to MindsDB, However, the DataSource class provides a way to implement data loading transformations and cleaning of data. Some special implementations of datasources that already do cleaning and various tasks: You can learn about them in mindsdb repository .","title":"MindsDB data source:"},{"location":"features/DataSources/#fileds","text":"An important one is the one MindsDB uses to work with files. from mindsdb import FileDS ds = FileDS ( file , clean_rows = True , custom_parser = None ) # Now you can pass this DataSource to MindsDB mdb . learn ( from_data = ds , predict = '<what column you want to predict>' , # the column we want to learn to predict given all the data in the file model_name = '<the name you want to give to this model>' # the name of this model ) FileDS arguments are: file can be a path in the local system, stringio object or a url to a file, supported types are (csv, json, xlsx, xls). clean_header (default=True) clean column names, so that they don't have special characters or white spaces. clean_rows (default=True) Goes row by row making sure that nulls are nulls and that no corrupt data exists in them, it also cleans empty rows custom_parser (default=None) special function to extract the actual table structure from the file in case you need special transformations.","title":"FileDS"},{"location":"features/DataSources/#s3ds","text":"Use an s3 object as the input. from mindsdb import Predictor , S3DS s3_ds = S3DS ( bucket_name = 'mindsdb-example-data' , file_path = 'home_rentals.csv' ) ` Predictor ( name = 'test' ) . learn ( from_data = s3_ds , to_predict = 'target' ) This data source also takes the optional initialization/constructor arguments: access_key -- The AWS access key [string] secret_key -- The AWS secret key [string] use_default_credentails -- Whether or not to use the default credentials on your machine (e.g. the credentials inside ~/.aws or the role of the machine), defaults to False [boolean]","title":"S3DS"},{"location":"features/DataSources/#mysqlds","text":"Used to select data from MySQL or MariaDB . from mindsdb import Predictor , MySqlDS mysql_ds = MySqlDS ( query = \"SELECT COUNT(*), SUM(spend), SUM(is_click), website FROM advertising_data\" , user = \"my_user\" , password = \"my very secret password\" , database = \"main_db\" , table = 'advertising_data' , port = 3306 ) Predictor ( name = 'test' ) . learn ( from_data = mysql_ds , to_predict = 'target' ) This data source also takes the optional initialization/constructor arguments: query -- Query which extracts the data, mutually exclusive with table [string] host -- The host of the database (e.g. localhost or some ip) [string] user -- User for the database [string] password -- Password for the user [string] database -- Database to use [string] port -- Port of the database [integer] table -- Table from which to select all the data, mutually exclusive with query [string]","title":"MySqlDS"},{"location":"features/DataSources/#postgresds","text":"Used to select data from PostgreSQL . from mindsdb import Predictor , PostgresDS pg_ds = PostgresDS ( query = \"SELECT COUNT(*), SUM(spend), SUM(is_click), website FROM advertising_data\" , user = \"my_user\" , password = \"my very secret password\" , database = \"main_db\" ) Predictor ( name = 'test' ) . learn ( from_data = pg_ds , to_predict = 'target' ) This data source also takes the optional initialization/constructor arguments: query -- Query which extracts the data, mutually exclusive with table [string] host -- The host of the database (e.g. localhost or some ip) [string] user -- User for the database [string] password -- Password for the user [string] database -- Database to use [string] port -- Port of the database [integer] table -- Table from which to select all the data, mutually exclusive with query [string]","title":"PostgresDS"},{"location":"features/DataSources/#clickhouseds","text":"Used to select data from ClickHouse . from mindsdb import Predictor , ClickhouseDS ch_ds = ClickhouseDS ( query = \"SELECT COUNT(*), SUM(spend), SUM(is_click), website FROM default.advertising_data\" , user = \"my_user\" , password = \"my very secret password\" ) Predictor ( name = 'test' ) . learn ( from_data = ch_ds , to_predict = 'target' ) This data source also take the optional initialization/constructor arguments: query -- Query whith which to extract the data, mutually exclusive with table [string] host -- The host of the database (e.g. localhost or some ip) [string] user -- User for the database [string] password -- Password for the user [string] port -- Port of the database [integer] table -- Table from which to select all the data, mutually exclusive with query [string]","title":"ClickhouseDS"},{"location":"installation/Linux/","text":"Install using MindsDB installers Install MindsDB on your Linux machine using an easy-to-use shell script. Download the script MindsDB for Linux This script will install MindsDB and MindsDB's dependencies, and start the MindsDB server. Note that you need Python 3.6.x, 3.7.x or 3.8.x pre installed. Install using pip We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.6 and pip version >= 19.3 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages. Install using Anaconda You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list You should see a list with the names of installed packages. Installation fail Don't worry, simply follow the below bellow instruction which should fix most issues. Python 64 bit version is required. If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. If you are using Linux install tkinter from your package manager in certain situations. Ubuntu/Debian: sudo apt-get install python3-tk tk Fedora: sudo dnf -y install python3-tkinter Arch: sudo pacman -S tk If none of this works, try installing mindsdb using the docker container and create an issue with the installation errors you got on our Github repository and we'll try to review it within a few hours.","title":"Linux"},{"location":"installation/Linux/#install-using-mindsdb-installers","text":"Install MindsDB on your Linux machine using an easy-to-use shell script. Download the script MindsDB for Linux This script will install MindsDB and MindsDB's dependencies, and start the MindsDB server. Note that you need Python 3.6.x, 3.7.x or 3.8.x pre installed.","title":"Install using MindsDB installers"},{"location":"installation/Linux/#install-using-pip","text":"We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.6 and pip version >= 19.3 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages.","title":"Install using pip"},{"location":"installation/Linux/#install-using-anaconda","text":"You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list You should see a list with the names of installed packages. Installation fail Don't worry, simply follow the below bellow instruction which should fix most issues. Python 64 bit version is required. If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. If you are using Linux install tkinter from your package manager in certain situations. Ubuntu/Debian: sudo apt-get install python3-tk tk Fedora: sudo dnf -y install python3-tkinter Arch: sudo pacman -S tk If none of this works, try installing mindsdb using the docker container and create an issue with the installation errors you got on our Github repository and we'll try to review it within a few hours.","title":"Install using Anaconda"},{"location":"installation/MacOS/","text":"Install using MindsDB installers Install MindsDB on your MacOS machine using an easy-to-use shell script. Download the script MindsDB for MacOS This script will install MindsDB and MindsDB's dependencies, and start the MindsDB server. Note that you need Python 3.6.x, 3.7.x or 3.8.x pre installed. Install using Anaconda You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list You should see a list with the names of installed packages. Install using pip We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.6 and pip version >= 19.3 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages. Installation fail Don't worry, simply follow the below bellow instruction which should fix most issues. If you got numpy.distutils.system_info.NotFoundError: No lapack/blas resources found. Note: Accelerate is no longer supported. error when installing on macOS and you are using Python 3.9 please, downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. Python 64 bit version is required. If you are using macOS and got error about system dependencies, try installing MindsDB with Anaconda and run the installation from the anaconda prompt . If none of this works, try installing mindsdb using the docker container and create an issue with the installation errors you got on our Github repository and we'll try to review it within a few hours.","title":"MacOS"},{"location":"installation/MacOS/#install-using-mindsdb-installers","text":"Install MindsDB on your MacOS machine using an easy-to-use shell script. Download the script MindsDB for MacOS This script will install MindsDB and MindsDB's dependencies, and start the MindsDB server. Note that you need Python 3.6.x, 3.7.x or 3.8.x pre installed.","title":"Install using MindsDB installers"},{"location":"installation/MacOS/#install-using-anaconda","text":"You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list You should see a list with the names of installed packages.","title":"Install using Anaconda"},{"location":"installation/MacOS/#install-using-pip","text":"We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.6 and pip version >= 19.3 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages. Installation fail Don't worry, simply follow the below bellow instruction which should fix most issues. If you got numpy.distutils.system_info.NotFoundError: No lapack/blas resources found. Note: Accelerate is no longer supported. error when installing on macOS and you are using Python 3.9 please, downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. Python 64 bit version is required. If you are using macOS and got error about system dependencies, try installing MindsDB with Anaconda and run the installation from the anaconda prompt . If none of this works, try installing mindsdb using the docker container and create an issue with the installation errors you got on our Github repository and we'll try to review it within a few hours.","title":"Install using pip"},{"location":"installation/Windows/","text":"Install using MindsDB installers Install MindsDB on your Windows machine using an easy-to-use installler. Download the installer MindsDB for Windows This installer will install Python3, MindsDB and MindsDB's dependencies, and create a shortcut on the desktop for starting MindsDB server. Install using Anaconda You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list You should see a list with the names of installed packages. Install using pip We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.6 and pip version >= 19.3 . Create new virtual environment called mindsdb: py -m venv mindsdb And, activate it: .\\mindsdb\\Scripts\\activate.bat Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages. Installation fail Don't worry, simply follow the below bellow instruction which should fix most issues. Python 64 bit version is required. Depending on your environment, you might have to use pip3 instead of pip , and python3.x instead of py in the above commands e.g pip3 install mindsdb If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. If installation fails when installing torch or torchvision try manually installing them following the simple instructions on their official website . If you are using Windows, but are not using Anaconda or Conda, try installing one of them and running the installation from the anaconda prompt . If you've previously installed mindsdb and are having issues upgrading to a new version, try installing with the command: pip install mindsdb --upgrade If that still fails, try: pip install mindsdb --no-cache-dir --force-reinstall If none of this works, try installing mindsdb using the docker container and create an issue with the installation errors you got on our Github repository and we'll try to review it within a few hours.","title":"Windows"},{"location":"installation/Windows/#install-using-mindsdb-installers","text":"Install MindsDB on your Windows machine using an easy-to-use installler. Download the installer MindsDB for Windows This installer will install Python3, MindsDB and MindsDB's dependencies, and create a shortcut on the desktop for starting MindsDB server.","title":"Install using MindsDB installers"},{"location":"installation/Windows/#install-using-anaconda","text":"You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list You should see a list with the names of installed packages.","title":"Install using Anaconda"},{"location":"installation/Windows/#install-using-pip","text":"We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.6 and pip version >= 19.3 . Create new virtual environment called mindsdb: py -m venv mindsdb And, activate it: .\\mindsdb\\Scripts\\activate.bat Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages. Installation fail Don't worry, simply follow the below bellow instruction which should fix most issues. Python 64 bit version is required. Depending on your environment, you might have to use pip3 instead of pip , and python3.x instead of py in the above commands e.g pip3 install mindsdb If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. If installation fails when installing torch or torchvision try manually installing them following the simple instructions on their official website . If you are using Windows, but are not using Anaconda or Conda, try installing one of them and running the installation from the anaconda prompt . If you've previously installed mindsdb and are having issues upgrading to a new version, try installing with the command: pip install mindsdb --upgrade If that still fails, try: pip install mindsdb --no-cache-dir --force-reinstall If none of this works, try installing mindsdb using the docker container and create an issue with the installation errors you got on our Github repository and we'll try to review it within a few hours.","title":"Install using pip"},{"location":"installation/docker/","text":"Install with Docker You can run MindsDB in a docker container assuming that you have docker installed in your machine. To make sure Docker is successfully installed on your machine run: docker run hello-world You should see the Hello from Docker! message displayed. If not, check the get started documentation. Run MindsDB container MindsDB images are uploaded to MindsDB repo on docker hub after each release. Pull image First, run the bellow command to pull our latest production image: docker pull mindsdb/mindsdb Or, to try out the latest beta version pull the beta image: docker pull mindsdb/mindsdb_beta Start container Next, run the bellow command to start the container: docker run -p 47334:47334 mindsdb/mindsdb That's all. MindsDB should automaticaly start the Studio on your default browser. Publish ports Note that, you must publish a container\u2019s port to the host -p 47334:47334 which is used by MindsDB GUI and HTTTP API. Also, to use MindsDB MySQL API or MongoDB API publish -p 47335:47335 -p 47336:47336 ports too.","title":"Docker"},{"location":"installation/docker/#install-with-docker","text":"You can run MindsDB in a docker container assuming that you have docker installed in your machine. To make sure Docker is successfully installed on your machine run: docker run hello-world You should see the Hello from Docker! message displayed. If not, check the get started documentation.","title":"Install with Docker"},{"location":"installation/docker/#run-mindsdb-container","text":"MindsDB images are uploaded to MindsDB repo on docker hub after each release.","title":"Run MindsDB container"},{"location":"installation/docker/#pull-image","text":"First, run the bellow command to pull our latest production image: docker pull mindsdb/mindsdb Or, to try out the latest beta version pull the beta image: docker pull mindsdb/mindsdb_beta","title":"Pull image"},{"location":"installation/docker/#start-container","text":"Next, run the bellow command to start the container: docker run -p 47334:47334 mindsdb/mindsdb That's all. MindsDB should automaticaly start the Studio on your default browser. Publish ports Note that, you must publish a container\u2019s port to the host -p 47334:47334 which is used by MindsDB GUI and HTTTP API. Also, to use MindsDB MySQL API or MongoDB API publish -p 47335:47335 -p 47336:47336 ports too.","title":"Start container"},{"location":"installation/source/","text":"Install MindsDB froum source This section describes how to install MindsDB from source. This is a prefered way to install MindsDB in case you want to contribute to our code or simply debug the MindsDB. Prerequisite Python version >=3.6 (64 bit) and pip version >= 19.3. Pip (usually is pre installed with latest Python versions). Git . Installation We recommend to install MindsDB inside virtual environment to avoid dependency issues. Clone the repository: git clone git@github.com:mindsdb/mindsdb.git Create virtual environment and activate it: python3 -m venv mindsdb source mindsdb/bin/activate Install requirements: cd mindsdb && pip install -r requirements.txt Install MindsDB: python setup.py develop You're done! To check if everything works, start MindsDB server: python -m mindsdb To access MindsDB APIs visit http://127.0.0.1:47334/api . To access MindsDB Studio visit http://127.0.0.1:47334/ Install troubleshooting No module named mindsdb If you got this error, make sure that your virtual environment is activated. ImportError: No module named {dependency name} In case you skipped the 3th step, simmilar error can happen. Make sure that you install all of the MindsDB requirements. This site can\u2019t be reached. 127.0.0.1 refused to connect. Please check the MindsDB server console in case the server is still in starting phase. If server started and there is some error displayed, please report that on our GitHub .","title":"From source"},{"location":"installation/source/#install-mindsdb-froum-source","text":"This section describes how to install MindsDB from source. This is a prefered way to install MindsDB in case you want to contribute to our code or simply debug the MindsDB.","title":"Install MindsDB froum source"},{"location":"installation/source/#prerequisite","text":"Python version >=3.6 (64 bit) and pip version >= 19.3. Pip (usually is pre installed with latest Python versions). Git .","title":"Prerequisite"},{"location":"installation/source/#installation","text":"We recommend to install MindsDB inside virtual environment to avoid dependency issues. Clone the repository: git clone git@github.com:mindsdb/mindsdb.git Create virtual environment and activate it: python3 -m venv mindsdb source mindsdb/bin/activate Install requirements: cd mindsdb && pip install -r requirements.txt Install MindsDB: python setup.py develop You're done! To check if everything works, start MindsDB server: python -m mindsdb To access MindsDB APIs visit http://127.0.0.1:47334/api . To access MindsDB Studio visit http://127.0.0.1:47334/","title":"Installation"},{"location":"installation/source/#install-troubleshooting","text":"No module named mindsdb If you got this error, make sure that your virtual environment is activated. ImportError: No module named {dependency name} In case you skipped the 3th step, simmilar error can happen. Make sure that you install all of the MindsDB requirements. This site can\u2019t be reached. 127.0.0.1 refused to connect. Please check the MindsDB server console in case the server is still in starting phase. If server started and there is some error displayed, please report that on our GitHub .","title":"Install troubleshooting"},{"location":"integrations/AwsSageMaker/","text":"In the next sections, you can find general informations on how to build MindsDB container image, train models, deploy them on SageMaker and get the predictions. The code can be found inside the mindsdb-sagemaker-container GitHub repository. Training and Inference flow on SageMaker","title":"Amazon SageMaker"},{"location":"integrations/AwsSageMaker/#training-and-inference-flow-on-sagemaker","text":"","title":"Training and Inference flow on SageMaker"},{"location":"integrations/CallSageMakerEndpoint/","text":"To call the SageMaker endpint you can create Jupyter Notebook or use call.py script. Jupyter Notebook import boto3 endpointName = 'mindsdb-impl' # load test dataset with open ( 'diabetest-test.csv' , 'r' ) as reader : payload = reader . read () # Talk to SageMaker client = boto3 . client ( 'sagemaker-runtime' ) response = client . invoke_endpoint ( EndpointName = endpointName , Body = payload , ContentType = text / csv , Accept = 'Accept' ) print ( response [ 'Body' ] . read () . decode ( 'ascii' )) Run the code and you should see the prediction response from the endpoint: { \"prediction\" : \"* We are 96% confident the value of \" Class \" is positive.\" , \"class_confidence\" : [ 0.964147493532568 ] } call.py Script The required arguments are: endpoint - The name of the SageMaker endpoint. dataset - The location of test dataset. content type - The mime type of the data. python3 call . py -- endpoint mindsdb - impl -- dataset test_data / diabetes - test . json -- content - type application / json","title":"Call SageMaker Endpoint"},{"location":"integrations/CallSageMakerEndpoint/#jupyter-notebook","text":"import boto3 endpointName = 'mindsdb-impl' # load test dataset with open ( 'diabetest-test.csv' , 'r' ) as reader : payload = reader . read () # Talk to SageMaker client = boto3 . client ( 'sagemaker-runtime' ) response = client . invoke_endpoint ( EndpointName = endpointName , Body = payload , ContentType = text / csv , Accept = 'Accept' ) print ( response [ 'Body' ] . read () . decode ( 'ascii' )) Run the code and you should see the prediction response from the endpoint: { \"prediction\" : \"* We are 96% confident the value of \" Class \" is positive.\" , \"class_confidence\" : [ 0.964147493532568 ] }","title":"Jupyter Notebook"},{"location":"integrations/CallSageMakerEndpoint/#callpy-script","text":"The required arguments are: endpoint - The name of the SageMaker endpoint. dataset - The location of test dataset. content type - The mime type of the data. python3 call . py -- endpoint mindsdb - impl -- dataset test_data / diabetes - test . json -- content - type application / json","title":"call.py Script"},{"location":"integrations/MindsDBSageContainer/","text":"The general flow is to train and deploy models within SageMaker, create endpoints and take advantage of automated machine learning with MindsDB. The mindsdb_impl Code Structure All of the components we need to package MindsDB for Amazon SageMager are located inside the mindsdb_impl directory: |-- Dockerfile |-- build_and_push.sh `-- mindsdb_impl |-- nginx.conf |-- predictor.py |-- serve |-- train `-- wsgi.py All of the files that will be packaged in the container working directory are inside mindsdb_impl: * nginx.conf - is the configuration file for the nginx . * predictor.py is the program that actually implements the Flask web server and the MindsDB predictions for this app. We have modified this to use the MindsDB Predictor and to accept different types of tabular data for predictions. * serve is the program that is started when the container is started for hosting. * train is the program that is invoked when the container is being run for training. We have modified this program to use MindsDB Predictor interface. * wsgi.py is a small wrapper used to invoke the Flask app. Build Docker Image The docker command for building the image is build and -t parameter provides the name of the image: docker build -t mindsdb-impl . After getting the Successfully built message, we should be able to list the image by running the list command: docker image list Test the Container Locally The local_test directory contains all of the scripts and data samples for testing the built container on the local machine. train_local.sh: Instantiate the container configured for training. serve_local.sh: Instantiate the container configured for serving. predict.sh: Run predictions against a locally instantiated server. test-dir: This directory is mounted in the container. test_data: This directory contains a few tabular format datasets used for getting the predictions. input/data/training/file.csv`: The training data. model: The directory where MindsDB writes the model files. call.py: This cli script can be used for testing the deployed model on the SageMaker endpoint To train the model execute: ./train_local.sh mindsdb-impl Next, start the inference server that will provide an endpoint for getting the predictions. Inside the local_test directory execute serve_local.sh script: ./serve_local.sh mindsdb-impl To run predictions against the invocations endpoint, use predict.sh script: ./predict.sh test_data/diabetest-test.csv text/csv The arguments sent to the script are the test data and content type of the data. Deploy the Image on Amazon ECR (Elastic Container Repository) To push the image use build-and-push.sh script. Note that the script will look for an AWS EC Repository in the default region that you are using, and create a new one if that doesn't exist. ./build-and-push.sh mindsdb-impl After the mindsdb-impl image is deployed to Amazon ECR, you can use it inside SageMaker.","title":"MindsDB container"},{"location":"integrations/MindsDBSageContainer/#the-mindsdb_impl-code-structure","text":"All of the components we need to package MindsDB for Amazon SageMager are located inside the mindsdb_impl directory: |-- Dockerfile |-- build_and_push.sh `-- mindsdb_impl |-- nginx.conf |-- predictor.py |-- serve |-- train `-- wsgi.py All of the files that will be packaged in the container working directory are inside mindsdb_impl: * nginx.conf - is the configuration file for the nginx . * predictor.py is the program that actually implements the Flask web server and the MindsDB predictions for this app. We have modified this to use the MindsDB Predictor and to accept different types of tabular data for predictions. * serve is the program that is started when the container is started for hosting. * train is the program that is invoked when the container is being run for training. We have modified this program to use MindsDB Predictor interface. * wsgi.py is a small wrapper used to invoke the Flask app.","title":"The mindsdb_impl Code Structure"},{"location":"integrations/MindsDBSageContainer/#build-docker-image","text":"The docker command for building the image is build and -t parameter provides the name of the image: docker build -t mindsdb-impl . After getting the Successfully built message, we should be able to list the image by running the list command: docker image list","title":"Build Docker Image"},{"location":"integrations/MindsDBSageContainer/#test-the-container-locally","text":"The local_test directory contains all of the scripts and data samples for testing the built container on the local machine. train_local.sh: Instantiate the container configured for training. serve_local.sh: Instantiate the container configured for serving. predict.sh: Run predictions against a locally instantiated server. test-dir: This directory is mounted in the container. test_data: This directory contains a few tabular format datasets used for getting the predictions. input/data/training/file.csv`: The training data. model: The directory where MindsDB writes the model files. call.py: This cli script can be used for testing the deployed model on the SageMaker endpoint To train the model execute: ./train_local.sh mindsdb-impl Next, start the inference server that will provide an endpoint for getting the predictions. Inside the local_test directory execute serve_local.sh script: ./serve_local.sh mindsdb-impl To run predictions against the invocations endpoint, use predict.sh script: ./predict.sh test_data/diabetest-test.csv text/csv The arguments sent to the script are the test data and content type of the data.","title":"Test the Container Locally"},{"location":"integrations/MindsDBSageContainer/#deploy-the-image-on-amazon-ecr-elastic-container-repository","text":"To push the image use build-and-push.sh script. Note that the script will look for an AWS EC Repository in the default region that you are using, and create a new one if that doesn't exist. ./build-and-push.sh mindsdb-impl After the mindsdb-impl image is deployed to Amazon ECR, you can use it inside SageMaker.","title":"Deploy the Image on Amazon ECR (Elastic Container Repository)"},{"location":"integrations/SageMakerSDK/","text":"The following section explaines how to train and host models using Amazon SageMaker SDK . Add dependencies Install SageMaker SDK : pip install sagemaker First, add IAM Role that have AmazonSageMakerFullAccess Policy. import sagemaker as sage role = \"arn:aws:iam::123213143532:role/service-role/AmazonSageMaker-ExecutionRole-20199\" sess = sage . Session () account = sess . boto_session . client ( 'sts' ) . get_caller_identity ()[ 'Account' ] Next, provide s3 bucket where the models will be saved, get aws region from session and add URI to MindsDB image in AWS ECR: bucket_path = \"s3://mdb-sagemaker/models/\" region = sess . boto_session . region_name image = ' {} .dkr.ecr. {} .amazonaws.com/mindsdb_lts:latest' . format ( account , region ) Start training The required properties for invoking SageMaker training using Estimator are: The image name(str) -- The MindsDB container URI on ECR. The role(str) -- AWS arn with SageMaker execution role. The instance count(int) -- The number of machines to use for training. The instance type(str) -- The type of machine to use for training. The output path(str) -- Path to the s3 bucket where the model artifact will be saved. The session(sagemaker.session.Session) -- The SageMaker session object that we\u2019ve defined in the code above. The base job name(str) -- The name of the training job. The hyperparameters(dict) -- The MindsDB container requires to_predict value so it knows which column to predict. mindsdb_impl = sage . estimator . Estimator ( image , role , 1 , 'ml.m4.xlarge' , output_path = bucket_path , sagemaker_session = sess , base_job_name = \"mindsdb-lts-sdk\" , hyperparameters = { \"to_predict\" : \"Class\" }) Deploy the model The required configuration for deploying model: initial_instance_count (int) -- The initial number of instances to run in the Endpoint created from this Model. instance_type (str) -- The EC2 instance type to deploy this Model to. For example, \u2018ml.p2.xlarge\u2019, or \u2018local\u2019 for local mode. endpoint_name (str) -- The name of the endpoint on SageMaker. dataset_location = 's3://mdb-sagemaker/diabetes.csv' mindsdb_impl . fit ( dataset_location ) predictor = mindsdb_impl . deploy ( 1 , 'ml.m4.xlarge' , endpoint_name = 'mindsdb-impl' ) ``` ### Make a prediction Load the test dataset and then call the Predictor predict method with test data . ``` python with open ( 'test_data/diabetes-test.csv' , 'r' ) as reader : when_data = reader . read () print ( predictor . predict ( when_data ) . decode ( 'utf-8' )) Delete the endpoint Don't forget to delete the endpoint after using it. sess . delete_endpoint ( 'mindsdb-impl' ) Full code example import sagemaker as sage # Add AmazonSageMaker Execution role here role = \"arn:aws:iam:\" sess = sage . Session () account = sess . boto_session . client ( 'sts' ) . get_caller_identity ()[ 'Account' ] bucket_path = \"s3://mdb-sagemaker/models/\" region = sess . boto_session . region_name image = ' {} .dkr.ecr. {} .amazonaws.com/mindsdb_lts:latest' . format ( account , region ) #Hyperparameters to_predict is required for MindsDB container mindsdb_impl = sage . estimator . Estimator ( image , role , 1 , 'ml.m4.xlarge' , output_path = bucket_path , sagemaker_session = sess , base_job_name = \"mindsdb-lts-sdk\" , hyperparameters = { \"to_predict\" : \"Class\" }) dataset_location = 's3://mdb-sagemaker/diabetes.csv' mindsdb_impl . fit ( dataset_location ) predictor = mindsdb_impl . deploy ( 1 , 'ml.m4.xlarge' , endpoint_name = 'mindsdb-impl' ) with open ( 'test_data/diabetes-test.csv' , 'r' ) as reader : when_data = reader . read () print ( predictor . predict ( when_data ) . decode ( 'utf-8' ))","title":"Train and host MindsDB Models using SageMaker SDK"},{"location":"integrations/SageMakerSDK/#add-dependencies","text":"Install SageMaker SDK : pip install sagemaker First, add IAM Role that have AmazonSageMakerFullAccess Policy. import sagemaker as sage role = \"arn:aws:iam::123213143532:role/service-role/AmazonSageMaker-ExecutionRole-20199\" sess = sage . Session () account = sess . boto_session . client ( 'sts' ) . get_caller_identity ()[ 'Account' ] Next, provide s3 bucket where the models will be saved, get aws region from session and add URI to MindsDB image in AWS ECR: bucket_path = \"s3://mdb-sagemaker/models/\" region = sess . boto_session . region_name image = ' {} .dkr.ecr. {} .amazonaws.com/mindsdb_lts:latest' . format ( account , region )","title":"Add dependencies"},{"location":"integrations/SageMakerSDK/#start-training","text":"The required properties for invoking SageMaker training using Estimator are: The image name(str) -- The MindsDB container URI on ECR. The role(str) -- AWS arn with SageMaker execution role. The instance count(int) -- The number of machines to use for training. The instance type(str) -- The type of machine to use for training. The output path(str) -- Path to the s3 bucket where the model artifact will be saved. The session(sagemaker.session.Session) -- The SageMaker session object that we\u2019ve defined in the code above. The base job name(str) -- The name of the training job. The hyperparameters(dict) -- The MindsDB container requires to_predict value so it knows which column to predict. mindsdb_impl = sage . estimator . Estimator ( image , role , 1 , 'ml.m4.xlarge' , output_path = bucket_path , sagemaker_session = sess , base_job_name = \"mindsdb-lts-sdk\" , hyperparameters = { \"to_predict\" : \"Class\" })","title":"Start training"},{"location":"integrations/SageMakerSDK/#deploy-the-model","text":"The required configuration for deploying model: initial_instance_count (int) -- The initial number of instances to run in the Endpoint created from this Model. instance_type (str) -- The EC2 instance type to deploy this Model to. For example, \u2018ml.p2.xlarge\u2019, or \u2018local\u2019 for local mode. endpoint_name (str) -- The name of the endpoint on SageMaker. dataset_location = 's3://mdb-sagemaker/diabetes.csv' mindsdb_impl . fit ( dataset_location ) predictor = mindsdb_impl . deploy ( 1 , 'ml.m4.xlarge' , endpoint_name = 'mindsdb-impl' ) ``` ### Make a prediction Load the test dataset and then call the Predictor predict method with test data . ``` python with open ( 'test_data/diabetes-test.csv' , 'r' ) as reader : when_data = reader . read () print ( predictor . predict ( when_data ) . decode ( 'utf-8' ))","title":"Deploy the model"},{"location":"integrations/SageMakerSDK/#delete-the-endpoint","text":"Don't forget to delete the endpoint after using it. sess . delete_endpoint ( 'mindsdb-impl' )","title":"Delete the endpoint"},{"location":"integrations/SageMakerSDK/#full-code-example","text":"import sagemaker as sage # Add AmazonSageMaker Execution role here role = \"arn:aws:iam:\" sess = sage . Session () account = sess . boto_session . client ( 'sts' ) . get_caller_identity ()[ 'Account' ] bucket_path = \"s3://mdb-sagemaker/models/\" region = sess . boto_session . region_name image = ' {} .dkr.ecr. {} .amazonaws.com/mindsdb_lts:latest' . format ( account , region ) #Hyperparameters to_predict is required for MindsDB container mindsdb_impl = sage . estimator . Estimator ( image , role , 1 , 'ml.m4.xlarge' , output_path = bucket_path , sagemaker_session = sess , base_job_name = \"mindsdb-lts-sdk\" , hyperparameters = { \"to_predict\" : \"Class\" }) dataset_location = 's3://mdb-sagemaker/diabetes.csv' mindsdb_impl . fit ( dataset_location ) predictor = mindsdb_impl . deploy ( 1 , 'ml.m4.xlarge' , endpoint_name = 'mindsdb-impl' ) with open ( 'test_data/diabetes-test.csv' , 'r' ) as reader : when_data = reader . read () print ( predictor . predict ( when_data ) . decode ( 'utf-8' ))","title":"Full code example"},{"location":"integrations/UseMindsDBinSage/","text":"The following section explains how to train and host models using Amazon SageMaker console . Create Train Job Follow the steps below to successfully start a train job and use MindsDB to create the models: 1. Open the Amazon SageMaker console at https://console.aws.amazon.com/sagemaker/. From the left panel choose Create Training Job and provide the following information Job name IAM role - it\u2019s best if you provide AmazonSageMakeFullAccess IAM policy Algorithm source - Your own algorithm container in ECR Provide container ECR path Container - the ECR registry Image URI that we have pushed Input mode - File Resource configuration - leave the default instance type and count Hyperparameters - MindsDB requires to_predict column name, so it knows which column we want to predict, e.g. Key - to_predict Value - Class(the column in diabetes dataset) Input data configuration Channel name - training Data source - s3 S3 location - path to the s3 bucket where the dataset is located 7.Output data configuration - path to the s3 where the models will be saved Model creation Create model and add the required settings: 1. Model name - must be unique IAM role - it\u2019s best if you provide AmazonSageMakeFullAccess IAM policy Container input options Provide model artifacts and inference image location Use a single model Location of the inference code - 846763053924.dkr.ecr.us-east-1.amazonaws.com/mindsdb_impl:latest Location of model artifacts - path to model.tar.gz inside s3 bucket. Endpoint configuration In the endpoint configuration, add the models to deploy, and the hardware requirements: Endpoint configuration name. Add model - select the previously created model. Choose Create endpoint configuration. Create endpoint The last step is to create endpoint and provide endpoint configuration that specify which models to deploy and the requirements: Endpoint name. Attach endpoint configuration - select the previously created endpoint configuration. Choose Create endpoint. After finishing the above steps, SageMaker will create a new instance and start the inference code.","title":"Train and host MindsDB models"},{"location":"integrations/UseMindsDBinSage/#create-train-job","text":"Follow the steps below to successfully start a train job and use MindsDB to create the models: 1. Open the Amazon SageMaker console at https://console.aws.amazon.com/sagemaker/. From the left panel choose Create Training Job and provide the following information Job name IAM role - it\u2019s best if you provide AmazonSageMakeFullAccess IAM policy Algorithm source - Your own algorithm container in ECR Provide container ECR path Container - the ECR registry Image URI that we have pushed Input mode - File Resource configuration - leave the default instance type and count Hyperparameters - MindsDB requires to_predict column name, so it knows which column we want to predict, e.g. Key - to_predict Value - Class(the column in diabetes dataset) Input data configuration Channel name - training Data source - s3 S3 location - path to the s3 bucket where the dataset is located 7.Output data configuration - path to the s3 where the models will be saved","title":"Create Train Job"},{"location":"integrations/UseMindsDBinSage/#model-creation","text":"Create model and add the required settings: 1. Model name - must be unique IAM role - it\u2019s best if you provide AmazonSageMakeFullAccess IAM policy Container input options Provide model artifacts and inference image location Use a single model Location of the inference code - 846763053924.dkr.ecr.us-east-1.amazonaws.com/mindsdb_impl:latest Location of model artifacts - path to model.tar.gz inside s3 bucket.","title":"Model creation"},{"location":"integrations/UseMindsDBinSage/#endpoint-configuration","text":"In the endpoint configuration, add the models to deploy, and the hardware requirements: Endpoint configuration name. Add model - select the previously created model. Choose Create endpoint configuration.","title":"Endpoint configuration"},{"location":"integrations/UseMindsDBinSage/#create-endpoint","text":"The last step is to create endpoint and provide endpoint configuration that specify which models to deploy and the requirements: Endpoint name. Attach endpoint configuration - select the previously created endpoint configuration. Choose Create endpoint. After finishing the above steps, SageMaker will create a new instance and start the inference code.","title":"Create endpoint"},{"location":"lightwood/API/","text":"Predictor API from lightwood import Predictor Lightwood has one main class; The Predictor , which is a modular construct that you can train and get predictions from. It is made out of 3 main building blocks ( features, encoders, mixers ) that you can configure, modify and expand as you wish. Building blocks Features : input_features : These are the columns in your dataset that you want to take as input for your predictor. output_features : These are the columns in your dataset that you want to learn how to predict. Encoders : These are tools to turn the data in your input or output features into vector/tensor representations and vice-versa. Mixers : How you mix the output of encoded features and also other mixers Constructor, __init__() my_predictor = Predictor ( output = [] | config = { ... } | load_from_path =< file_path > ) Predictor, can take any of the following arguments load_from_path : If you have a saved predictor that you want to load, just give the path to the file output : A list with the column names you want to predict. ( Note: If you pass this argument, lightwood will simply try to guess the best config possible ) config : A dictionary, containing the configuration on how to glue all the building blocks. config The config argument allows you to pass a dictionary that defines and gives you absolute control over how to build your predictive model. A config example goes as follows: from lightwood import COLUMN_DATA_TYPES , BUILTIN_MIXERS , BUILTIN_ENCODERS config = { ## REQUIRED: 'input_features' : [ # by default each feature has an encoder, so all you have to do is specify the data type { 'name' : 'sensor1' , 'type' : COLUMN_DATA_TYPES . NUMERIC }, { 'name' : 'sensor2' , 'type' : COLUMN_DATA_TYPES . NUMERIC }, # some encoders have attributes that can be specified on the configuration # in this particular lets assume we have a photo of the product, we would like to encode this image and optimize for speed { 'name' : 'product_photo' , 'type' : COLUMN_DATA_TYPES . IMAGE , 'encoder_class' : BUILTIN_ENCODERS . Image . Img2VecEncoder , # note that this is just a class, you can build your own if you wish 'encoder_attrs' : { 'aim' : 'speed' # you can check the encoder attributes here: # https://github.com/mindsdb/lightwood/blob/master/lightwood/encoders/image/img_2_vec.py } } ], 'output_features' : [ { 'name' : 'action_to_take' , 'type' : COLUMN_DATA_TYPES . CATEGORICAL } ], ## OPTIONAL 'mixer' : { 'class' : BUILTIN_MIXERS . NnMixer } } features Both input_features and output_features configs are simple dicts that have the following schema name : is the name of the column as it is in the input data frame type : is the type of data contained. Where out of the box, supported COLUMN_DATA_TYPES are NUMERIC, CATEGORICAL, DATETIME, IMAGE, TEXT, TIME_SERIES : If you specify the type, lightwood will use the default encoder for that type, however, you can specify/define any encoder that you want to use. encoder_class : This is if you want to replace the default encoder with a different one, so you put the encoder class there encoder_attrs : These are the attributes that you want to setup on the encoder once the class its initialized mixer The default_mixer key, provides information as to what mixer to use. The schema for this variable is as follows: mixer_schema = Schema ({ 'class' : object , Optional ( 'attrs' ): dict }) class : Its the actual class, that defines the Mixer, you can use any of the BUILTIN_MIXERS or pass your own. attrs : This is a dictionary containing the attributes you want to replace on the mixer object once its initialized. We do this, so you have maximum flexibility as to what you can customize on your Mixers. learn() This method is used to make the predictor learn from some data, thus the learn method takes the following arguments. from_data : A pandas dataframe, that has some or all the columns in the config. The reason why we decide to only suppor pandas dataframes, its because, its easy to load any data to a pandas draframe, and spark for python dataframe is a format we support. test_data : (Optional) This is if you want to specify what data to test with, if no test_data passed, lightwood will break the from_data into test and train automatically. callback_on_iter : (Optional) This is function callback that is called every 100 epocs the during the learn process. predict() This method is used to make predictions and it can take one of the following arguments when : this is a dictionary of conditions to predict under. when_data : Sometimes you want to predict more than one row a ta time, so here it is: a pandas dataframe containing the conditional values you want to use to make a prediction. save() Use this method to save the predictor into a desired path calculate_accuracy() Returns the predictors overall accuracy.","title":"Predictor API"},{"location":"lightwood/API/#predictor-api","text":"from lightwood import Predictor Lightwood has one main class; The Predictor , which is a modular construct that you can train and get predictions from. It is made out of 3 main building blocks ( features, encoders, mixers ) that you can configure, modify and expand as you wish. Building blocks Features : input_features : These are the columns in your dataset that you want to take as input for your predictor. output_features : These are the columns in your dataset that you want to learn how to predict. Encoders : These are tools to turn the data in your input or output features into vector/tensor representations and vice-versa. Mixers : How you mix the output of encoded features and also other mixers","title":"Predictor API"},{"location":"lightwood/API/#constructor-__init__","text":"my_predictor = Predictor ( output = [] | config = { ... } | load_from_path =< file_path > ) Predictor, can take any of the following arguments load_from_path : If you have a saved predictor that you want to load, just give the path to the file output : A list with the column names you want to predict. ( Note: If you pass this argument, lightwood will simply try to guess the best config possible ) config : A dictionary, containing the configuration on how to glue all the building blocks.","title":"Constructor, __init__()"},{"location":"lightwood/API/#config","text":"The config argument allows you to pass a dictionary that defines and gives you absolute control over how to build your predictive model. A config example goes as follows: from lightwood import COLUMN_DATA_TYPES , BUILTIN_MIXERS , BUILTIN_ENCODERS config = { ## REQUIRED: 'input_features' : [ # by default each feature has an encoder, so all you have to do is specify the data type { 'name' : 'sensor1' , 'type' : COLUMN_DATA_TYPES . NUMERIC }, { 'name' : 'sensor2' , 'type' : COLUMN_DATA_TYPES . NUMERIC }, # some encoders have attributes that can be specified on the configuration # in this particular lets assume we have a photo of the product, we would like to encode this image and optimize for speed { 'name' : 'product_photo' , 'type' : COLUMN_DATA_TYPES . IMAGE , 'encoder_class' : BUILTIN_ENCODERS . Image . Img2VecEncoder , # note that this is just a class, you can build your own if you wish 'encoder_attrs' : { 'aim' : 'speed' # you can check the encoder attributes here: # https://github.com/mindsdb/lightwood/blob/master/lightwood/encoders/image/img_2_vec.py } } ], 'output_features' : [ { 'name' : 'action_to_take' , 'type' : COLUMN_DATA_TYPES . CATEGORICAL } ], ## OPTIONAL 'mixer' : { 'class' : BUILTIN_MIXERS . NnMixer } }","title":"config"},{"location":"lightwood/API/#features","text":"Both input_features and output_features configs are simple dicts that have the following schema name : is the name of the column as it is in the input data frame type : is the type of data contained. Where out of the box, supported COLUMN_DATA_TYPES are NUMERIC, CATEGORICAL, DATETIME, IMAGE, TEXT, TIME_SERIES : If you specify the type, lightwood will use the default encoder for that type, however, you can specify/define any encoder that you want to use. encoder_class : This is if you want to replace the default encoder with a different one, so you put the encoder class there encoder_attrs : These are the attributes that you want to setup on the encoder once the class its initialized","title":"features"},{"location":"lightwood/API/#mixer","text":"The default_mixer key, provides information as to what mixer to use. The schema for this variable is as follows: mixer_schema = Schema ({ 'class' : object , Optional ( 'attrs' ): dict }) class : Its the actual class, that defines the Mixer, you can use any of the BUILTIN_MIXERS or pass your own. attrs : This is a dictionary containing the attributes you want to replace on the mixer object once its initialized. We do this, so you have maximum flexibility as to what you can customize on your Mixers.","title":"mixer"},{"location":"lightwood/API/#learn","text":"This method is used to make the predictor learn from some data, thus the learn method takes the following arguments. from_data : A pandas dataframe, that has some or all the columns in the config. The reason why we decide to only suppor pandas dataframes, its because, its easy to load any data to a pandas draframe, and spark for python dataframe is a format we support. test_data : (Optional) This is if you want to specify what data to test with, if no test_data passed, lightwood will break the from_data into test and train automatically. callback_on_iter : (Optional) This is function callback that is called every 100 epocs the during the learn process.","title":"learn()"},{"location":"lightwood/API/#predict","text":"This method is used to make predictions and it can take one of the following arguments when : this is a dictionary of conditions to predict under. when_data : Sometimes you want to predict more than one row a ta time, so here it is: a pandas dataframe containing the conditional values you want to use to make a prediction.","title":"predict()"},{"location":"lightwood/API/#save","text":"Use this method to save the predictor into a desired path","title":"save()"},{"location":"lightwood/API/#calculate_accuracy","text":"Returns the predictors overall accuracy.","title":"calculate_accuracy()"},{"location":"lightwood/info/","text":"Lightwood is a Pytorch based framework with two objectives: Make it so simple that you can build predictive models with a line of code. Make it so flexible that you can change and customize everything. Lightwood was inspired on Keras + Ludwig but runs on Pytorch and gives you full control of what you can do. Prerequisites Python >=3.6 64bit version Installing Lightwood You can install Lightwood using pip : pip3 install lightwood If this fails, please report the bug on github and try installing the current master branch: git clone git@github.com:mindsdb/lightwood.git ; cd lightwood ; pip install --no-cache-dir -e . Please note that, depending on your os and python setup, you might want to use pip instead of pip3 . You need python 3.6 or higher. Note on MacOS, you need to install libomp: brew install libomp Install using virtual environment We suggest you to install Lightwood on a virtual environment to avoid dependency issues. Make sure your Python version is >=3.6. To set up a virtual environment: Install on Windows Install the latest version of pip : python -m pip install --upgrade pip pip --version Activate your virtual environment and install lightwood: py -m pip install --user virtualenv . \\e nv \\S cripts \\a ctivate pip install lightwood You can also use python instead of py Install on Linux or macOS Before installing Lightwood in a virtual environment you need to first create and activate the venv : python -m venv env source env/bin/activate pip install lightwood Quick example Assume that you have a training file (sensor_data.csv) such as this one. sensor1 sensor2 sensor3 1 -1 -1 0 1 0 -1 -1 1 1 0 0 0 1 0 -1 1 -1 0 0 0 -1 -1 1 1 0 0 And you would like to learn to predict the values of sensor3 given the readings in sensor1 and sensor2 . Learn You can train a Predictor as follows: from lightwood import Predictor import pandas sensor3_predictor = Predictor ( output = [ 'sensor3' ]) sensor3_predictor . learn ( from_data = pandas . read_csv ( 'sensor_data.csv' )) Predict You can now be given new readings from sensor1 and sensor2 predict what sensor3 will be. prediction = sensor3_predictor . predict ( when = { 'sensor1' : 1 , 'sensor2' : - 1 }) print ( prediction ) Of course, that example was just the tip of the iceberg, please read about the main concepts of lightwood, the API and then jump into examples.","title":"Introduction to Lightwood"},{"location":"lightwood/info/#prerequisites","text":"Python >=3.6 64bit version","title":"Prerequisites"},{"location":"lightwood/info/#installing-lightwood","text":"You can install Lightwood using pip : pip3 install lightwood If this fails, please report the bug on github and try installing the current master branch: git clone git@github.com:mindsdb/lightwood.git ; cd lightwood ; pip install --no-cache-dir -e . Please note that, depending on your os and python setup, you might want to use pip instead of pip3 . You need python 3.6 or higher. Note on MacOS, you need to install libomp: brew install libomp","title":"Installing Lightwood"},{"location":"lightwood/info/#install-using-virtual-environment","text":"We suggest you to install Lightwood on a virtual environment to avoid dependency issues. Make sure your Python version is >=3.6. To set up a virtual environment:","title":"Install using virtual environment"},{"location":"lightwood/info/#install-on-windows","text":"Install the latest version of pip : python -m pip install --upgrade pip pip --version Activate your virtual environment and install lightwood: py -m pip install --user virtualenv . \\e nv \\S cripts \\a ctivate pip install lightwood You can also use python instead of py","title":"Install on Windows"},{"location":"lightwood/info/#install-on-linux-or-macos","text":"Before installing Lightwood in a virtual environment you need to first create and activate the venv : python -m venv env source env/bin/activate pip install lightwood","title":"Install on Linux or macOS"},{"location":"lightwood/info/#quick-example","text":"Assume that you have a training file (sensor_data.csv) such as this one. sensor1 sensor2 sensor3 1 -1 -1 0 1 0 -1 -1 1 1 0 0 0 1 0 -1 1 -1 0 0 0 -1 -1 1 1 0 0 And you would like to learn to predict the values of sensor3 given the readings in sensor1 and sensor2 .","title":"Quick example"},{"location":"lightwood/info/#learn","text":"You can train a Predictor as follows: from lightwood import Predictor import pandas sensor3_predictor = Predictor ( output = [ 'sensor3' ]) sensor3_predictor . learn ( from_data = pandas . read_csv ( 'sensor_data.csv' ))","title":"Learn"},{"location":"lightwood/info/#predict","text":"You can now be given new readings from sensor1 and sensor2 predict what sensor3 will be. prediction = sensor3_predictor . predict ( when = { 'sensor1' : 1 , 'sensor2' : - 1 }) print ( prediction ) Of course, that example was just the tip of the iceberg, please read about the main concepts of lightwood, the API and then jump into examples.","title":"Predict"},{"location":"model/clickhouse/","text":"Train a model from ClickHouse database Train new model To train a new model, you will neeed to INSERT a new record inside the mindsdb.predictors table. How to create mindsb database and tables Note that after connecting MindsDB and ClickHouse , on start, MindsDB server will automaticly create the mindsdb database and add predictors table. The INSERT query for training new model is quite simple e.g: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' , '{\"additional_training_params:value\"}' ); The values provided in INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features include a comma separated string e.g 'feature1, feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Train new model example The following example shows you how to train new model from clickhouse-client. The table used for training the model is timeseries data same as Air Pollution in Seoul dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_model' , 'SO2' , 'SELECT * FROM default.pollution_measurement' , '{\"timeseries_settings\":{\"order_by\": [\"Measurement date\"], \"window\":20}}' ); This INSERT query will train a new model called pollution_measurement that predicts the passenger SO2 value. Since, this is a timeseries data the timeseries_settings will order data by Measurement date column and will set the window for rows to \"look back\" when making a prediction. Model training status To check that the training successfully finished you can SELECT from mindsdb.predictors table and get the training status e.g: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's all You have succesfully trained new model from ClickHouse database. Next step is to get predictions by querying the model .","title":"ClickHouse"},{"location":"model/clickhouse/#train-a-model-from-clickhouse-database","text":"","title":"Train a model from ClickHouse database"},{"location":"model/clickhouse/#train-new-model","text":"To train a new model, you will neeed to INSERT a new record inside the mindsdb.predictors table. How to create mindsb database and tables Note that after connecting MindsDB and ClickHouse , on start, MindsDB server will automaticly create the mindsdb database and add predictors table. The INSERT query for training new model is quite simple e.g: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' , '{\"additional_training_params:value\"}' ); The values provided in INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features include a comma separated string e.g 'feature1, feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"Train new model"},{"location":"model/clickhouse/#train-new-model-example","text":"The following example shows you how to train new model from clickhouse-client. The table used for training the model is timeseries data same as Air Pollution in Seoul dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_model' , 'SO2' , 'SELECT * FROM default.pollution_measurement' , '{\"timeseries_settings\":{\"order_by\": [\"Measurement date\"], \"window\":20}}' ); This INSERT query will train a new model called pollution_measurement that predicts the passenger SO2 value. Since, this is a timeseries data the timeseries_settings will order data by Measurement date column and will set the window for rows to \"look back\" when making a prediction.","title":"Train new model example"},{"location":"model/clickhouse/#model-training-status","text":"To check that the training successfully finished you can SELECT from mindsdb.predictors table and get the training status e.g: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's all You have succesfully trained new model from ClickHouse database. Next step is to get predictions by querying the model .","title":"Model training status"},{"location":"model/mariadb/","text":"How to train a model from MariaDB? How to train new model? To train a new model, you will neeed to INSERT a new record inside the mindsdb.predictors table. How to create mindsb.predictors table Note that after connecting MindsDB and MariaDB , on start, MindsDB server will automaticly create the mindsdb database and add predictors table. The INSERT query for training new model is quite simple e.g: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' ); The values provided in INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features include a comma separated string e.g 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Train new model example The following example shows you how to train new model from mariadb-client. The table used for training the model is same as Used cars dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , 'SELECT * FROM data.used_cars_data' ); The INSERT query will train a new model called used_cars_model that predicts the cars price value. How to check model training status? To check that the training successfully finished you can SELECT from mindsdb.predictors table and get the training status e.g: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's all You have succesfully trained new model from MariaDB database. Next step is to get predictions by querying the model .","title":"MariaDB"},{"location":"model/mariadb/#how-to-train-a-model-from-mariadb","text":"","title":"How to train a model from MariaDB?"},{"location":"model/mariadb/#how-to-train-new-model","text":"To train a new model, you will neeed to INSERT a new record inside the mindsdb.predictors table. How to create mindsb.predictors table Note that after connecting MindsDB and MariaDB , on start, MindsDB server will automaticly create the mindsdb database and add predictors table. The INSERT query for training new model is quite simple e.g: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' ); The values provided in INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features include a comma separated string e.g 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"How to train new model?"},{"location":"model/mariadb/#train-new-model-example","text":"The following example shows you how to train new model from mariadb-client. The table used for training the model is same as Used cars dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , 'SELECT * FROM data.used_cars_data' ); The INSERT query will train a new model called used_cars_model that predicts the cars price value.","title":"Train new model example"},{"location":"model/mariadb/#how-to-check-model-training-status","text":"To check that the training successfully finished you can SELECT from mindsdb.predictors table and get the training status e.g: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's all You have succesfully trained new model from MariaDB database. Next step is to get predictions by querying the model .","title":"How to check model training status?"},{"location":"model/mysql/","text":"Train a model from MySQL database Train new model To train a new model, you will neeed to INSERT a new record inside the mindsdb.predictors table. How to create mindsb.predictors table Note that after connecting MindsDB and MySQL servers , on start, MindsDB server will automaticly create the mindsdb database and add predictors table. The INSERT query for training new model is quite simple e.g: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' ); The values provided in INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features include a comma separated string e.g 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Train new model example The following example shows you how to train new model from mysql-client. The table used for training the model is same as Us consumption dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_predictor' , 'SO2' , 'SELECT * FROM default.pollution_measurement' , '{\"timeseries_settings\":{\"order_by\": [\"Measurement date\"], \"window\":20}}' ); This INSERT query will train a new model called airq_predictor that predicts the SO2 value. Since, this is a timeseries data the timeseries_settings will order data by t column and will set the window for rows to \"look back\" when making a prediction. Model training status To check that the training successfully finished you can SELECT from mindsdb.predictors table and get the training status e.g: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's all You have succesfully trained new model from MySQL database. Next step is to get predictions by querying the model .","title":"MySQL"},{"location":"model/mysql/#train-a-model-from-mysql-database","text":"","title":"Train a model from MySQL database"},{"location":"model/mysql/#train-new-model","text":"To train a new model, you will neeed to INSERT a new record inside the mindsdb.predictors table. How to create mindsb.predictors table Note that after connecting MindsDB and MySQL servers , on start, MindsDB server will automaticly create the mindsdb database and add predictors table. The INSERT query for training new model is quite simple e.g: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' ); The values provided in INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features include a comma separated string e.g 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"Train new model"},{"location":"model/mysql/#train-new-model-example","text":"The following example shows you how to train new model from mysql-client. The table used for training the model is same as Us consumption dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'airq_predictor' , 'SO2' , 'SELECT * FROM default.pollution_measurement' , '{\"timeseries_settings\":{\"order_by\": [\"Measurement date\"], \"window\":20}}' ); This INSERT query will train a new model called airq_predictor that predicts the SO2 value. Since, this is a timeseries data the timeseries_settings will order data by t column and will set the window for rows to \"look back\" when making a prediction.","title":"Train new model example"},{"location":"model/mysql/#model-training-status","text":"To check that the training successfully finished you can SELECT from mindsdb.predictors table and get the training status e.g: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's all You have succesfully trained new model from MySQL database. Next step is to get predictions by querying the model .","title":"Model training status"},{"location":"model/postgresql/","text":"Train a model from PostgreSQL database Train new model To train a new model, you will neeed to INSERT a new record inside the mindsdb.predictors table. How to create mindsb schema and tables Note that after connecting MindsDB and PostgreSQL , on start, MindsDB server will automaticly create the mindsdb schema and add predictors table. The INSERT query for training new model is quite simple e.g: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' , '{\"additional_training_params:value\"}' ); The values provided in INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features include a comma separated string e.g 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . Train new model example The following example shows you how to train new model from psql-client. The table used for training the model is same as Airline Passenger sattisfaction dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'airline_survey_model' , 'satisfaction' , 'SELECT * FROM airline_passenger_satisfaction' ); This INSERT query will train a new model called airline_survey_model that predicts the passenger satisfaction value. Model training status To check that the training successfully finished you can SELECT from mindsdb.predictors table and get the training status e.g: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's all You have succesfully trained new model from PostgreSQL database. Next step is to get predictions by querying the model .","title":"PostgreSQL"},{"location":"model/postgresql/#train-a-model-from-postgresql-database","text":"","title":"Train a model from PostgreSQL database"},{"location":"model/postgresql/#train-new-model","text":"To train a new model, you will neeed to INSERT a new record inside the mindsdb.predictors table. How to create mindsb schema and tables Note that after connecting MindsDB and PostgreSQL , on start, MindsDB server will automaticly create the mindsdb schema and add predictors table. The INSERT query for training new model is quite simple e.g: INSERT INTO mindsdb . predictors ( name , predict , select_data_query , training_options ) VALUES ( 'model_name' , 'target_variable' , 'SELECT * FROM table_name' , '{\"additional_training_params:value\"}' ); The values provided in INSERT query are: name (string) -- The name of the model. predict (string) -- The feature you want to predict. To predict multiple features include a comma separated string e.g 'feature1,feature2'. select_data_query (string) -- The SELECT query that will ingest the data to train the model. training_options (JSON as comma separated string) -- optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface .","title":"Train new model"},{"location":"model/postgresql/#train-new-model-example","text":"The following example shows you how to train new model from psql-client. The table used for training the model is same as Airline Passenger sattisfaction dataset. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'airline_survey_model' , 'satisfaction' , 'SELECT * FROM airline_passenger_satisfaction' ); This INSERT query will train a new model called airline_survey_model that predicts the passenger satisfaction value.","title":"Train new model example"},{"location":"model/postgresql/#model-training-status","text":"To check that the training successfully finished you can SELECT from mindsdb.predictors table and get the training status e.g: SELECT * FROM mindsdb . predictors WHERE name = '<model_name>' ; That's all You have succesfully trained new model from PostgreSQL database. Next step is to get predictions by querying the model .","title":"Model training status"},{"location":"model/quality/","text":"Evaluate model quality using Studio Once the model is trained you can use the model quality preview to get insights about the trained model. Visualize model quality Note that any model could be evaluated using MindsDB Studio. That means not only the models trained with Studio but also models trained from SQL clients, MindsDB API's or SDK. To do that: From the left navigation menu select the Predictors dashboard. Click on the PREVIEW button on the model you want to evaluate. Click on the minus sign to expand each section. Model Accuracy In this section MindsDB will show you the visualizations about: Data splitting (80% train data and 20% test data) Model accuracy (How accurate is the model when blindsided) Column importance This section tries to answer the What is important for this model? question. MindsDB tries various combinations of missing columns to determine the importance of each one. The column importance rating ranges from 0 which means the column is useless to 10 meaning the column's predictive ability is great. Confusion matrix This section tries to answer the When can you trust this model? question. Here, the confusion matrix shows the performance that the model gets when solving a classification problem. Each entry in the confusion matrix can tell how many samples of each class were correctly classified and how many and where incorrectly classified. To get a detailed explanation you can move the mouse cursor over the graphics. Let's Query the model After evaluating the performance of the model, next step is to get predictions by querying the model .","title":"Evaluate from Studio"},{"location":"model/quality/#evaluate-model-quality-using-studio","text":"Once the model is trained you can use the model quality preview to get insights about the trained model. Visualize model quality Note that any model could be evaluated using MindsDB Studio. That means not only the models trained with Studio but also models trained from SQL clients, MindsDB API's or SDK. To do that: From the left navigation menu select the Predictors dashboard. Click on the PREVIEW button on the model you want to evaluate. Click on the minus sign to expand each section.","title":"Evaluate model quality using Studio"},{"location":"model/quality/#model-accuracy","text":"In this section MindsDB will show you the visualizations about: Data splitting (80% train data and 20% test data) Model accuracy (How accurate is the model when blindsided)","title":"Model Accuracy"},{"location":"model/quality/#column-importance","text":"This section tries to answer the What is important for this model? question. MindsDB tries various combinations of missing columns to determine the importance of each one. The column importance rating ranges from 0 which means the column is useless to 10 meaning the column's predictive ability is great.","title":"Column importance"},{"location":"model/quality/#confusion-matrix","text":"This section tries to answer the When can you trust this model? question. Here, the confusion matrix shows the performance that the model gets when solving a classification problem. Each entry in the confusion matrix can tell how many samples of each class were correctly classified and how many and where incorrectly classified. To get a detailed explanation you can move the mouse cursor over the graphics. Let's Query the model After evaluating the performance of the model, next step is to get predictions by querying the model .","title":"Confusion matrix"},{"location":"model/train/","text":"Train new model using Studio Before training the new model you must connect to a Datasource. To learn how to do that check out: Connect to MySQL Connect to PostgreSQL Connect to MariaDB . Connect to ClickHouse Connect to remote URL Upload local dataset Connect to Snowflake Basic mode Training the new model from MindsDB Studio is quite easy: From the left navigation menu open Predictors dashboard. Click on the TRAIN NEW button. In the New Predictor modal: Select the From datasource option. Add the name of the model. Check the column name (feature) that you want to predict. Press TRAIN . Advanced mode In the Advanced mode , you can find a few additional options to train the Machine Learning model as use GPU for training, exclude columns from training or change the sample margin of error. To do that, inside the New Predictor modal: Select the From datasource option. Add the name of the model. Check the column name (feature) that you want to predict. Press the ADVANCED MODE button and: Check the USE GPU checkbox. Check the columns that you want to exclude from model training. Add the sample margin of error value. Press TRAIN . Timeseries To build the timeseries model, you need to select Yes, it is timeseries checkbox inside the Advanced Mode section: Select the From datasource option. Add the name of the model. Check the column name (feature) that you want to predict. Press the ADVANCED MODE button and: Check the Yes, it is. checkbox. Select the Order by: value, the column based on which the data should be ordered. Select the Group by: value, the column based on which to group multiple entities in timeseries data. Add the Look Back Window: value, the number of rows to look back into when making a prediction. Press TRAIN . That's all You have successfully trained a new Machine Learning model. The next step is to evaluate the model quality .","title":"Create and train from Studio"},{"location":"model/train/#train-new-model-using-studio","text":"Before training the new model you must connect to a Datasource. To learn how to do that check out: Connect to MySQL Connect to PostgreSQL Connect to MariaDB . Connect to ClickHouse Connect to remote URL Upload local dataset Connect to Snowflake","title":"Train new model using Studio"},{"location":"model/train/#basic-mode","text":"Training the new model from MindsDB Studio is quite easy: From the left navigation menu open Predictors dashboard. Click on the TRAIN NEW button. In the New Predictor modal: Select the From datasource option. Add the name of the model. Check the column name (feature) that you want to predict. Press TRAIN .","title":"Basic mode"},{"location":"model/train/#advanced-mode","text":"In the Advanced mode , you can find a few additional options to train the Machine Learning model as use GPU for training, exclude columns from training or change the sample margin of error. To do that, inside the New Predictor modal: Select the From datasource option. Add the name of the model. Check the column name (feature) that you want to predict. Press the ADVANCED MODE button and: Check the USE GPU checkbox. Check the columns that you want to exclude from model training. Add the sample margin of error value. Press TRAIN .","title":"Advanced mode"},{"location":"model/train/#timeseries","text":"To build the timeseries model, you need to select Yes, it is timeseries checkbox inside the Advanced Mode section: Select the From datasource option. Add the name of the model. Check the column name (feature) that you want to predict. Press the ADVANCED MODE button and: Check the Yes, it is. checkbox. Select the Order by: value, the column based on which the data should be ordered. Select the Group by: value, the column based on which to group multiple entities in timeseries data. Add the Look Back Window: value, the number of rows to look back into when making a prediction. Press TRAIN . That's all You have successfully trained a new Machine Learning model. The next step is to evaluate the model quality .","title":"Timeseries"},{"location":"model/query/clickhouse/","text":"Query the model from ClickHouse database This section assumes that you have trained new model using clickhouse-client or MindsDB Studio . To query the model, you will neeed to SELECT from the model table as: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE < feature_one > AND < feature_two > Query the model from other databases Note that even if you have trained the model from the ClickHouse database, you will be able to query it from other databases too. Query example The following example shows you how to train new model from clickhouse-client. The table used for training the model is timeseries data same as Air Pollution in Seoul dataset. MindsDB will predict the SO2 (Sulfur dioxide) value in the air based on the values added in WHERE statement. SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) You should get a similar response from MindsDB as: SO2 confidence info 0.009897379182791115 0.99 Check JSON bellow info: { \"predicted_value\" : 0.009897379182791113 , \"confidence\" : 0.99 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 0.0059810733322441575 , 0.01381368503333807 ], \"important_missing_information\" : [ \"Address\" ] }","title":"ClickHouse"},{"location":"model/query/clickhouse/#query-the-model-from-clickhouse-database","text":"This section assumes that you have trained new model using clickhouse-client or MindsDB Studio . To query the model, you will neeed to SELECT from the model table as: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE < feature_one > AND < feature_two > Query the model from other databases Note that even if you have trained the model from the ClickHouse database, you will be able to query it from other databases too.","title":"Query the model from ClickHouse database"},{"location":"model/query/clickhouse/#query-example","text":"The following example shows you how to train new model from clickhouse-client. The table used for training the model is timeseries data same as Air Pollution in Seoul dataset. MindsDB will predict the SO2 (Sulfur dioxide) value in the air based on the values added in WHERE statement. SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) You should get a similar response from MindsDB as: SO2 confidence info 0.009897379182791115 0.99 Check JSON bellow info: { \"predicted_value\" : 0.009897379182791113 , \"confidence\" : 0.99 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 0.0059810733322441575 , 0.01381368503333807 ], \"important_missing_information\" : [ \"Address\" ] }","title":"Query example"},{"location":"model/query/mariadb/","text":"How to query the model from MariaDB database? This section assumes that you have trained new model using mariadb or MindsDB Studio . To query the model, you will neeed to SELECT from the model table as: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE < feature_one > AND < feature_two > Query the model from other databases Note that even if you have trained the model from the MariaDB database, you will be able to query it from other databases too. Query example The following example shows you how to train new model from mariadb-client. The table used for training the model is same as Used cars dataset. MindsDB will predict the price based on the values added in WHERE clause. SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"A6\" AND mileage = 36203 AND transmission = \"Automatic\" AND fuelType = \"Diesel\" AND mpg = \"64.2\" AND engineSize = 2 AND year = 2016 AND tax = 20 ; You should get a similar response from MindsDB as: price confidence info 16117 0.98 Check JSON bellow info: { \"predicted_value\" : 16117.627834024992 , \"confidence\" : 0.98 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10737.135673357996 , 21498.119994691988 ], \"important_missing_information\" : [] }","title":"MariaDB"},{"location":"model/query/mariadb/#how-to-query-the-model-from-mariadb-database","text":"This section assumes that you have trained new model using mariadb or MindsDB Studio . To query the model, you will neeed to SELECT from the model table as: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE < feature_one > AND < feature_two > Query the model from other databases Note that even if you have trained the model from the MariaDB database, you will be able to query it from other databases too.","title":"How to query the model from MariaDB database?"},{"location":"model/query/mariadb/#query-example","text":"The following example shows you how to train new model from mariadb-client. The table used for training the model is same as Used cars dataset. MindsDB will predict the price based on the values added in WHERE clause. SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"A6\" AND mileage = 36203 AND transmission = \"Automatic\" AND fuelType = \"Diesel\" AND mpg = \"64.2\" AND engineSize = 2 AND year = 2016 AND tax = 20 ; You should get a similar response from MindsDB as: price confidence info 16117 0.98 Check JSON bellow info: { \"predicted_value\" : 16117.627834024992 , \"confidence\" : 0.98 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10737.135673357996 , 21498.119994691988 ], \"important_missing_information\" : [] }","title":"Query example"},{"location":"model/query/mysql/","text":"Query the model from MySQL database This section assumes that you have trained new model using mysql or MindsDB Studio . To query the model, you will neeed to SELECT from the model table as: Query the model from other databases Note that even if you have trained the model from the MySQL database, you will be able to query it from other databases too. Query example The following example shows you how to query the model from mysql-client. The table used for training the model is the same as Us consumption dataset. MindsDB will predict the consumption based on the values added in when_data . SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE when_data = '{\"income\": 1.182497938, \"production\": 5.854555956,\"savings\": 3.183292657, \"unemployment\": 0.1, \"t\":\"2020-01-02\"}' ; You should get a similar response from MindsDB as: consumption confidence info 1.0 0.93 Check JSON bellow info: { \"predicted_value\" : \"1.0\" , \"confidence\" : 0.93 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [] }","title":"MySQL"},{"location":"model/query/mysql/#query-the-model-from-mysql-database","text":"This section assumes that you have trained new model using mysql or MindsDB Studio . To query the model, you will neeed to SELECT from the model table as: Query the model from other databases Note that even if you have trained the model from the MySQL database, you will be able to query it from other databases too.","title":"Query the model from MySQL database"},{"location":"model/query/mysql/#query-example","text":"The following example shows you how to query the model from mysql-client. The table used for training the model is the same as Us consumption dataset. MindsDB will predict the consumption based on the values added in when_data . SELECT consumption AS predicted , consumption_confidence AS confidence , consumption_explain AS info FROM mindsdb . us_consumption WHERE when_data = '{\"income\": 1.182497938, \"production\": 5.854555956,\"savings\": 3.183292657, \"unemployment\": 0.1, \"t\":\"2020-01-02\"}' ; You should get a similar response from MindsDB as: consumption confidence info 1.0 0.93 Check JSON bellow info: { \"predicted_value\" : \"1.0\" , \"confidence\" : 0.93 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [] }","title":"Query example"},{"location":"model/query/postgresql/","text":"Query the model from PostgreSQL database This section assumes that you have trained new model using psql or MindsDB Studio . To query the model, you will neeed to SELECT from the model table as: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE < feature_one > AND < feature_two > Query the model from other databases Note that even if you have trained the model from the PostgreSQL database, you will be able to query it from other databases too. Query example The following example shows you how to query the model from psql-client. The table used for training the model is the same as Airline Passenger sattisfaction dataset. MindsDB will predict the satisfaction based on the values added in WHERE statement. SELECT satisfaction AS predicted , satisfaction_confidence AS confidence , satisfaction_explain AS info FROM mindsdb . airline_survey_model WHERE \"Customer Type\" = 'Loyal Customer' AND age = 52 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; You should get a similar response from MindsDB as: satisfaction confidence info satisfied 0.94 Check JSON bellow info: { \"predicted_value\" : \"satisfied\" , \"confidence\" : 0.94 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"id\" , \"Inflight wifi service\" , \"Online boarding\" , \"Seat comfort\" , \"Baggage handling\" ] }","title":"PostgreSQL"},{"location":"model/query/postgresql/#query-the-model-from-postgresql-database","text":"This section assumes that you have trained new model using psql or MindsDB Studio . To query the model, you will neeed to SELECT from the model table as: SELECT < target_variable > AS predicted , < target_variable_confidence > AS confidence , < target_variable_explain > AS info FROM mindsdb . < model_name > WHERE < feature_one > AND < feature_two > Query the model from other databases Note that even if you have trained the model from the PostgreSQL database, you will be able to query it from other databases too.","title":"Query the model from PostgreSQL database"},{"location":"model/query/postgresql/#query-example","text":"The following example shows you how to query the model from psql-client. The table used for training the model is the same as Airline Passenger sattisfaction dataset. MindsDB will predict the satisfaction based on the values added in WHERE statement. SELECT satisfaction AS predicted , satisfaction_confidence AS confidence , satisfaction_explain AS info FROM mindsdb . airline_survey_model WHERE \"Customer Type\" = 'Loyal Customer' AND age = 52 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; You should get a similar response from MindsDB as: satisfaction confidence info satisfied 0.94 Check JSON bellow info: { \"predicted_value\" : \"satisfied\" , \"confidence\" : 0.94 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"id\" , \"Inflight wifi service\" , \"Online boarding\" , \"Seat comfort\" , \"Baggage handling\" ] }","title":"Query example"},{"location":"model/query/scout/","text":"Query the model using Studio To get predictive analytics from trained model: From the left navigation menu open Query dashboard. Click on the NEW QUERY button. In the New Query modal: Select the From predictor option(the name of the pre trained model). Add the features values for which you want to get the predictions. Query the UsedCars model example In this example we want to solve the problem of estimating the right price for a used car that has Diesel as a fuel type and has around 20 000 miles. To get the price: From the predictors dashboard click on the Query button. Click on the NEW QUERY button. In the New Query modal: Add the features values for which you want to get the predictions e.g Mileage 20 000. Fuel type Diesel.","title":"Query the model using Studio"},{"location":"model/query/scout/#query-the-model-using-studio","text":"To get predictive analytics from trained model: From the left navigation menu open Query dashboard. Click on the NEW QUERY button. In the New Query modal: Select the From predictor option(the name of the pre trained model). Add the features values for which you want to get the predictions.","title":"Query the model using Studio"},{"location":"model/query/scout/#query-the-usedcars-model-example","text":"In this example we want to solve the problem of estimating the right price for a used car that has Diesel as a fuel type and has around 20 000 miles. To get the price: From the predictors dashboard click on the Query button. Click on the NEW QUERY button. In the New Query modal: Add the features values for which you want to get the predictions e.g Mileage 20 000. Fuel type Diesel.","title":"Query the UsedCars model example"},{"location":"server/SDKs/","text":"The MindsDB SDK's are providing all of the MindsDB's native functionalities through MindsDB HTTP Interface. Currently, MindsDB provides SDK's for JavaScript and Python. Installing JavaScript SDK The JavaScript SDK can be installed with npm or yarn: npm install mindsdb-js-sdk or yarn add mindsdb-js-sdk Also, you can install it from source: git clone git@github.com:mindsdb/mindsdb_js_sdk.git cd mindsdb_js_sdk npm install Usage example The following example covers the basic flow: connect to MindsDB Server, train new model, make predictions. import MindsDB from 'mindsdb-js-sdk' ; //connection MindsDB . connect ( url ); const connected = await MindsDB . ping (); if ( ! connected ) return ; // lists of predictors and datasources const predictorsList = MindsDB . dataSources (); const predictors = MindsDB . predictors (); // get datasource const rentalsDatasource = await MindsDB . DataSource ({ name : 'home_rentals' }). load (); // get predictor const rentalsPredictor = await MindsDB . Predictor ({ name : 'home_rentals' }). load (); // query const result = rentalsPredictor . queryPredict ({ 'initial)|_price' : 2000 , 'sqft' : 500 }); console . log ( result ); MindsDB . disconnect (); Installing Python SDK The Python SDK can be installed from PyPI: pip install mindsdb-client Or you can install it from source: git clone git@github.com:mindsdb/mindsdb_python_sdk.git cd mindsdb_python_sdk python setup.py develop pip install -r requirements.txt Usage example The following example covers the basic flow: connect to MindsDB Server, train new model, make predictions. from mindsdb_client import MindsDB # connect mdb = MindsDB ( server = 'http://mindsdb.server:47334' , params = { 'email' : 'login@email.com' , 'password' : 'loginpass' }) # upload datasource mdb . datasources . add ( 'home_rentals_data' , path = 'home_rentals.csv' ) # create a new predictor and learn to predict predictor = mdb . predictors . learn ( name = 'home_rentals' , data_source_name = 'home_rentals_data' , to_predict = 'rental_price' ) # predict result = predictor . predict ({ 'initial_price' : '2000' , 'number_of_bathrooms' : '1' , 'sqft' : '700' })","title":"MindsDB SDK"},{"location":"server/SDKs/#installing-javascript-sdk","text":"The JavaScript SDK can be installed with npm or yarn: npm install mindsdb-js-sdk or yarn add mindsdb-js-sdk Also, you can install it from source: git clone git@github.com:mindsdb/mindsdb_js_sdk.git cd mindsdb_js_sdk npm install","title":"Installing JavaScript SDK"},{"location":"server/SDKs/#usage-example","text":"The following example covers the basic flow: connect to MindsDB Server, train new model, make predictions. import MindsDB from 'mindsdb-js-sdk' ; //connection MindsDB . connect ( url ); const connected = await MindsDB . ping (); if ( ! connected ) return ; // lists of predictors and datasources const predictorsList = MindsDB . dataSources (); const predictors = MindsDB . predictors (); // get datasource const rentalsDatasource = await MindsDB . DataSource ({ name : 'home_rentals' }). load (); // get predictor const rentalsPredictor = await MindsDB . Predictor ({ name : 'home_rentals' }). load (); // query const result = rentalsPredictor . queryPredict ({ 'initial)|_price' : 2000 , 'sqft' : 500 }); console . log ( result ); MindsDB . disconnect ();","title":"Usage example"},{"location":"server/SDKs/#installing-python-sdk","text":"The Python SDK can be installed from PyPI: pip install mindsdb-client Or you can install it from source: git clone git@github.com:mindsdb/mindsdb_python_sdk.git cd mindsdb_python_sdk python setup.py develop pip install -r requirements.txt","title":"Installing Python SDK"},{"location":"server/SDKs/#usage-example_1","text":"The following example covers the basic flow: connect to MindsDB Server, train new model, make predictions. from mindsdb_client import MindsDB # connect mdb = MindsDB ( server = 'http://mindsdb.server:47334' , params = { 'email' : 'login@email.com' , 'password' : 'loginpass' }) # upload datasource mdb . datasources . add ( 'home_rentals_data' , path = 'home_rentals.csv' ) # create a new predictor and learn to predict predictor = mdb . predictors . learn ( name = 'home_rentals' , data_source_name = 'home_rentals_data' , to_predict = 'rental_price' ) # predict result = predictor . predict ({ 'initial_price' : '2000' , 'number_of_bathrooms' : '1' , 'sqft' : '700' })","title":"Usage example"},{"location":"tutorials/AdvancedExamples/","text":"Multiple column predictions What is a multiple column prediction ? In some cases, you might want to predict more than one column of your data. In order for mindsdb to predict multiple columns, you simply need to change the to_predict argument from a string (denoting the name of the column) to an array containing the names of the columns you want to predict. In the following example, we've altered the real estate model to predict the location and neighborhood both, instead of the rental_price . Code example import mindsdb mdb = mindsdb . Predictor ( name = 'multilabel_real_estate_model' ) mdb . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , to_predict = [ 'location' , 'neighborhood' ] # Array with the names of the columns we want to predict ) Multimedia inputs (images, audio and video) Currently, we only support images as inputs into models. We are working on support audio, you can check this issue to track the progress. Video input support is not yet planned. For any sort of media files, simply provide the full path to the file in the column. For example: [ { 'img' : '/mnt/data/img_1.jpg' 'is_bicycle' : True } ,{ 'img' : '/mnt/data/img_2.jpg' 'is_bicycle' : True } ,{ 'img' : '/mnt/data/img_3.jpg' 'is_bicycle' : False } ] Please provide the full path to a file on your local machine, not a url or the binary data from an image loaded up in a dataframe. Currently, the timeline on supporting multimedia output is still undecided, if you need that feature or want to implement, feel free to contact us. That being said, image outputs might actually work, we just haven't tested anything yet. Unbalanced dataset Given a dataset that is \"imbalanced\", the model being trained might not give the results you expect. For example, let's say you have the following dataset: data . csv x , is_power_of_9 1 , False 2 , False 3 , False 4 , False 5 , False 6 , False 7 , False 8 , False 9 , True 10 , False 11 , False . . . 81 , True 82 , False 83 , False 84 , False . . . 100 , False On which we want to predicted the aptly-named column is_power_of_9 . We have 2 occurrences of the output True and 98 occurrences of the False output. So a model could always predict False and it would have an accuracy of 98%, which is pretty decent, so unless there's a way for the model to learn those 2 instance of True without losing on accuracy, it might just decided 98% is the best possible model. However, let's say we really care about those True predictions being correct, or at least we want the model to consider them equally important, in that case we would call the learn function using the equal_accuracy_for_all_output_categories argument set to true. This essentially means that a model with 50% accuracy, with 2 correct predictions for True (100%) and 48 for False (49%) is considered better than a model with 98% accuracy that only predictions False when mindsdb trains the model. We could call this as: predictior.learn(from_data='data.csv', equal_accuracy_for_all_output_categories=True)","title":"Advanced usecases"},{"location":"tutorials/AdvancedExamples/#multiple-column-predictions","text":"","title":"Multiple column predictions"},{"location":"tutorials/AdvancedExamples/#what-is-a-multiple-column-prediction","text":"In some cases, you might want to predict more than one column of your data. In order for mindsdb to predict multiple columns, you simply need to change the to_predict argument from a string (denoting the name of the column) to an array containing the names of the columns you want to predict. In the following example, we've altered the real estate model to predict the location and neighborhood both, instead of the rental_price .","title":"What is a multiple column prediction ?"},{"location":"tutorials/AdvancedExamples/#code-example","text":"import mindsdb mdb = mindsdb . Predictor ( name = 'multilabel_real_estate_model' ) mdb . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , to_predict = [ 'location' , 'neighborhood' ] # Array with the names of the columns we want to predict )","title":"Code example"},{"location":"tutorials/AdvancedExamples/#multimedia-inputs-images-audio-and-video","text":"Currently, we only support images as inputs into models. We are working on support audio, you can check this issue to track the progress. Video input support is not yet planned. For any sort of media files, simply provide the full path to the file in the column. For example: [ { 'img' : '/mnt/data/img_1.jpg' 'is_bicycle' : True } ,{ 'img' : '/mnt/data/img_2.jpg' 'is_bicycle' : True } ,{ 'img' : '/mnt/data/img_3.jpg' 'is_bicycle' : False } ] Please provide the full path to a file on your local machine, not a url or the binary data from an image loaded up in a dataframe. Currently, the timeline on supporting multimedia output is still undecided, if you need that feature or want to implement, feel free to contact us. That being said, image outputs might actually work, we just haven't tested anything yet.","title":"Multimedia inputs (images, audio and video)"},{"location":"tutorials/AdvancedExamples/#unbalanced-dataset","text":"Given a dataset that is \"imbalanced\", the model being trained might not give the results you expect. For example, let's say you have the following dataset: data . csv x , is_power_of_9 1 , False 2 , False 3 , False 4 , False 5 , False 6 , False 7 , False 8 , False 9 , True 10 , False 11 , False . . . 81 , True 82 , False 83 , False 84 , False . . . 100 , False On which we want to predicted the aptly-named column is_power_of_9 . We have 2 occurrences of the output True and 98 occurrences of the False output. So a model could always predict False and it would have an accuracy of 98%, which is pretty decent, so unless there's a way for the model to learn those 2 instance of True without losing on accuracy, it might just decided 98% is the best possible model. However, let's say we really care about those True predictions being correct, or at least we want the model to consider them equally important, in that case we would call the learn function using the equal_accuracy_for_all_output_categories argument set to true. This essentially means that a model with 50% accuracy, with 2 correct predictions for True (100%) and 48 for False (49%) is considered better than a model with 98% accuracy that only predictions False when mindsdb trains the model. We could call this as: predictior.learn(from_data='data.csv', equal_accuracy_for_all_output_categories=True)","title":"Unbalanced dataset"},{"location":"tutorials/BasicExample/","text":"This is a basic example of mindsdb_native usage in predicting the real estate prices for an area. If you want to follow out visually, watch bellow video: Goal The goal is to be able to predict the best rental_price for new properties given the information that we have in home_rentals.csv. Learning from mindsdb import Predictor # We tell the Predictor what column or key we want to learn and from what data Predictor ( name = 'real_estate_model' ) . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , # the path to the file where we can learn from, (note: can be url) to_predict = 'rental_price' , # the column we want to learn to predict given all the data in the file ) Note : that the argument from_data can be a path to a json, csv (or other separators), excel given as a file or as a URL, or a pandas Dataframe Predicting mdb = mindsdb . Predictor ( name = 'real_estate_model' ) # use the model to make predictions result = Predictor ( name = 'home_rentals_price' ) . predict ( when_data = { 'number_of_rooms' : 1 , 'initial_price' : 1222 , 'sqft' : 1190 }) # The result will be an array containing predictions for each data point (in this case only one), a confidence for said prediction and a few other extra informations print ( 'The predicted price is between $ {price} with {conf} confidence' . format ( price = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence_interval' ], conf = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence' ])) Notes About the Learning The first thing we can do is to learn from the csv file. Learn in the scope of MindsDB is to let it figure out a neural network that can best learn from this data as well as train and test such a model given the data that we have. When you run this script, note that it will start logging various information about the data and about the training process. This information can be useful in allowing you to figure out which parts of your data are of low quality or might contain erroneous values. About getting predictions from the model Please note the when_data argument, in this case assuming we only know that: 'number_of_rooms': 1, 'initial_price':1222 'sqft': 1190 So, as long as the columns that you pass in the when_data statement exists in the data it learned from it will work (see columns in home_rentals.csv ). Running online You can follow this example on Google Colab.","title":"Starter Example"},{"location":"tutorials/BasicExample/#goal","text":"The goal is to be able to predict the best rental_price for new properties given the information that we have in home_rentals.csv.","title":"Goal"},{"location":"tutorials/BasicExample/#learning","text":"from mindsdb import Predictor # We tell the Predictor what column or key we want to learn and from what data Predictor ( name = 'real_estate_model' ) . learn ( from_data = \"https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv\" , # the path to the file where we can learn from, (note: can be url) to_predict = 'rental_price' , # the column we want to learn to predict given all the data in the file ) Note : that the argument from_data can be a path to a json, csv (or other separators), excel given as a file or as a URL, or a pandas Dataframe","title":"Learning"},{"location":"tutorials/BasicExample/#predicting","text":"mdb = mindsdb . Predictor ( name = 'real_estate_model' ) # use the model to make predictions result = Predictor ( name = 'home_rentals_price' ) . predict ( when_data = { 'number_of_rooms' : 1 , 'initial_price' : 1222 , 'sqft' : 1190 }) # The result will be an array containing predictions for each data point (in this case only one), a confidence for said prediction and a few other extra informations print ( 'The predicted price is between $ {price} with {conf} confidence' . format ( price = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence_interval' ], conf = result [ 0 ] . explanation [ 'rental_price' ][ 'confidence' ]))","title":"Predicting"},{"location":"tutorials/BasicExample/#notes","text":"","title":"Notes"},{"location":"tutorials/BasicExample/#about-the-learning","text":"The first thing we can do is to learn from the csv file. Learn in the scope of MindsDB is to let it figure out a neural network that can best learn from this data as well as train and test such a model given the data that we have. When you run this script, note that it will start logging various information about the data and about the training process. This information can be useful in allowing you to figure out which parts of your data are of low quality or might contain erroneous values.","title":"About the Learning"},{"location":"tutorials/BasicExample/#about-getting-predictions-from-the-model","text":"Please note the when_data argument, in this case assuming we only know that: 'number_of_rooms': 1, 'initial_price':1222 'sqft': 1190 So, as long as the columns that you pass in the when_data statement exists in the data it learned from it will work (see columns in home_rentals.csv ).","title":"About getting predictions from the model"},{"location":"tutorials/BasicExample/#running-online","text":"You can follow this example on Google Colab.","title":"Running online"},{"location":"tutorials/ChurnReduction/","text":"Industry Department Role Telecomunications Marketing Marketing Lead Processed Dataset Customer churn or customer turnover is the loss of clients or customers. Telecommunication companies often use customer attrition analysis and customer attrition rates as one of their key business metrics because the cost of retaining an existing customer is far less than acquiring a new one. Use churn prediction models that predict customer churn by assessing their propensity of risk to churn. CreditScore Geography Gender Age Tenure Balance NumOfProducts HasCrCard IsActiveMember EstimatedSalary Exited 502 France Female 42 8 159661 3 1 0 113932 1 822 France Male 50 7 0 2 1 1 10062.8 0 549 France Female 25 5 0 2 0 0 190858 0 587 Spain Male 45 6 0 1 0 0 158685 0 582 Germany Male 41 6 70349.5 2 0 1 178074 0 556 France Female 61 2 117419 1 1 1 94153.8 0 550 Germany Male 38 2 103391 1 0 1 90878.1 0 Click to expand Features Informations: 1. customerIDCustomer ID 2. gender Whether the customer is a male or a female 3. SeniorCitizen Whether the customer is a senior citizen or not (1, 0) 4. Partner Whether the customer has a partner or not (Yes, No) 5. Dependents Whether the customer has dependents or not (Yes, No) tenureNumber of months the customer has stayed with the company 6. PhoneService Whether the customer has a phone service or not (Yes, No) 7. MultipleLines Whether the customer has multiple lines or not (Yes, No, No phone service) 8. InternetServiceCustomer\u2019s internet service provider (DSL, Fiber optic, No) 9. OnlineSecurity Whether the customer has online security or not (Yes, No, No internet service) 10. OnlineBackup Whether the customer has online backup or not (Yes, No, No internet service) 11. DeviceProtection Whether the customer has device protection or not (Yes, No, No internet service) 12. TechSupport Whether the customer has tech support or not (Yes, No, No internet service) 13. StreamingTV Whether the customer has streaming TV or not (Yes, No, No internet service) 14. StreamingMovies Whether the customer has streaming movies or not (Yes, No, No internet service) 15. ContractThe contract term of the customer (Month-to-month, One year, Two year) 16. PaperlessBilling Whether the customer has paperless billing or not (Yes, No) 17. PaymentMethodThe customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) 18. MonthlyChargesThe amount charged to the customer monthly 19. TotalChargesThe total amount charged to the customer 20 Churn Whether the customer churned or not (Yes or No) MindsDB Code example import mindsdb import pandas as pd from sklearn.metrics import accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'employee_retention_model' ) mdb . learn ( from_data = 'dataset/train.csv' , to_predict = 'Churn' ) test_df = pd . read_csv ( 'dataset/test.csv' ) predictions = mdb . predict ( when_data = 'dataset/test.csv' ) predicted_val = [ x . explanation [ 'Churn' ][ 'predicted_value' ] for x in predictions ] real_val = list ( map ( str , list ( test_df [ 'Churn' ]))) accuracy = accuracy_score ( real_val , predicted_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'accuracy_score' , 'backend' : backend , 'prediction_per_row' : additional_info } if __name__ == '__main__' : result = run () print ( result ) Mindsdb accuracy Accuraccy Backend Last run MindsDB Version Latest Version 0.7659574468085106 Lightwood 17 April 2020","title":"Customer Churn Reduction"},{"location":"tutorials/ChurnReduction/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/ChurnReduction/#_1","text":"Customer churn or customer turnover is the loss of clients or customers. Telecommunication companies often use customer attrition analysis and customer attrition rates as one of their key business metrics because the cost of retaining an existing customer is far less than acquiring a new one. Use churn prediction models that predict customer churn by assessing their propensity of risk to churn. CreditScore Geography Gender Age Tenure Balance NumOfProducts HasCrCard IsActiveMember EstimatedSalary Exited 502 France Female 42 8 159661 3 1 0 113932 1 822 France Male 50 7 0 2 1 1 10062.8 0 549 France Female 25 5 0 2 0 0 190858 0 587 Spain Male 45 6 0 1 0 0 158685 0 582 Germany Male 41 6 70349.5 2 0 1 178074 0 556 France Female 61 2 117419 1 1 1 94153.8 0 550 Germany Male 38 2 103391 1 0 1 90878.1 0 Click to expand Features Informations: 1. customerIDCustomer ID 2. gender Whether the customer is a male or a female 3. SeniorCitizen Whether the customer is a senior citizen or not (1, 0) 4. Partner Whether the customer has a partner or not (Yes, No) 5. Dependents Whether the customer has dependents or not (Yes, No) tenureNumber of months the customer has stayed with the company 6. PhoneService Whether the customer has a phone service or not (Yes, No) 7. MultipleLines Whether the customer has multiple lines or not (Yes, No, No phone service) 8. InternetServiceCustomer\u2019s internet service provider (DSL, Fiber optic, No) 9. OnlineSecurity Whether the customer has online security or not (Yes, No, No internet service) 10. OnlineBackup Whether the customer has online backup or not (Yes, No, No internet service) 11. DeviceProtection Whether the customer has device protection or not (Yes, No, No internet service) 12. TechSupport Whether the customer has tech support or not (Yes, No, No internet service) 13. StreamingTV Whether the customer has streaming TV or not (Yes, No, No internet service) 14. StreamingMovies Whether the customer has streaming movies or not (Yes, No, No internet service) 15. ContractThe contract term of the customer (Month-to-month, One year, Two year) 16. PaperlessBilling Whether the customer has paperless billing or not (Yes, No) 17. PaymentMethodThe customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) 18. MonthlyChargesThe amount charged to the customer monthly 19. TotalChargesThe total amount charged to the customer 20 Churn Whether the customer churned or not (Yes or No)","title":""},{"location":"tutorials/ChurnReduction/#mindsdb-code-example","text":"import mindsdb import pandas as pd from sklearn.metrics import accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'employee_retention_model' ) mdb . learn ( from_data = 'dataset/train.csv' , to_predict = 'Churn' ) test_df = pd . read_csv ( 'dataset/test.csv' ) predictions = mdb . predict ( when_data = 'dataset/test.csv' ) predicted_val = [ x . explanation [ 'Churn' ][ 'predicted_value' ] for x in predictions ] real_val = list ( map ( str , list ( test_df [ 'Churn' ]))) accuracy = accuracy_score ( real_val , predicted_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'accuracy_score' , 'backend' : backend , 'prediction_per_row' : additional_info } if __name__ == '__main__' : result = run () print ( result )","title":"MindsDB Code example"},{"location":"tutorials/ChurnReduction/#mindsdb-accuracy","text":"Accuraccy Backend Last run MindsDB Version Latest Version 0.7659574468085106 Lightwood 17 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/CreditScoring/","text":"Industry Department Role Financial Services Finance Business executive Processed Dataset The German Credit dataset is a publically available from the UCI Machine Learning Repository . The dataset contains data on 20 variables and the classification whether an applicant is considered a Good or a Bad credit risks. over_draft credit_usage credit_history purpose current_balance Average_Credit_Balance employment location personal_status other_parties residence_since property_magnitude cc_age other_payment_plans housing existing_credits job num_dependents own_telephone foreign_worker class no checking 9 existing paid education 3832 no known savings >=7 1 male single none 4 real estate 64 none own 1 unskilled resident 1 none yes good 0<=X<200 12 existing paid radio/tv 1092 <100 1<=X<4 4 female div/dep/mar guarantor 4 real estate 49 none own 2 skilled 1 yes yes good <0 12 critical/other existing credit furniture/equipment 2246 <100 >=7 3 male single none 3 life insurance 60 none own 2 skilled 1 none yes bad 0<=X<200 24 existing paid furniture/equipment 4057 <100 4<=X<7 3 male div/sep none 3 car 43 none own 1 skilled 1 yes yes bad no checking 24 existing paid furniture/equipment 929 no known savings 4<=X<7 4 male single none 2 car 31 stores own 1 skilled 1 yes yes good no checking 15 critical/other existing credit furniture/equipment 2788 <100 4<=X<7 2 female div/dep/mar co applicant 3 car 24 bank own 2 skilled 1 none yes good <0 36 all paid furniture/equipment 2746 <100 >=7 4 male single none 4 car 31 bank own 1 skilled 1 none yes bad Click to expand Features Informations: * Attribute 1: (qualitative) * Status of existing checking account * A11 : ... < 0 DM * A12 : 0 <= ... < 200 DM * A13 : ... >= 200 DM / salary assignments for at least 1 year * A14 : no checking account * Attribute 2: (numerical) * Duration in month * Attribute 3: (qualitative) * Credit history * A30 : no credits taken/ all credits paid back duly * A31 : all credits at this bank paid back duly * A32 : existing credits paid back duly till now * A33 : delay in paying off in the past * A34 : critical account/ other credits existing (not at this bank) * Attribute 4: (qualitative) * Purpose * A40 : car (new) * A41 : car (used) * A42 : furniture/equipment * A43 : radio/television * A44 : domestic appliances * A45 : repairs * A46 : education * A47 : (vacation - does not exist?) * A48 : retraining * A49 : business * A410 : others * Attribute 5: (numerical) * Credit amount * Attibute 6: (qualitative) * Savings account/bonds * A61 : ... < 100 DM * A62 : 100 <= ... < 500 DM * A63 : 500 <= ... < 1000 DM * A64 : .. >= 1000 DM * A65 : unknown/ no savings account * Attribute 7: (qualitative) * Present employment since * A71 : unemployed * A72 : ... < 1 year * A73 : 1 <= ... < 4 years * A74 : 4 <= ... < 7 years * A75 : .. >= 7 years * Attribute 8: (numerical) * Installment rate in percentage of disposable income * Attribute 9: (qualitative) * Personal status and sex * A91 : male : divorced/separated * A92 : female : divorced/separated/married * A93 : male : single * A94 : male : married/widowed * A95 : female : single * Attribute 10: (qualitative) * Other debtors / guarantors * A101 : none * A102 : co-applicant * A103 : guarantor * Attribute 11: (numerical) * Present residence since * Attribute 12: (qualitative) * Property * A121 : real estate * A122 : if not A121 : building society savings agreement/ life insurance * A123 : if not A121/A122 : car or other, not in attribute 6 * A124 : unknown / no property * Attribute 13: (numerical) * Age in years * Attribute 14: (qualitative) * Other installment plans * A141 : bank * A142 : stores * A143 : none * Attribute 15: (qualitative) * Housing * A151 : rent * A152 : own * A153 : for free * Attribute 16: (numerical) * Number of existing credits at this bank * Attribute 17: (qualitative) * Job * A171 : unemployed/ unskilled - non-resident * A172 : unskilled - resident * A173 : skilled employee / official * A174 : management/ self-employed/ * highly qualified employee/ officer * Attribute 18: (numerical) * Number of people being liable to provide maintenance for * Attribute 19: (qualitative) * Telephone * A191 : none * A192 : yes, registered under the customers name * Attribute 20: (qualitative) * foreign worker * A201 : yes * A202 : no MindsDB Code example from mindsdb import Predictor import pandas as pd from sklearn.metrics import balanced_accuracy_score , confusion_matrix def run ( sample = False ): mdb = Predictor ( name = 'german_data' ) mdb . learn ( to_predict = 'class' , from_data = 'processed_data/train.csv' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) predicted_val = [ x . explanation [ 'class' ][ 'predicted_value' ] for x in predictions ] real_val = list ( pd . read_csv ( 'processed_data/test.csv' )[ 'class' ]) accuracy = balanced_accuracy_score ( real_val , predicted_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend , 'single_row_predictions' : additional_info } # Run as main if __name__ == '__main__' : print ( run ()) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.64880952380 Lightwood 15 April 2020","title":"Credit Scoring"},{"location":"tutorials/CreditScoring/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/CreditScoring/#_1","text":"The German Credit dataset is a publically available from the UCI Machine Learning Repository . The dataset contains data on 20 variables and the classification whether an applicant is considered a Good or a Bad credit risks. over_draft credit_usage credit_history purpose current_balance Average_Credit_Balance employment location personal_status other_parties residence_since property_magnitude cc_age other_payment_plans housing existing_credits job num_dependents own_telephone foreign_worker class no checking 9 existing paid education 3832 no known savings >=7 1 male single none 4 real estate 64 none own 1 unskilled resident 1 none yes good 0<=X<200 12 existing paid radio/tv 1092 <100 1<=X<4 4 female div/dep/mar guarantor 4 real estate 49 none own 2 skilled 1 yes yes good <0 12 critical/other existing credit furniture/equipment 2246 <100 >=7 3 male single none 3 life insurance 60 none own 2 skilled 1 none yes bad 0<=X<200 24 existing paid furniture/equipment 4057 <100 4<=X<7 3 male div/sep none 3 car 43 none own 1 skilled 1 yes yes bad no checking 24 existing paid furniture/equipment 929 no known savings 4<=X<7 4 male single none 2 car 31 stores own 1 skilled 1 yes yes good no checking 15 critical/other existing credit furniture/equipment 2788 <100 4<=X<7 2 female div/dep/mar co applicant 3 car 24 bank own 2 skilled 1 none yes good <0 36 all paid furniture/equipment 2746 <100 >=7 4 male single none 4 car 31 bank own 1 skilled 1 none yes bad Click to expand Features Informations: * Attribute 1: (qualitative) * Status of existing checking account * A11 : ... < 0 DM * A12 : 0 <= ... < 200 DM * A13 : ... >= 200 DM / salary assignments for at least 1 year * A14 : no checking account * Attribute 2: (numerical) * Duration in month * Attribute 3: (qualitative) * Credit history * A30 : no credits taken/ all credits paid back duly * A31 : all credits at this bank paid back duly * A32 : existing credits paid back duly till now * A33 : delay in paying off in the past * A34 : critical account/ other credits existing (not at this bank) * Attribute 4: (qualitative) * Purpose * A40 : car (new) * A41 : car (used) * A42 : furniture/equipment * A43 : radio/television * A44 : domestic appliances * A45 : repairs * A46 : education * A47 : (vacation - does not exist?) * A48 : retraining * A49 : business * A410 : others * Attribute 5: (numerical) * Credit amount * Attibute 6: (qualitative) * Savings account/bonds * A61 : ... < 100 DM * A62 : 100 <= ... < 500 DM * A63 : 500 <= ... < 1000 DM * A64 : .. >= 1000 DM * A65 : unknown/ no savings account * Attribute 7: (qualitative) * Present employment since * A71 : unemployed * A72 : ... < 1 year * A73 : 1 <= ... < 4 years * A74 : 4 <= ... < 7 years * A75 : .. >= 7 years * Attribute 8: (numerical) * Installment rate in percentage of disposable income * Attribute 9: (qualitative) * Personal status and sex * A91 : male : divorced/separated * A92 : female : divorced/separated/married * A93 : male : single * A94 : male : married/widowed * A95 : female : single * Attribute 10: (qualitative) * Other debtors / guarantors * A101 : none * A102 : co-applicant * A103 : guarantor * Attribute 11: (numerical) * Present residence since * Attribute 12: (qualitative) * Property * A121 : real estate * A122 : if not A121 : building society savings agreement/ life insurance * A123 : if not A121/A122 : car or other, not in attribute 6 * A124 : unknown / no property * Attribute 13: (numerical) * Age in years * Attribute 14: (qualitative) * Other installment plans * A141 : bank * A142 : stores * A143 : none * Attribute 15: (qualitative) * Housing * A151 : rent * A152 : own * A153 : for free * Attribute 16: (numerical) * Number of existing credits at this bank * Attribute 17: (qualitative) * Job * A171 : unemployed/ unskilled - non-resident * A172 : unskilled - resident * A173 : skilled employee / official * A174 : management/ self-employed/ * highly qualified employee/ officer * Attribute 18: (numerical) * Number of people being liable to provide maintenance for * Attribute 19: (qualitative) * Telephone * A191 : none * A192 : yes, registered under the customers name * Attribute 20: (qualitative) * foreign worker * A201 : yes * A202 : no","title":""},{"location":"tutorials/CreditScoring/#mindsdb-code-example","text":"from mindsdb import Predictor import pandas as pd from sklearn.metrics import balanced_accuracy_score , confusion_matrix def run ( sample = False ): mdb = Predictor ( name = 'german_data' ) mdb . learn ( to_predict = 'class' , from_data = 'processed_data/train.csv' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) predicted_val = [ x . explanation [ 'class' ][ 'predicted_value' ] for x in predictions ] real_val = list ( pd . read_csv ( 'processed_data/test.csv' )[ 'class' ]) accuracy = balanced_accuracy_score ( real_val , predicted_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend , 'single_row_predictions' : additional_info } # Run as main if __name__ == '__main__' : print ( run ())","title":"MindsDB Code example"},{"location":"tutorials/CreditScoring/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.64880952380 Lightwood 15 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/CustomerLifetimeValue/","text":"Industry Department Role Retail & Online Marketing Marketing Lead Processed Dataset This is a dataset for binary sentiment classification containing a set of 25,000 highly popular movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided. Features informations review sentiment MindsDB Code example import mindsdb from sklearn.metrics import accuracy_score predictor = mindsdb . Predictor ( name = 'movie_sentiment_predictor' ) predictor . learn ( from_data = 'train.tsv' , to_predict = [ 'sentiment' ]) accuracy_data = predictions . test ( 'test.tsv' , accuracy_score ) accuracy_pct = accuracy_data [ 'sentiment_accuracy' ] * 100 print ( f 'Accuracy of { accuracy_pct } % !' ) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.8573 Lightwood 06 February 2020","title":"Custiner Lifetime Value Optimization"},{"location":"tutorials/CustomerLifetimeValue/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/CustomerLifetimeValue/#_1","text":"This is a dataset for binary sentiment classification containing a set of 25,000 highly popular movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided.","title":""},{"location":"tutorials/CustomerLifetimeValue/#features-informations","text":"review sentiment","title":"Features informations"},{"location":"tutorials/CustomerLifetimeValue/#mindsdb-code-example","text":"import mindsdb from sklearn.metrics import accuracy_score predictor = mindsdb . Predictor ( name = 'movie_sentiment_predictor' ) predictor . learn ( from_data = 'train.tsv' , to_predict = [ 'sentiment' ]) accuracy_data = predictions . test ( 'test.tsv' , accuracy_score ) accuracy_pct = accuracy_data [ 'sentiment_accuracy' ] * 100 print ( f 'Accuracy of { accuracy_pct } % !' )","title":"MindsDB Code example"},{"location":"tutorials/CustomerLifetimeValue/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.8573 Lightwood 06 February 2020","title":"Mindsdb accuracy"},{"location":"tutorials/FraudDetection/","text":"Industry Department Role Retail & Online Finance Business executive Processed Dataset The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. The goal is to identify fraudulent credit card transactions. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. Time V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 Amount Class 1 -1.35835 -1.34016 1.77321 0.37978 -0.503198 1.8005 0.791461 0.247676 -1.51465 0.207643 0.624501 0.0660837 0.717293 -0.165946 2.34586 -2.89008 1.10997 -0.121359 -2.26186 0.52498 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.0553528 -0.0597518 378.66 0 4 1.22966 0.141004 0.0453708 1.20261 0.191881 0.272708 -0.005159 0.0812129 0.46496 -0.0992543 -1.41691 -0.153826 -0.751063 0.167372 0.0501436 -0.443587 0.00282051 -0.611987 -0.045575 -0.219633 -0.167716 -0.27071 -0.154104 -0.780055 0.750137 -0.257237 0.0345074 0.00516777 4.99 0 11 1.06937 0.287722 0.828613 2.71252 -0.178398 0.337544 -0.0967169 0.115982 -0.221083 0.46023 -0.773657 0.323387 -0.0110759 -0.178485 -0.655564 -0.199925 0.124005 -0.980496 -0.982916 -0.153197 -0.0368755 0.0744124 -0.0714074 0.104744 0.548265 0.104094 0.0214911 0.0212933 27.5 0 14 -5.40126 -5.45015 1.1863 1.73624 3.04911 -1.76341 -1.55974 0.160842 1.23309 0.345173 0.91723 0.970117 -0.266568 -0.47913 -0.526609 0.472004 -0.725481 0.0750814 -0.406867 -2.19685 -0.5036 0.98446 2.45859 0.0421189 -0.481631 -0.621272 0.392053 0.949594 46.8 0 29 1.11088 0.168717 0.517144 1.32541 -0.191573 0.0195037 -0.0318491 0.11762 0.0176647 0.0448648 1.34507 1.28634 -0.252267 0.274458 -0.810394 -0.587005 0.0874511 -0.550474 -0.154749 -0.19012 -0.0377087 0.0957015 -0.0481976 0.232115 0.606201 -0.342097 0.0367696 0.00747996 6.54 0 33 -0.607877 1.03135 1.74045 1.23211 0.418592 0.119168 0.850893 -0.176267 -0.243501 0.148455 -0.387003 0.398299 0.481917 -0.365439 0.235545 -1.34781 0.504648 -0.798405 0.75971 0.254325 -0.0873292 0.258315 -0.264775 0.118282 0.173508 -0.217041 0.0943119 -0.0330413 14.8 0 35 1.3864 -0.794209 0.778224 -0.864708 -1.06413 0.351296 -1.19145 0.0526856 -0.304404 0.576517 -1.63111 0.0425595 2.0479 -0.739338 1.45622 -0.27205 -0.932007 1.92653 -0.659939 -0.273033 -0.228727 -0.123522 -0.131025 -0.929668 0.181379 1.19493 0.000531332 0.0199106 30.9 0 Click to expand Features Informations: * Time Number of seconds elapsed between this transaction and the first transaction in the dataset * V1may be result of a PCA Dimensionality reduction to protect user identities and sensitive features(v1-v28) * V2 * V3 * V4 * V5 * V6 * V7 * V8 * V9 * V10 * V11 * V12 * V13 * V14 * V15 * V16 * V17 * V18 * V19 * V20 * V21 * V22 * V23 * V24 * V25 * V26 * V27 * V28abc * AmountTransaction amount * Class1 for fraudulent transactions, 0 otherwise MindsDB Code example import mindsdb import pandas as pd from sklearn.metrics import balanced_accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'cc_fraud' ) mdb . learn ( from_data = 'processed_data/train.csv' , to_predict = 'Class' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) pred_val = [ int ( x [ 'Class' ]) for x in predictions ] real_val = [ int ( x ) for x in list ( pd . read_csv ( 'processed_data/test.csv' ))[ 'Class' ])] accuracy = balanced_accuracy_score ( real_val , pred_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'backend' : backend , 'additional info' : additional_info } # Run as main if __name__ == '__main__' : print ( run ()) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.921724518459069 Lightwood 16 April 2020","title":"Fraud Detection"},{"location":"tutorials/FraudDetection/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/FraudDetection/#_1","text":"The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. The goal is to identify fraudulent credit card transactions. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. Time V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21 V22 V23 V24 V25 V26 V27 V28 Amount Class 1 -1.35835 -1.34016 1.77321 0.37978 -0.503198 1.8005 0.791461 0.247676 -1.51465 0.207643 0.624501 0.0660837 0.717293 -0.165946 2.34586 -2.89008 1.10997 -0.121359 -2.26186 0.52498 0.247998 0.771679 0.909412 -0.689281 -0.327642 -0.139097 -0.0553528 -0.0597518 378.66 0 4 1.22966 0.141004 0.0453708 1.20261 0.191881 0.272708 -0.005159 0.0812129 0.46496 -0.0992543 -1.41691 -0.153826 -0.751063 0.167372 0.0501436 -0.443587 0.00282051 -0.611987 -0.045575 -0.219633 -0.167716 -0.27071 -0.154104 -0.780055 0.750137 -0.257237 0.0345074 0.00516777 4.99 0 11 1.06937 0.287722 0.828613 2.71252 -0.178398 0.337544 -0.0967169 0.115982 -0.221083 0.46023 -0.773657 0.323387 -0.0110759 -0.178485 -0.655564 -0.199925 0.124005 -0.980496 -0.982916 -0.153197 -0.0368755 0.0744124 -0.0714074 0.104744 0.548265 0.104094 0.0214911 0.0212933 27.5 0 14 -5.40126 -5.45015 1.1863 1.73624 3.04911 -1.76341 -1.55974 0.160842 1.23309 0.345173 0.91723 0.970117 -0.266568 -0.47913 -0.526609 0.472004 -0.725481 0.0750814 -0.406867 -2.19685 -0.5036 0.98446 2.45859 0.0421189 -0.481631 -0.621272 0.392053 0.949594 46.8 0 29 1.11088 0.168717 0.517144 1.32541 -0.191573 0.0195037 -0.0318491 0.11762 0.0176647 0.0448648 1.34507 1.28634 -0.252267 0.274458 -0.810394 -0.587005 0.0874511 -0.550474 -0.154749 -0.19012 -0.0377087 0.0957015 -0.0481976 0.232115 0.606201 -0.342097 0.0367696 0.00747996 6.54 0 33 -0.607877 1.03135 1.74045 1.23211 0.418592 0.119168 0.850893 -0.176267 -0.243501 0.148455 -0.387003 0.398299 0.481917 -0.365439 0.235545 -1.34781 0.504648 -0.798405 0.75971 0.254325 -0.0873292 0.258315 -0.264775 0.118282 0.173508 -0.217041 0.0943119 -0.0330413 14.8 0 35 1.3864 -0.794209 0.778224 -0.864708 -1.06413 0.351296 -1.19145 0.0526856 -0.304404 0.576517 -1.63111 0.0425595 2.0479 -0.739338 1.45622 -0.27205 -0.932007 1.92653 -0.659939 -0.273033 -0.228727 -0.123522 -0.131025 -0.929668 0.181379 1.19493 0.000531332 0.0199106 30.9 0 Click to expand Features Informations: * Time Number of seconds elapsed between this transaction and the first transaction in the dataset * V1may be result of a PCA Dimensionality reduction to protect user identities and sensitive features(v1-v28) * V2 * V3 * V4 * V5 * V6 * V7 * V8 * V9 * V10 * V11 * V12 * V13 * V14 * V15 * V16 * V17 * V18 * V19 * V20 * V21 * V22 * V23 * V24 * V25 * V26 * V27 * V28abc * AmountTransaction amount * Class1 for fraudulent transactions, 0 otherwise","title":""},{"location":"tutorials/FraudDetection/#mindsdb-code-example","text":"import mindsdb import pandas as pd from sklearn.metrics import balanced_accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'cc_fraud' ) mdb . learn ( from_data = 'processed_data/train.csv' , to_predict = 'Class' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) pred_val = [ int ( x [ 'Class' ]) for x in predictions ] real_val = [ int ( x ) for x in list ( pd . read_csv ( 'processed_data/test.csv' ))[ 'Class' ])] accuracy = balanced_accuracy_score ( real_val , pred_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'backend' : backend , 'additional info' : additional_info } # Run as main if __name__ == '__main__' : print ( run ())","title":"MindsDB Code example"},{"location":"tutorials/FraudDetection/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.921724518459069 Lightwood 16 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/HotelBooking/","text":"Industry Department Role Travel Operations Business executive Processed Dataset This data set contains booking information for a city hotel and a resort hotel, and includes information such as when the booking was made, length of stay, the number of adults, children, and/or babies, and the number of available parking spaces, among other things. hotel is_canceled lead_time arrival_date_year arrival_date_month arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights stays_in_week_nights adults children babies meal country market_segment distribution_channel is_repeated_guest previous_cancellations previous_bookings_not_canceled reserved_room_type assigned_room_type booking_changes deposit_type agent company days_in_waiting_list customer_type adr required_car_parking_spaces total_of_special_requests reservation_status reservation_status_date Resort Hotel 0 342 2015 July 27 1 0 0 2 0 0 BB PRT Direct Direct 0 0 0 C C 3 No Deposit nan nan 0 Transient 0 0 0 Check-Out 2015-07-01 Resort Hotel 0 737 2015 July 27 1 0 0 2 0 0 BB PRT Direct Direct 0 0 0 C C 4 No Deposit nan nan 0 Transient 0 0 0 Check-Out 2015-07-01 Resort Hotel 0 7 2015 July 27 1 0 1 1 0 0 BB GBR Direct Direct 0 0 0 A C 0 No Deposit nan nan 0 Transient 75 0 0 Check-Out 2015-07-02 Resort Hotel 0 13 2015 July 27 1 0 1 1 0 0 BB GBR Corporate Corporate 0 0 0 A A 0 No Deposit 304 nan 0 Transient 75 0 0 Check-Out 2015-07-02 Resort Hotel 0 14 2015 July 27 1 0 2 2 0 0 BB GBR Online TA TA/TO 0 0 0 A A 0 No Deposit 240 nan 0 Transient 98 0 1 Check-Out 2015-07-03 Resort Hotel 0 14 2015 July 27 1 0 2 2 0 0 BB GBR Online TA TA/TO 0 0 0 A A 0 No Deposit 240 nan 0 Transient 98 0 1 Check-Out 2015-07-03 Resort Hotel 0 0 2015 July 27 1 0 2 2 0 0 BB PRT Direct Direct 0 0 0 C C 0 No Deposit nan nan 0 Transient 107 0 0 Check-Out 2015-07-03 Click to expand Features Informations: 1. hotel Hotel (H1 = Resort Hotel or H2 = City Hotel) 2. is_canceled Value indicating if the booking was canceled (1) or not (0) lead_timeNumber of days that elapsed between the entering date of the booking into the PMS and the arrival date 3. arrival_date_year Year of arrival date 4. arrival_date_month Month of arrival date 5. arrival_date_week_number Week number of year for arrival date 6. arrival_date_day_of_month Day of arrival date 7. stays_in_weekend_nights Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel 8. stays_in_week_nightsNumber of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel 9. adultsNumber of adults 10. childrenNumber of children 11. babiesNumber of babies 12. mealType of meal booked. Categories are presented in standard hospitality meal packages: Undefined/SC \u2013 no meal package; BB \u2013 Bed & Breakfast; HB \u2013 Half board (breakfast and one other meal \u2013 usually dinner); FB \u2013 Full board (breakfast, lunch and dinner) 13. countryCountry of origin. Categories are represented in the ISO 3155\u20133:2013 format 14. market_segment Market segment designation. In categories, the term \u201cTA\u201d means \u201cTravel Agents\u201d and \u201cTO\u201d means \u201cTour Operators\u201d 15. distribution_channel Booking distribution channel. The term \u201cTA\u201d means \u201cTravel Agents\u201d and \u201cTO\u201d means \u201cTour Operators\u201d 16. is_repeated_guestValue indicating if the booking name was from a repeated guest (1) or not (0) 17. previous_cancellationsNumber of previous bookings that were cancelled by the customer prior to the current booking 18. previous_bookings_not_canceledNumber of previous bookings not cancelled by the customer prior to the current booking 19. reserved_room_typeCode of room type reserved. Code is presented instead of designation for anonymity reasons. 20. assigned_room_type Code for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons. 21. booking_changes Number of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation 22. deposit_type Indication on if the customer made a deposit to guarantee the booking. This variable can assume three categories: No Deposit \u2013 no deposit was made; Non Refund \u2013 a deposit was made in the value of the total stay cost; Refundable \u2013 a deposit was made with a value under the total cost of stay. 23. agentID of the travel agency that made the booking 24. companyID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons 25. days_in_waiting_list Number of days the booking was in the waiting list before it was confirmed to the customer 26. customer_type Type of booking, assuming one of four categories: Contract - when the booking has an allotment or other type of contract associated to it; Group \u2013 when the booking is associated to a group; Transient \u2013 when the booking is not part of a group or contract, and is not associated to other 27. transient booking; Transient-party \u2013 when the booking is transient, but is associated to at least other transient booking adrAverage Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights 28. required_car_parking_spaces Number of car parking spaces required by the customer 29. total_of_special_requests Number of special requests made by the customer (e.g. twin bed or high floor) 30. reservation_status Reservation last status, assuming one of three categories: Canceled \u2013 booking was canceled by the customer; Check-Out \u2013 customer has checked in but already departed; No-Show \u2013 customer did not check-in and did inform the hotel of the reason why 31. reservation_status_date Date at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel MindsDB Code example import mindsdb import sys import pandas as pd from sklearn.metrics import balanced_accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'hotel_booking' ) mdb . learn ( from_data = 'dataset/train.csv' , to_predict = 'is_canceled' ) test_df = pd . read_csv ( 'dataset/test.csv' ) predictions = mdb . predict ( when_data = 'dataset/test.csv' ) results = [ str ( x [ 'is_canceled' ]) for x in predictions ] real = list ( map ( str , list ( test_df [ 'is_canceled' ]))) accuracy = balanced_accuracy_score ( real , results ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend , 'additional_info' : additional_info } if __name__ == '__main__' : result = run () print ( result ) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.8414694158197122, Lightwood 17 April 2020","title":"Hotel Booking Demand"},{"location":"tutorials/HotelBooking/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/HotelBooking/#_1","text":"This data set contains booking information for a city hotel and a resort hotel, and includes information such as when the booking was made, length of stay, the number of adults, children, and/or babies, and the number of available parking spaces, among other things. hotel is_canceled lead_time arrival_date_year arrival_date_month arrival_date_week_number arrival_date_day_of_month stays_in_weekend_nights stays_in_week_nights adults children babies meal country market_segment distribution_channel is_repeated_guest previous_cancellations previous_bookings_not_canceled reserved_room_type assigned_room_type booking_changes deposit_type agent company days_in_waiting_list customer_type adr required_car_parking_spaces total_of_special_requests reservation_status reservation_status_date Resort Hotel 0 342 2015 July 27 1 0 0 2 0 0 BB PRT Direct Direct 0 0 0 C C 3 No Deposit nan nan 0 Transient 0 0 0 Check-Out 2015-07-01 Resort Hotel 0 737 2015 July 27 1 0 0 2 0 0 BB PRT Direct Direct 0 0 0 C C 4 No Deposit nan nan 0 Transient 0 0 0 Check-Out 2015-07-01 Resort Hotel 0 7 2015 July 27 1 0 1 1 0 0 BB GBR Direct Direct 0 0 0 A C 0 No Deposit nan nan 0 Transient 75 0 0 Check-Out 2015-07-02 Resort Hotel 0 13 2015 July 27 1 0 1 1 0 0 BB GBR Corporate Corporate 0 0 0 A A 0 No Deposit 304 nan 0 Transient 75 0 0 Check-Out 2015-07-02 Resort Hotel 0 14 2015 July 27 1 0 2 2 0 0 BB GBR Online TA TA/TO 0 0 0 A A 0 No Deposit 240 nan 0 Transient 98 0 1 Check-Out 2015-07-03 Resort Hotel 0 14 2015 July 27 1 0 2 2 0 0 BB GBR Online TA TA/TO 0 0 0 A A 0 No Deposit 240 nan 0 Transient 98 0 1 Check-Out 2015-07-03 Resort Hotel 0 0 2015 July 27 1 0 2 2 0 0 BB PRT Direct Direct 0 0 0 C C 0 No Deposit nan nan 0 Transient 107 0 0 Check-Out 2015-07-03 Click to expand Features Informations: 1. hotel Hotel (H1 = Resort Hotel or H2 = City Hotel) 2. is_canceled Value indicating if the booking was canceled (1) or not (0) lead_timeNumber of days that elapsed between the entering date of the booking into the PMS and the arrival date 3. arrival_date_year Year of arrival date 4. arrival_date_month Month of arrival date 5. arrival_date_week_number Week number of year for arrival date 6. arrival_date_day_of_month Day of arrival date 7. stays_in_weekend_nights Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel 8. stays_in_week_nightsNumber of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel 9. adultsNumber of adults 10. childrenNumber of children 11. babiesNumber of babies 12. mealType of meal booked. Categories are presented in standard hospitality meal packages: Undefined/SC \u2013 no meal package; BB \u2013 Bed & Breakfast; HB \u2013 Half board (breakfast and one other meal \u2013 usually dinner); FB \u2013 Full board (breakfast, lunch and dinner) 13. countryCountry of origin. Categories are represented in the ISO 3155\u20133:2013 format 14. market_segment Market segment designation. In categories, the term \u201cTA\u201d means \u201cTravel Agents\u201d and \u201cTO\u201d means \u201cTour Operators\u201d 15. distribution_channel Booking distribution channel. The term \u201cTA\u201d means \u201cTravel Agents\u201d and \u201cTO\u201d means \u201cTour Operators\u201d 16. is_repeated_guestValue indicating if the booking name was from a repeated guest (1) or not (0) 17. previous_cancellationsNumber of previous bookings that were cancelled by the customer prior to the current booking 18. previous_bookings_not_canceledNumber of previous bookings not cancelled by the customer prior to the current booking 19. reserved_room_typeCode of room type reserved. Code is presented instead of designation for anonymity reasons. 20. assigned_room_type Code for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons. 21. booking_changes Number of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation 22. deposit_type Indication on if the customer made a deposit to guarantee the booking. This variable can assume three categories: No Deposit \u2013 no deposit was made; Non Refund \u2013 a deposit was made in the value of the total stay cost; Refundable \u2013 a deposit was made with a value under the total cost of stay. 23. agentID of the travel agency that made the booking 24. companyID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons 25. days_in_waiting_list Number of days the booking was in the waiting list before it was confirmed to the customer 26. customer_type Type of booking, assuming one of four categories: Contract - when the booking has an allotment or other type of contract associated to it; Group \u2013 when the booking is associated to a group; Transient \u2013 when the booking is not part of a group or contract, and is not associated to other 27. transient booking; Transient-party \u2013 when the booking is transient, but is associated to at least other transient booking adrAverage Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights 28. required_car_parking_spaces Number of car parking spaces required by the customer 29. total_of_special_requests Number of special requests made by the customer (e.g. twin bed or high floor) 30. reservation_status Reservation last status, assuming one of three categories: Canceled \u2013 booking was canceled by the customer; Check-Out \u2013 customer has checked in but already departed; No-Show \u2013 customer did not check-in and did inform the hotel of the reason why 31. reservation_status_date Date at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel","title":""},{"location":"tutorials/HotelBooking/#mindsdb-code-example","text":"import mindsdb import sys import pandas as pd from sklearn.metrics import balanced_accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'hotel_booking' ) mdb . learn ( from_data = 'dataset/train.csv' , to_predict = 'is_canceled' ) test_df = pd . read_csv ( 'dataset/test.csv' ) predictions = mdb . predict ( when_data = 'dataset/test.csv' ) results = [ str ( x [ 'is_canceled' ]) for x in predictions ] real = list ( map ( str , list ( test_df [ 'is_canceled' ]))) accuracy = balanced_accuracy_score ( real , results ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend , 'additional_info' : additional_info } if __name__ == '__main__' : result = run () print ( result )","title":"MindsDB Code example"},{"location":"tutorials/HotelBooking/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.8414694158197122, Lightwood 17 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/MedicalDiagnosis/","text":"Industry Department Role Health Care Health Business Executive/Physician Breast Cancer Wisconsin (Diagnostic) Data Set . From the given information of the breast cancer dataset, classify whether it is a malignant cancer or benign cancer. diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave_points_mean symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst 0 17.99 10.38 122.8 1001 0.1184 0.2776 0.3001 0.1471 0.2419 0.07871 1.095 0.9053 8.589 153.4 0.006399 0.04904 0.05373 0.01587 0.03003 0.006193 25.38 17.33 184.6 2019 0.1622 0.6656 0.7119 0.2654 0.4601 0.1189 0 20.57 17.77 132.9 1326 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 0.5435 0.7339 3.398 74.08 0.005225 0.01308 0.0186 0.0134 0.01389 0.003532 24.99 23.41 158.8 1956 0.1238 0.1866 0.2416 0.186 0.275 0.08902 0 19.69 21.25 130 1203 0.1096 0.1599 0.1974 0.1279 0.2069 0.05999 0.7456 0.7869 4.585 94.03 0.00615 0.04006 0.03832 0.02058 0.0225 0.004571 23.57 25.53 152.5 1709 0.1444 0.4245 0.4504 0.243 0.3613 0.08758 0 11.42 20.38 77.58 386.1 0.1425 0.2839 0.2414 0.1052 0.2597 0.09744 0.4956 1.156 3.445 27.23 0.00911 0.07458 0.05661 0.01867 0.05963 0.009208 14.91 26.5 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.173 0 20.29 14.34 135.1 1297 0.1003 0.1328 0.198 0.1043 0.1809 0.05883 0.7572 0.7813 5.438 94.44 0.01149 0.02461 0.05688 0.01885 0.01756 0.005115 22.54 16.67 152.2 1575 0.1374 0.205 0.4 0.1625 0.2364 0.07678 0 12.45 15.7 82.57 477.1 0.1278 0.17 0.1578 0.08089 0.2087 0.07613 0.3345 0.8902 2.217 27.19 0.00751 0.03345 0.03672 0.01137 0.02165 0.005082 15.47 23.75 103.4 741.6 0.1791 0.5249 0.5355 0.1741 0.3985 0.1244 0 18.25 19.98 119.6 1040 0.09463 0.109 0.1127 0.074 0.1794 0.05742 0.4467 0.7732 3.18 53.91 0.004314 0.01382 0.02254 0.01039 0.01369 0.002179 22.88 27.66 153.2 1606 0.1442 0.2576 0.3784 0.1932 0.3063 0.08368 Click to expand Features Informations: 1. id ID number 2. diagnosis The diagnosis of breast tissues (M = malignant, B = benign) 3. radius_mean mean of distances from center to points on the perimeter 4. texture_means tandard deviation of gray-scale values 5. perimeter_mean mean size of the core tumor 6. area_mean 7. smoothness_mean mean of local variation in radius lengths 8. compactness_mean mean of perimeter^2 / area - 1.0 9. concavity_mean mean of severity of concave portions of the contour 10. concave points_mean mean for number of concave portions of the contour 11. symmetry_mean 12. fractal_dimension_mean mean for \"coastline approximation\" - 1 13. radius_sestandard error for the mean of distances from center to points on the perimeter 14. texture_sestandard error for standard deviation of gray-scale values 15. perimeter_se 16. area_se 17. smoothness_sestandard error for local variation in radius lengths 18. compactness_sestandard error for perimeter^2 / area - 1.0 19. concavity_sestandard error for severity of concave portions of the contour concave points_sestandard error for number of concave portions of the contour 20. symmetry_se 21. fractal_dimension_sestandard error for \"coastline approximation\" - 1 22. radius_worst\"worst\" or largest mean value for mean of distances from center to points on the perimeter 23. texture_worst\"worst\" or largest mean value for standard deviation of gray-scale values 24. perimeter_worst 25. area_worst 26. smoothness_worst \"worst\" or largest mean value for local variation in radius lengths 27. compactness_worst \"worst\" or largest mean value for perimeter^2 / area - 1.0 28. concavity_worst \"worst\" or largest mean value for severity of 29. 29. concave portions of the contour 30. concave points_worst \"worst\" or largest mean value for number of concave portions of the contour 31. symmetry_worst 32. fractal_dimension_worst\"worst\" or largest mean value for \"coastline approximation\" - 1 MindsDB Code example import mindsdb import sys import pandas as pd from sklearn.metrics import balanced_accuracy_score def run ( sample ): mdb = mindsdb . Predictor ( name = 'cancer_model' ) mdb . learn ( from_data = 'processed_data/train.csv' , to_predict = 'diagnosis' ) test_df = pd . read_csv ( 'processed_data/test.csv' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) results = [ str ( x [ 'diagnosis' ]) for x in predictions ] real = list ( map ( str , list ( test_df [ 'diagnosis' ]))) accuracy = balanced_accuracy_score ( real , results ) return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend } if __name__ == '__main__' : sample = bool ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else False result = run ( sample ) print ( result ) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.9666666666666667 Lightwood 17 April 2020","title":"Medical Diagnosis"},{"location":"tutorials/MedicalDiagnosis/#_1","text":"Breast Cancer Wisconsin (Diagnostic) Data Set . From the given information of the breast cancer dataset, classify whether it is a malignant cancer or benign cancer. diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave_points_mean symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst 0 17.99 10.38 122.8 1001 0.1184 0.2776 0.3001 0.1471 0.2419 0.07871 1.095 0.9053 8.589 153.4 0.006399 0.04904 0.05373 0.01587 0.03003 0.006193 25.38 17.33 184.6 2019 0.1622 0.6656 0.7119 0.2654 0.4601 0.1189 0 20.57 17.77 132.9 1326 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 0.5435 0.7339 3.398 74.08 0.005225 0.01308 0.0186 0.0134 0.01389 0.003532 24.99 23.41 158.8 1956 0.1238 0.1866 0.2416 0.186 0.275 0.08902 0 19.69 21.25 130 1203 0.1096 0.1599 0.1974 0.1279 0.2069 0.05999 0.7456 0.7869 4.585 94.03 0.00615 0.04006 0.03832 0.02058 0.0225 0.004571 23.57 25.53 152.5 1709 0.1444 0.4245 0.4504 0.243 0.3613 0.08758 0 11.42 20.38 77.58 386.1 0.1425 0.2839 0.2414 0.1052 0.2597 0.09744 0.4956 1.156 3.445 27.23 0.00911 0.07458 0.05661 0.01867 0.05963 0.009208 14.91 26.5 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.173 0 20.29 14.34 135.1 1297 0.1003 0.1328 0.198 0.1043 0.1809 0.05883 0.7572 0.7813 5.438 94.44 0.01149 0.02461 0.05688 0.01885 0.01756 0.005115 22.54 16.67 152.2 1575 0.1374 0.205 0.4 0.1625 0.2364 0.07678 0 12.45 15.7 82.57 477.1 0.1278 0.17 0.1578 0.08089 0.2087 0.07613 0.3345 0.8902 2.217 27.19 0.00751 0.03345 0.03672 0.01137 0.02165 0.005082 15.47 23.75 103.4 741.6 0.1791 0.5249 0.5355 0.1741 0.3985 0.1244 0 18.25 19.98 119.6 1040 0.09463 0.109 0.1127 0.074 0.1794 0.05742 0.4467 0.7732 3.18 53.91 0.004314 0.01382 0.02254 0.01039 0.01369 0.002179 22.88 27.66 153.2 1606 0.1442 0.2576 0.3784 0.1932 0.3063 0.08368 Click to expand Features Informations: 1. id ID number 2. diagnosis The diagnosis of breast tissues (M = malignant, B = benign) 3. radius_mean mean of distances from center to points on the perimeter 4. texture_means tandard deviation of gray-scale values 5. perimeter_mean mean size of the core tumor 6. area_mean 7. smoothness_mean mean of local variation in radius lengths 8. compactness_mean mean of perimeter^2 / area - 1.0 9. concavity_mean mean of severity of concave portions of the contour 10. concave points_mean mean for number of concave portions of the contour 11. symmetry_mean 12. fractal_dimension_mean mean for \"coastline approximation\" - 1 13. radius_sestandard error for the mean of distances from center to points on the perimeter 14. texture_sestandard error for standard deviation of gray-scale values 15. perimeter_se 16. area_se 17. smoothness_sestandard error for local variation in radius lengths 18. compactness_sestandard error for perimeter^2 / area - 1.0 19. concavity_sestandard error for severity of concave portions of the contour concave points_sestandard error for number of concave portions of the contour 20. symmetry_se 21. fractal_dimension_sestandard error for \"coastline approximation\" - 1 22. radius_worst\"worst\" or largest mean value for mean of distances from center to points on the perimeter 23. texture_worst\"worst\" or largest mean value for standard deviation of gray-scale values 24. perimeter_worst 25. area_worst 26. smoothness_worst \"worst\" or largest mean value for local variation in radius lengths 27. compactness_worst \"worst\" or largest mean value for perimeter^2 / area - 1.0 28. concavity_worst \"worst\" or largest mean value for severity of 29. 29. concave portions of the contour 30. concave points_worst \"worst\" or largest mean value for number of concave portions of the contour 31. symmetry_worst 32. fractal_dimension_worst\"worst\" or largest mean value for \"coastline approximation\" - 1","title":""},{"location":"tutorials/MedicalDiagnosis/#mindsdb-code-example","text":"import mindsdb import sys import pandas as pd from sklearn.metrics import balanced_accuracy_score def run ( sample ): mdb = mindsdb . Predictor ( name = 'cancer_model' ) mdb . learn ( from_data = 'processed_data/train.csv' , to_predict = 'diagnosis' ) test_df = pd . read_csv ( 'processed_data/test.csv' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) results = [ str ( x [ 'diagnosis' ]) for x in predictions ] real = list ( map ( str , list ( test_df [ 'diagnosis' ]))) accuracy = balanced_accuracy_score ( real , results ) return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend } if __name__ == '__main__' : sample = bool ( sys . argv [ 1 ]) if len ( sys . argv ) > 1 else False result = run ( sample ) print ( result )","title":"MindsDB Code example"},{"location":"tutorials/MedicalDiagnosis/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.9666666666666667 Lightwood 17 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/PatientHealthOutcomes/","text":"Industry Department Role Health Care Health Business executive / Physician Processed Dataset In the Heart Disease UCI dataset, the data comes from 4 databases: the Hungarian Institute of Cardiology, the University Hospital in Zurich, the University Hospital in Basel Switzerland, and the V.A. Medical Center Long Beach and Cleveland Clinic Foundation. The \"goal\" is to determine the presence of heart disease in the patient. age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 37 1 2 130 250 0 1 187 0 3.5 0 0 2 1 41 0 1 130 204 0 0 172 0 1.4 2 0 2 1 56 1 1 120 236 0 1 178 0 0.8 2 0 2 1 57 0 0 120 354 0 1 163 1 0.6 2 0 2 1 57 1 0 140 192 0 1 148 0 0.4 1 0 1 1 56 0 1 140 294 0 0 153 0 1.3 1 0 2 1 Click to expand Features Informations: 1. age: age in years 2. sex: sex (1 = male; 0 = female) 3. cp: chest pain type * Value 1: typical angina * Value 2: atypical angina * Value 3: non-anginal pain * Value 4: asymptomatic 4. trestbps: resting blood pressure (in mm Hg on admission to the hospital) 5. chol: serum cholestoral in mg/dl 6. fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) 7. restecg: resting electrocardiographic results * Value 0: normal * Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria 8. thalach: maximum heart rate achieved 9. exang: exercise induced angina (1 = yes; 0 = no) 10. oldpeak = ST depression induced by exercise relative to rest 11. slope: the slope of the peak exercise ST segment * Value 1: upsloping * Value 2: flat * Value 3: downsloping 12. ca: number of major vessels (0-3) colored by flourosopy 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect 14. num: diagnosis of heart disease (angiographic disease status) * Value 0: < 50% diameter narrowing * Value 1: > 50% diameter narrowing MindsDB Code example import mindsdb import pandas as pd from sklearn.metrics import balanced_accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'hd' ) mdb . learn ( from_data = 'processed_data/train.csv' , to_predict = 'target' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) pred_val = [ int ( x [ 'target' ]) for x in predictions ] real_val = [ int ( x ) for x in list ( pd . read_csv ( 'processed_data/test.csv' )[ 'target' ])] accuracy = balanced_accuracy_score ( real_val , pred_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'backend' : backend , 'additional info' : additional_info } # Run as main if __name__ == '__main__' : print ( run ()) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.8256302521008403 Lightwood 16 April 2020","title":"Patient Health"},{"location":"tutorials/PatientHealthOutcomes/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/PatientHealthOutcomes/#_1","text":"In the Heart Disease UCI dataset, the data comes from 4 databases: the Hungarian Institute of Cardiology, the University Hospital in Zurich, the University Hospital in Basel Switzerland, and the V.A. Medical Center Long Beach and Cleveland Clinic Foundation. The \"goal\" is to determine the presence of heart disease in the patient. age sex cp trestbps chol fbs restecg thalach exang oldpeak slope ca thal target 63 1 3 145 233 1 0 150 0 2.3 0 0 1 1 37 1 2 130 250 0 1 187 0 3.5 0 0 2 1 41 0 1 130 204 0 0 172 0 1.4 2 0 2 1 56 1 1 120 236 0 1 178 0 0.8 2 0 2 1 57 0 0 120 354 0 1 163 1 0.6 2 0 2 1 57 1 0 140 192 0 1 148 0 0.4 1 0 1 1 56 0 1 140 294 0 0 153 0 1.3 1 0 2 1 Click to expand Features Informations: 1. age: age in years 2. sex: sex (1 = male; 0 = female) 3. cp: chest pain type * Value 1: typical angina * Value 2: atypical angina * Value 3: non-anginal pain * Value 4: asymptomatic 4. trestbps: resting blood pressure (in mm Hg on admission to the hospital) 5. chol: serum cholestoral in mg/dl 6. fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) 7. restecg: resting electrocardiographic results * Value 0: normal * Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria 8. thalach: maximum heart rate achieved 9. exang: exercise induced angina (1 = yes; 0 = no) 10. oldpeak = ST depression induced by exercise relative to rest 11. slope: the slope of the peak exercise ST segment * Value 1: upsloping * Value 2: flat * Value 3: downsloping 12. ca: number of major vessels (0-3) colored by flourosopy 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect 14. num: diagnosis of heart disease (angiographic disease status) * Value 0: < 50% diameter narrowing * Value 1: > 50% diameter narrowing","title":""},{"location":"tutorials/PatientHealthOutcomes/#mindsdb-code-example","text":"import mindsdb import pandas as pd from sklearn.metrics import balanced_accuracy_score def run (): mdb = mindsdb . Predictor ( name = 'hd' ) mdb . learn ( from_data = 'processed_data/train.csv' , to_predict = 'target' ) predictions = mdb . predict ( when_data = 'processed_data/test.csv' ) pred_val = [ int ( x [ 'target' ]) for x in predictions ] real_val = [ int ( x ) for x in list ( pd . read_csv ( 'processed_data/test.csv' )[ 'target' ])] accuracy = balanced_accuracy_score ( real_val , pred_val ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'backend' : backend , 'additional info' : additional_info } # Run as main if __name__ == '__main__' : print ( run ())","title":"MindsDB Code example"},{"location":"tutorials/PatientHealthOutcomes/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.8256302521008403 Lightwood 16 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/PredictiveMaintenance/","text":"Industry Department Role High-Tech & Manufacturing Operations Data Scientist Processed Dataset This dataset contains force and torque measurements on a robot after failure detection. Each failure is characterized by 15 force/torque samples collected at regular time intervals. id time F_x F_y F_z T_x T_y T_z target 1 0 -1 -1 63 -3 -1 0 True 1 1 0 0 62 -3 -1 0 True 1 2 -1 -1 61 -3 0 0 True 1 3 -1 -1 63 -2 -1 0 True 1 4 -1 -1 63 -3 -1 0 True 1 5 -1 -1 63 -3 -1 0 True 1 6 -1 -1 63 -3 0 0 True Click to expand Features Informations: id time F_x F_y F_z T_x T_y T_z target Fx1 ... Fx15 is the evolution of force Fx in the observation window import mindsdb import pandas as pd from sklearn.metrics import r2_score def run (): mdb = mindsdb . Predictor ( name = 'robotic_failure' ) mdb . learn ( from_data = 'dataset/train.csv' , to_predict = [ 'target' ]) predictions = mdb . predict ( when = 'test.csv' ) pred_val = [ x [ 'target' ] for x in predictions ] real_val = list ( pd . read_csv ( 'dataset/test.csv' )[ 'target' ]) accuracy = r2_score ( real_val , pred_val ) print ( f 'Got an r2 score of: { accuracy } ' ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend , 'additional_info' : additional_info } # Run as main if __name__ == '__main__' : print ( run ()) Mindsdb accuracy Accuracy Backend Last run MindsDB Version Latest Version 0.8399922571492469 Lightwood 15 April 2020","title":"Predictive Maintenance"},{"location":"tutorials/PredictiveMaintenance/#processed-dataset","text":"","title":"Processed Dataset"},{"location":"tutorials/PredictiveMaintenance/#_1","text":"This dataset contains force and torque measurements on a robot after failure detection. Each failure is characterized by 15 force/torque samples collected at regular time intervals. id time F_x F_y F_z T_x T_y T_z target 1 0 -1 -1 63 -3 -1 0 True 1 1 0 0 62 -3 -1 0 True 1 2 -1 -1 61 -3 0 0 True 1 3 -1 -1 63 -2 -1 0 True 1 4 -1 -1 63 -3 -1 0 True 1 5 -1 -1 63 -3 -1 0 True 1 6 -1 -1 63 -3 0 0 True Click to expand Features Informations: id time F_x F_y F_z T_x T_y T_z target Fx1 ... Fx15 is the evolution of force Fx in the observation window import mindsdb import pandas as pd from sklearn.metrics import r2_score def run (): mdb = mindsdb . Predictor ( name = 'robotic_failure' ) mdb . learn ( from_data = 'dataset/train.csv' , to_predict = [ 'target' ]) predictions = mdb . predict ( when = 'test.csv' ) pred_val = [ x [ 'target' ] for x in predictions ] real_val = list ( pd . read_csv ( 'dataset/test.csv' )[ 'target' ]) accuracy = r2_score ( real_val , pred_val ) print ( f 'Got an r2 score of: { accuracy } ' ) #show additional info for each transaction row additional_info = [ x . explanation for x in predictions ] return { 'accuracy' : accuracy , 'accuracy_function' : 'balanced_accuracy_score' , 'backend' : backend , 'additional_info' : additional_info } # Run as main if __name__ == '__main__' : print ( run ())","title":""},{"location":"tutorials/PredictiveMaintenance/#mindsdb-accuracy","text":"Accuracy Backend Last run MindsDB Version Latest Version 0.8399922571492469 Lightwood 15 April 2020","title":"Mindsdb accuracy"},{"location":"tutorials/Timeseries/","text":"Handling Timeseries Data Timeseries interface A timeseries is a problem where rows are related to each other in a sequential way, such that the prediction of the value in the present row should take into account a number of previous rows. To build a timeseries model you need to pass timeseries_settings dictionary to learn timeseries_settings = { order_by: List<String> | Mandatory window: Int | Mandatory group_by: List<String> | Optional (default: []) nr_predictions: Int | Optional (default: 1) use_previous_target: Bool | Optional (default: True) historical_columns: List<String> | Optional (default: []) } Let's go through these settings one by one: order_by - The columns based on which the data should be ordered group_by - The columns based on which to group multiple unrelated entities present in your timeseries data. For example, let's say your data consists of sequential readings from 3x sensors. Treating the problem as a timeseries makes sense for individual sensors, so you would specify: group_by=['sensor_id'] nr_predictions - The number of points in the future that predictions should be made for, defaults to 1 . Once trained, the model will be able to predict up to this many points into the future. use_previous_target - Use the previous values of the target column[s] for making predictions. Defaults to True . window - The number of rows to \"look back\" into when making a prediction, after the rows are ordered by the order_by column and split into groups. historical_columns - The temporal dynamics of these columns will be used as additional context to train the time series encoder of the predictor. Note that non-historical columns will also be used to forecast, though without considering their change through time [Status: Experimental]. Code example import mindsdb mdb = mindsdb . Predictor ( name = 'assembly_machines_model' ) mdb . learn ( from_data = 'assembly_machines_historical_data.tsv' , to_predict = 'failure' , timeseries_settings = { 'order_by' : [ 'timestamp' ], # Order the observations by timestamp 'group_by' : [ 'machine_id' ], # The ordering should be done on a per-machine basis, rather than for every single row 'nr_predictions' : 3 , # Predict failures for the timestamp given and for 2 more timesteps in the future 'use_previous_target' : True , # Use the previous values in the target column (`failure`), since when the last failure happened could be a relevant data-point for our prediction. 'window' : 20 # Consider the previous 20 rows for every single row our model is trying to predict 'historical_columns' : [ 'sensor_activity' ] # Mark `sensor_activity` column as historical, to use its temporal dynamics as additional context } ) results = mdb . predict ( when_data = 'new_assembly_machines_data.tsv' ) Historical data When making timeseries predictions, it's important to provide mindsdb with the context for those predictions, i.e. with the previous rows that came before the one you are trying to predict for. Say your columns are: date, nr_customers, store . You order by date , group by store and need to predict nr_customers . You set window=3 . You train your model and then want to make a prediction using a csv with the following content: date, nr_customers, store 2020-10-06, unknown , A1 This prediction will be less than ideal, since mindsdb doesn't know how many customers came to the store on 2020-10-05 or 2020-10-04 , which is probably the main insight the trained model is using to make predictions. So instead you need to pass a file with the following content: date, nr_customers, store 2020-10-04, 55 , A1 2020-10-05, 123 , A1 2020-10-06, None , A1 Note that mindsdb will generate a prediction for every row here (even if the target value nr_customers already exists), but you only care about the prediction for the last row, the previous 2 are there to provide historical context. Also note that, if you window was, say, equal to 5 , you would have had to provide 4 more rows instead of 2 more. Also note that, if you were to give the file: date, nr_customers, store 2020-10-04, 55 , B11 2020-10-05, 123 , A2 2020-10-06, None , A1 This wouldn't count as historical context, since you are grouping by the store column, so only rows where store is A1 will be relevant historical context for predicting a row where the store == A1 Database integration There is an experimental feature, when you train mindsdb from a database, that auto-generates a query to select historical context, based on the query you used to source your training data. This can be enabled by passing advanced_args={'use_database_history': True} to the predict call (or to the SELECT call if operating from within a database). This is still very experimental and has many blindspots, so if you're interested in using this please contact us so we can help and get your feedback on how to improve this. Database example (from SQL) -- Pending, feel free to contribute some or ask us directly about this feature Database example (from code) -- Pending, feel free to contribute some or ask us directly about this feature","title":"Timeseries Data"},{"location":"tutorials/Timeseries/#handling-timeseries-data","text":"","title":"Handling Timeseries Data"},{"location":"tutorials/Timeseries/#timeseries-interface","text":"A timeseries is a problem where rows are related to each other in a sequential way, such that the prediction of the value in the present row should take into account a number of previous rows. To build a timeseries model you need to pass timeseries_settings dictionary to learn timeseries_settings = { order_by: List<String> | Mandatory window: Int | Mandatory group_by: List<String> | Optional (default: []) nr_predictions: Int | Optional (default: 1) use_previous_target: Bool | Optional (default: True) historical_columns: List<String> | Optional (default: []) } Let's go through these settings one by one: order_by - The columns based on which the data should be ordered group_by - The columns based on which to group multiple unrelated entities present in your timeseries data. For example, let's say your data consists of sequential readings from 3x sensors. Treating the problem as a timeseries makes sense for individual sensors, so you would specify: group_by=['sensor_id'] nr_predictions - The number of points in the future that predictions should be made for, defaults to 1 . Once trained, the model will be able to predict up to this many points into the future. use_previous_target - Use the previous values of the target column[s] for making predictions. Defaults to True . window - The number of rows to \"look back\" into when making a prediction, after the rows are ordered by the order_by column and split into groups. historical_columns - The temporal dynamics of these columns will be used as additional context to train the time series encoder of the predictor. Note that non-historical columns will also be used to forecast, though without considering their change through time [Status: Experimental].","title":"Timeseries interface"},{"location":"tutorials/Timeseries/#code-example","text":"import mindsdb mdb = mindsdb . Predictor ( name = 'assembly_machines_model' ) mdb . learn ( from_data = 'assembly_machines_historical_data.tsv' , to_predict = 'failure' , timeseries_settings = { 'order_by' : [ 'timestamp' ], # Order the observations by timestamp 'group_by' : [ 'machine_id' ], # The ordering should be done on a per-machine basis, rather than for every single row 'nr_predictions' : 3 , # Predict failures for the timestamp given and for 2 more timesteps in the future 'use_previous_target' : True , # Use the previous values in the target column (`failure`), since when the last failure happened could be a relevant data-point for our prediction. 'window' : 20 # Consider the previous 20 rows for every single row our model is trying to predict 'historical_columns' : [ 'sensor_activity' ] # Mark `sensor_activity` column as historical, to use its temporal dynamics as additional context } ) results = mdb . predict ( when_data = 'new_assembly_machines_data.tsv' )","title":"Code example"},{"location":"tutorials/Timeseries/#historical-data","text":"When making timeseries predictions, it's important to provide mindsdb with the context for those predictions, i.e. with the previous rows that came before the one you are trying to predict for. Say your columns are: date, nr_customers, store . You order by date , group by store and need to predict nr_customers . You set window=3 . You train your model and then want to make a prediction using a csv with the following content: date, nr_customers, store 2020-10-06, unknown , A1 This prediction will be less than ideal, since mindsdb doesn't know how many customers came to the store on 2020-10-05 or 2020-10-04 , which is probably the main insight the trained model is using to make predictions. So instead you need to pass a file with the following content: date, nr_customers, store 2020-10-04, 55 , A1 2020-10-05, 123 , A1 2020-10-06, None , A1 Note that mindsdb will generate a prediction for every row here (even if the target value nr_customers already exists), but you only care about the prediction for the last row, the previous 2 are there to provide historical context. Also note that, if you window was, say, equal to 5 , you would have had to provide 4 more rows instead of 2 more. Also note that, if you were to give the file: date, nr_customers, store 2020-10-04, 55 , B11 2020-10-05, 123 , A2 2020-10-06, None , A1 This wouldn't count as historical context, since you are grouping by the store column, so only rows where store is A1 will be relevant historical context for predicting a row where the store == A1","title":"Historical data"},{"location":"tutorials/Timeseries/#database-integration","text":"There is an experimental feature, when you train mindsdb from a database, that auto-generates a query to select historical context, based on the query you used to source your training data. This can be enabled by passing advanced_args={'use_database_history': True} to the predict call (or to the SELECT call if operating from within a database). This is still very experimental and has many blindspots, so if you're interested in using this please contact us so we can help and get your feedback on how to improve this.","title":"Database integration"},{"location":"tutorials/Timeseries/#database-example-from-sql","text":"-- Pending, feel free to contribute some or ask us directly about this feature","title":"Database example (from SQL)"},{"location":"tutorials/Timeseries/#database-example-from-code","text":"-- Pending, feel free to contribute some or ask us directly about this feature","title":"Database example (from code)"},{"location":"tutorials/clickhouse/","text":"Machine Learning Models as Tables We will start this article by raising one of the most asked questions regarding Machine Learning, What is the difference between Machine Learning and Artificial Intelligence? When we think about machine learning we can think about it as a subset of Artificial Intelligence. In simple words, the idea behind machine learning is to enable machines to learn by themselves, by using small to large datasets and finding common patterns inside the data. That said, data is the core of any machine learning algorithm and having access to the data is one of the crucial steps for machine learning success. This will bring us to the second question, Where data lives these days? A great deal in databases With the increase of data in volume, variety, velocity in today's databases. Is there a better place to bring machine learning to, than being able to do machine learning right in the databases?We believe that database users meet the most important aspect of applied machine learning, which is to understand what predictive questions are important and what data is relevant to answer those questions. Additionally, adding the statistical analysis for creating the most appropriate model to that, will yield the best combination which is auto machine learning straight from the database. Bringing AutoML to those that know data best can significantly augment the capacity to solve important problems. That\u2019s why we decided to build a seamless integration with Clickhouse, in a way such that any ClickHouse user can create, train and test machine learning models with the same knowledge they have of Structured Query Language (SQL). How can we achieve this? We make use of ClickHouse\u2019s neat capabilities of accessing external tables as if they were internal tables. As such, the integration of these models is painless and transparent allowing us to: * Exposing machine learning models like tables that can be queried. You simply SELECT what you want to predict and you pass in the WHERE statement the conditions for the prediction. * Automatically, build, test and train machine learning models with a simple INSERT statement, where you specify what you want to learn and from what query. Why MindsDB? MindsDB is a fast-growing Open Source AutoML framework with built-in explainability - a unique visualization capability that helps better understand and trust the accuracy of predictions. With MindsDB, developers can build, train and test Machine Learning models, without needing the help of a Data Scientist/Machine Learning Engineer. It is different from typical AutoML frameworks in that MindsDB has a strong focus on trustworthiness by means of explainability, allowing users to get valuable insights into why and how the model is reaching its predictions. Why ClickHouse? Speed and efficiency are key to ClickHouse. ClickHouse can process queries up to 100 times faster than traditional databases and is the perfect solution for Digital advertising, E-commerce, Web and App analytics, Monitoring, Telecommunications analytics. In the rest of this article, we will try to describe in detail the above points with integration between MindsDB, as an Auto-Machine Learning framework and ClickHouse, as an OLAP Database Management System. How to install MindsDB Installing MindsDB is as easy as any other Python package. All you need for installation are a Python version greater than 3.6.x and around 1 GB available disk space. Other than that you just use pip or pip3 to install it as: pip install mindsdb For more detailed installation instructions please check out installation docs. If you got an error or have any questions, please post them to our support forum community.mindsdb.com How to install ClickHouse If you already have ClickHouse installed and your analytics data saved then you\u2019re ready to start playing with MindsDB, so just skip to the Connect MindsDB to ClickHouse section.If not, ClickHouse can run on any Linux or Mac OS X with x86_64 CPU architecture. Depending on your machine check out available installation options. Once the installation is done, you can start the server as a daemon: sudo service clickhouse-server start Starting the server will not display any output, so you can execute: sudo service clickhouse-server status to check that the ClickHouse is running successfully. Next, use clickhouse-client to connect to it:clickhouse-clientIf you get Code: 516. DB::Exception: Received from localhost:9000. DB::Exception: default: Authentication failed: error you will need to provide the default password that you added during the installation process: clickhouse-client --password ****** That\u2019s it. You have successfully connected to your local ClickHouse server. Import dataset to ClickHouse As with any other database management system, ClickHouse also groups tables into databases. To list the available databases you can run a show databases query that will display the default databases: SHOW DATABASES; For storing the data, we will create a new database called data: CREATE DATABASE IF NOT EXISTS data; The dataset that we will use in this tutorial provides time-series air pollution measurement information from data.seoul. Let\u2019s create a table and store the data in ClickHouse. Note that you can follow up to this tutorial with different data, just edit the example queries in line with your data. CREATE TABLE pollution_measurement ( ` Measurement date ` DateTime , ` Station code ` String , Address String , Latitude Float32 , Longitude Float32 , SO2 Decimal32 ( 5 ), NO2 Decimal32 ( 5 ), O3 Decimal32 ( 5 ), CO Decimal32 ( 5 ), PM10 Decimal32 ( 1 ), ` PM2 . 5 ` Decimal32 ( 1 ) ) ENGINE = MergeTree () ORDER BY ( ` Station code ` , ` Measurement date ` ); Note that we need to use backticks to escape the special characters in the column name. The parameters added to the Decimal32(p) are the precision of the decimal digits for e.g Decimal32(5) can contain numbers from -99999.99999 to 99999.99999. The Engine = MergeTree, specify the type of the table in ClickHouse. To learn more about all of the available table engines head over to the table-engines documentation. Lastly, what we need to do is to import the data inside the pollution_measurement table: clickhouse-client --date_time_input_format=best_effort --query=\"INSERT INTO data.pollution_measurement FORMAT CSV\" < Measurement_summary.csv The --date_time_input_format=best_effort enables the datetime parser to parse the basic and all ISO 8601 date and time formats. The pollution_measurement data should be added to the data.pollution_measurement table. To make sure it is successfully added execute SELECT query: SELECT * FROM data.pollution_measurement LIMIT 5;\u200d We are halfway there! We have successfully installed MindsDB and ClickHouse and have the data saved in the database. Now, we will use MindsDB to connect to ClickHouse and train and query Machine Learning models from the air pollution measurement data. If you don\u2019t want to install ClickHouse locally, ClickHouse Docker image is a good solution. Connect MindsDB to ClickHouse Let\u2019s start MindsDB: python3 -m mindsdb --api mysql --config config.json The --api parameter specifies the type of API to use (mysql). The --config specifies the location of the configuration file. The minimum required configuration for connecting to ClickHouse is: { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_clickhouse\" : { \"database_name\" : \"default_clickhouse\" , \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 8123 , \"type\" : \"clickhouse\" , \"user\" : \"default\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } In the default_clickhouse key, include the values for connecting to ClickHouse. In the mindsdb_native storage_dir add a path to the location where MindsDB will save some configuration as (metadata and .pickle files). The Running on http://0.0.0.0:47334/ (Press CTRL+C to quit) message will be displayed if MindsDB is successfully started. That means MindsDB server is running and listening on localhost:47334.First, when MindsDB starts it creates a database and tables inside the ClickHouse. The database created is of ENGINE type MySQL(connection), where 'connection' is established from the parameters provided inside the config.json. USE mindsdb; SHOW TABLES; The default table created inside mindsdb database will be predictors where MindsDB shall keep information about the predictors(ML models), training status, accuracy, target variable and additional training options. DESCRIBE TABLE predictors; When a user creates a new model or makes a query to any table, the query is sent by MySQL text protocol to MindsDB, where it hits the MindsDB\u2019s API\u2019s responsible for training, analyzing, querying the models. Now, we have everything ready to create a model. We are going to use the data inside the pollution_measurement table to predict the Sulfur Dioxide(SO2) in the air. Creating the model is as simple as writing the INSERT query, where we will provide values for the few required attributes. Before creating the predictor make sure mindsdb database is used: use mindsdb; INSERT INTO predictors(name, predict, select_data_query) VALUES ('airq_predictor', 'SO2', 'SELECT * FROM data.pollution_measurement where SO2 > 0 ORDER BY rand() LIMIT 10000'); The Predictor in MindsDB\u2019s words means the Machine Learning model. The columns values for creating the predictor(model) are: name (string) - the name of the predictor.predict (string) - the feature you want to predict, in this example it will be SO2. select_data_query (string) - the SELECT query that will ingest the data to train the model. training_options (dictionary) - optional value that contains additional training parameters. For a full list of the parameters check the mindsdb.docs. In the background, the INSERT to predictors query will call mindsdb-native that will do a black-box analysis and start a process of extracting, analyzing, and transforming the data. It will take some time to train the model depending on the data size, columns, columns type etc, so to keep it faster we are using 5000 random columns by adding ORDERED BY rand() LIMIT 10000 to the SELECT query. You should see a message like: INSERT INTO predictors (name, predict_cols, select_data_query) VALUESOk.1 rows in set. Elapsed: 0.824 sec. To check if the training of the model successfully finished, you can run: SELECT * FROM predictors WHERE name='airq_predictor'\u200d Status complete means that the model training has finished successfully. Now, let\u2019s create predictive analytics from the data by querying the created predictor. The idea was to predict the value of Sulfur Dioxide in the Seoul air station depending on the different measured parameters as NO2, O3, CO, location etc. SELECT SO2 as predicted , SO2_confidence as confidence from airq_predictor WHERE NO2 = 0 . 005 AND CO = 1 . 2 AND PM10 = 5 ; Now you can see that MindsDB predicted that the value of Sulfur Dioxide is 0.00115645 with around 98% confidence.To get additional information about the predicted value and confidence, we should include the explain column. In that case, the MindsDB\u2019s explain functionality apart from confidence can provide additional information such as prediction quality, confidence interval, missing information for improving the prediction etc. We can extend the query and include an additional column for explanation information: SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) Now we get additional information: { \"predicted_value\": 0.001156540079952395, \"confidence\": 0.9869, \"prediction_quality\": \"very confident\", \"confidence_interval\": [0.003184904620383531, 0.013975553923630717], \"important_missing_information\": [\"Station code\", \"Latitude\", \u201cO3\u201d], \"confidence_composition\": { \"CO\": 0.006 }, \"extra_insights\": { \"if_missing\": [{ \"NO2\": 0.007549311956155897 }, { \"CO\": 0.005459383721227349 }, { \"PM10\": 0.003870252306568623 }] } } By looking at the new information we can see that MindsDB is very confident about the quality of this prediction. The range of values where the predicted value lies within is determined inside the confidence interval. Also, the extra insights are providing SO2 value in a case where some of the included features (in WHERE clause) are not provided. MindsDB thinks that Station code and Latitude and O3 are very important features for more precise prediction so those values shall be included in the WHERE clause. Let\u2019s try including Station code and see the new predictions: SELECT SO2 AS predicted , SO2_confidence AS confidence FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) AND ( ` Station code ` = '32' ) AND ` PM2 . 5 `= 50 Now the predicted value has changed by adding the feature that MindsDB thought is quite important for better prediction. Additionally we can try and predict the Sulfur Dioxide in the air in some future date. What we can do is just include the Measurement date value inside WHERE clause for the specific date we want to get prediction e.g : SELECT SO2 AS predicted , SO2_confidence AS confidence FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) AND ( ` Station code ` = '32' ) AND ` PM2 . 5 `= 50 AND ` Measurement date `= \u2019 2020 - 07 - 03 00 : 01 : 00 \u2019 Or few weeks later as: At the end, the whole flow was as simple as seeing MindsDB as a database inside ClickHouse and executing queries for INSERT and SELECT directly from it. If you follow up to this tutorial with your own data, we are happy to hear about how MindsDB has come in useful to you. Everything that we did in this tutorial will be available through the MindsDB\u2019s Graphical User Interface MindsDB Studio in the next release. That means with a few clicks on MindsDB Studio you can successfully train ML models from your ClickHouse database too.","title":"AI Tables in ClickHouse"},{"location":"tutorials/clickhouse/#machine-learning-models-as-tables","text":"We will start this article by raising one of the most asked questions regarding Machine Learning, What is the difference between Machine Learning and Artificial Intelligence? When we think about machine learning we can think about it as a subset of Artificial Intelligence. In simple words, the idea behind machine learning is to enable machines to learn by themselves, by using small to large datasets and finding common patterns inside the data. That said, data is the core of any machine learning algorithm and having access to the data is one of the crucial steps for machine learning success. This will bring us to the second question, Where data lives these days?","title":"Machine Learning Models as Tables"},{"location":"tutorials/clickhouse/#a-great-deal-in-databases","text":"With the increase of data in volume, variety, velocity in today's databases. Is there a better place to bring machine learning to, than being able to do machine learning right in the databases?We believe that database users meet the most important aspect of applied machine learning, which is to understand what predictive questions are important and what data is relevant to answer those questions. Additionally, adding the statistical analysis for creating the most appropriate model to that, will yield the best combination which is auto machine learning straight from the database. Bringing AutoML to those that know data best can significantly augment the capacity to solve important problems. That\u2019s why we decided to build a seamless integration with Clickhouse, in a way such that any ClickHouse user can create, train and test machine learning models with the same knowledge they have of Structured Query Language (SQL).","title":"A great deal in databases"},{"location":"tutorials/clickhouse/#how-can-we-achieve-this","text":"We make use of ClickHouse\u2019s neat capabilities of accessing external tables as if they were internal tables. As such, the integration of these models is painless and transparent allowing us to: * Exposing machine learning models like tables that can be queried. You simply SELECT what you want to predict and you pass in the WHERE statement the conditions for the prediction. * Automatically, build, test and train machine learning models with a simple INSERT statement, where you specify what you want to learn and from what query.","title":"How can we achieve this?"},{"location":"tutorials/clickhouse/#why-mindsdb","text":"MindsDB is a fast-growing Open Source AutoML framework with built-in explainability - a unique visualization capability that helps better understand and trust the accuracy of predictions. With MindsDB, developers can build, train and test Machine Learning models, without needing the help of a Data Scientist/Machine Learning Engineer. It is different from typical AutoML frameworks in that MindsDB has a strong focus on trustworthiness by means of explainability, allowing users to get valuable insights into why and how the model is reaching its predictions.","title":"Why MindsDB?"},{"location":"tutorials/clickhouse/#why-clickhouse","text":"Speed and efficiency are key to ClickHouse. ClickHouse can process queries up to 100 times faster than traditional databases and is the perfect solution for Digital advertising, E-commerce, Web and App analytics, Monitoring, Telecommunications analytics. In the rest of this article, we will try to describe in detail the above points with integration between MindsDB, as an Auto-Machine Learning framework and ClickHouse, as an OLAP Database Management System.","title":"Why ClickHouse?"},{"location":"tutorials/clickhouse/#how-to-install-mindsdb","text":"Installing MindsDB is as easy as any other Python package. All you need for installation are a Python version greater than 3.6.x and around 1 GB available disk space. Other than that you just use pip or pip3 to install it as: pip install mindsdb For more detailed installation instructions please check out installation docs. If you got an error or have any questions, please post them to our support forum community.mindsdb.com","title":"How to install MindsDB"},{"location":"tutorials/clickhouse/#how-to-install-clickhouse","text":"If you already have ClickHouse installed and your analytics data saved then you\u2019re ready to start playing with MindsDB, so just skip to the Connect MindsDB to ClickHouse section.If not, ClickHouse can run on any Linux or Mac OS X with x86_64 CPU architecture. Depending on your machine check out available installation options. Once the installation is done, you can start the server as a daemon: sudo service clickhouse-server start Starting the server will not display any output, so you can execute: sudo service clickhouse-server status to check that the ClickHouse is running successfully. Next, use clickhouse-client to connect to it:clickhouse-clientIf you get Code: 516. DB::Exception: Received from localhost:9000. DB::Exception: default: Authentication failed: error you will need to provide the default password that you added during the installation process: clickhouse-client --password ****** That\u2019s it. You have successfully connected to your local ClickHouse server.","title":"How to install ClickHouse"},{"location":"tutorials/clickhouse/#import-dataset-to-clickhouse","text":"As with any other database management system, ClickHouse also groups tables into databases. To list the available databases you can run a show databases query that will display the default databases: SHOW DATABASES; For storing the data, we will create a new database called data: CREATE DATABASE IF NOT EXISTS data; The dataset that we will use in this tutorial provides time-series air pollution measurement information from data.seoul. Let\u2019s create a table and store the data in ClickHouse. Note that you can follow up to this tutorial with different data, just edit the example queries in line with your data. CREATE TABLE pollution_measurement ( ` Measurement date ` DateTime , ` Station code ` String , Address String , Latitude Float32 , Longitude Float32 , SO2 Decimal32 ( 5 ), NO2 Decimal32 ( 5 ), O3 Decimal32 ( 5 ), CO Decimal32 ( 5 ), PM10 Decimal32 ( 1 ), ` PM2 . 5 ` Decimal32 ( 1 ) ) ENGINE = MergeTree () ORDER BY ( ` Station code ` , ` Measurement date ` ); Note that we need to use backticks to escape the special characters in the column name. The parameters added to the Decimal32(p) are the precision of the decimal digits for e.g Decimal32(5) can contain numbers from -99999.99999 to 99999.99999. The Engine = MergeTree, specify the type of the table in ClickHouse. To learn more about all of the available table engines head over to the table-engines documentation. Lastly, what we need to do is to import the data inside the pollution_measurement table: clickhouse-client --date_time_input_format=best_effort --query=\"INSERT INTO data.pollution_measurement FORMAT CSV\" < Measurement_summary.csv The --date_time_input_format=best_effort enables the datetime parser to parse the basic and all ISO 8601 date and time formats. The pollution_measurement data should be added to the data.pollution_measurement table. To make sure it is successfully added execute SELECT query: SELECT * FROM data.pollution_measurement LIMIT 5;\u200d We are halfway there! We have successfully installed MindsDB and ClickHouse and have the data saved in the database. Now, we will use MindsDB to connect to ClickHouse and train and query Machine Learning models from the air pollution measurement data. If you don\u2019t want to install ClickHouse locally, ClickHouse Docker image is a good solution.","title":"Import dataset to ClickHouse"},{"location":"tutorials/clickhouse/#connect-mindsdb-to-clickhouse","text":"Let\u2019s start MindsDB: python3 -m mindsdb --api mysql --config config.json The --api parameter specifies the type of API to use (mysql). The --config specifies the location of the configuration file. The minimum required configuration for connecting to ClickHouse is: { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_clickhouse\" : { \"database_name\" : \"default_clickhouse\" , \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"pass\" , \"port\" : 8123 , \"type\" : \"clickhouse\" , \"user\" : \"default\" } }, \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } }, \"storage_dir\" : \"/storage\" } In the default_clickhouse key, include the values for connecting to ClickHouse. In the mindsdb_native storage_dir add a path to the location where MindsDB will save some configuration as (metadata and .pickle files). The Running on http://0.0.0.0:47334/ (Press CTRL+C to quit) message will be displayed if MindsDB is successfully started. That means MindsDB server is running and listening on localhost:47334.First, when MindsDB starts it creates a database and tables inside the ClickHouse. The database created is of ENGINE type MySQL(connection), where 'connection' is established from the parameters provided inside the config.json. USE mindsdb; SHOW TABLES; The default table created inside mindsdb database will be predictors where MindsDB shall keep information about the predictors(ML models), training status, accuracy, target variable and additional training options. DESCRIBE TABLE predictors; When a user creates a new model or makes a query to any table, the query is sent by MySQL text protocol to MindsDB, where it hits the MindsDB\u2019s API\u2019s responsible for training, analyzing, querying the models. Now, we have everything ready to create a model. We are going to use the data inside the pollution_measurement table to predict the Sulfur Dioxide(SO2) in the air. Creating the model is as simple as writing the INSERT query, where we will provide values for the few required attributes. Before creating the predictor make sure mindsdb database is used: use mindsdb; INSERT INTO predictors(name, predict, select_data_query) VALUES ('airq_predictor', 'SO2', 'SELECT * FROM data.pollution_measurement where SO2 > 0 ORDER BY rand() LIMIT 10000'); The Predictor in MindsDB\u2019s words means the Machine Learning model. The columns values for creating the predictor(model) are: name (string) - the name of the predictor.predict (string) - the feature you want to predict, in this example it will be SO2. select_data_query (string) - the SELECT query that will ingest the data to train the model. training_options (dictionary) - optional value that contains additional training parameters. For a full list of the parameters check the mindsdb.docs. In the background, the INSERT to predictors query will call mindsdb-native that will do a black-box analysis and start a process of extracting, analyzing, and transforming the data. It will take some time to train the model depending on the data size, columns, columns type etc, so to keep it faster we are using 5000 random columns by adding ORDERED BY rand() LIMIT 10000 to the SELECT query. You should see a message like: INSERT INTO predictors (name, predict_cols, select_data_query) VALUESOk.1 rows in set. Elapsed: 0.824 sec. To check if the training of the model successfully finished, you can run: SELECT * FROM predictors WHERE name='airq_predictor'\u200d Status complete means that the model training has finished successfully. Now, let\u2019s create predictive analytics from the data by querying the created predictor. The idea was to predict the value of Sulfur Dioxide in the Seoul air station depending on the different measured parameters as NO2, O3, CO, location etc. SELECT SO2 as predicted , SO2_confidence as confidence from airq_predictor WHERE NO2 = 0 . 005 AND CO = 1 . 2 AND PM10 = 5 ; Now you can see that MindsDB predicted that the value of Sulfur Dioxide is 0.00115645 with around 98% confidence.To get additional information about the predicted value and confidence, we should include the explain column. In that case, the MindsDB\u2019s explain functionality apart from confidence can provide additional information such as prediction quality, confidence interval, missing information for improving the prediction etc. We can extend the query and include an additional column for explanation information: SELECT SO2 AS predicted , SO2_confidence AS confidence , SO2_explain AS info FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) Now we get additional information: { \"predicted_value\": 0.001156540079952395, \"confidence\": 0.9869, \"prediction_quality\": \"very confident\", \"confidence_interval\": [0.003184904620383531, 0.013975553923630717], \"important_missing_information\": [\"Station code\", \"Latitude\", \u201cO3\u201d], \"confidence_composition\": { \"CO\": 0.006 }, \"extra_insights\": { \"if_missing\": [{ \"NO2\": 0.007549311956155897 }, { \"CO\": 0.005459383721227349 }, { \"PM10\": 0.003870252306568623 }] } } By looking at the new information we can see that MindsDB is very confident about the quality of this prediction. The range of values where the predicted value lies within is determined inside the confidence interval. Also, the extra insights are providing SO2 value in a case where some of the included features (in WHERE clause) are not provided. MindsDB thinks that Station code and Latitude and O3 are very important features for more precise prediction so those values shall be included in the WHERE clause. Let\u2019s try including Station code and see the new predictions: SELECT SO2 AS predicted , SO2_confidence AS confidence FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) AND ( ` Station code ` = '32' ) AND ` PM2 . 5 `= 50 Now the predicted value has changed by adding the feature that MindsDB thought is quite important for better prediction. Additionally we can try and predict the Sulfur Dioxide in the air in some future date. What we can do is just include the Measurement date value inside WHERE clause for the specific date we want to get prediction e.g : SELECT SO2 AS predicted , SO2_confidence AS confidence FROM airq_predictor WHERE ( NO2 = 0 . 005 ) AND ( CO = 1 . 2 ) AND ( PM10 = 5 ) AND ( ` Station code ` = '32' ) AND ` PM2 . 5 `= 50 AND ` Measurement date `= \u2019 2020 - 07 - 03 00 : 01 : 00 \u2019 Or few weeks later as: At the end, the whole flow was as simple as seeing MindsDB as a database inside ClickHouse and executing queries for INSERT and SELECT directly from it. If you follow up to this tutorial with your own data, we are happy to hear about how MindsDB has come in useful to you. Everything that we did in this tutorial will be available through the MindsDB\u2019s Graphical User Interface MindsDB Studio in the next release. That means with a few clicks on MindsDB Studio you can successfully train ML models from your ClickHouse database too.","title":"Connect MindsDB to ClickHouse"},{"location":"tutorials/mariadb/","text":"AI-Tables in MariaDB Database users are the best to know what data is relevant for ML models. Virtual AI tables in MariaDB allows users to run Automated Machine Learning models directly from inside the database. This tutorial is an overview of this integration capability. Anyone that has dealt with Machine Learning (ML) understands that data is a fundamental ingredient to it. Given that a great deal of the world\u2019s organized data already exists inside databases, doesn\u2019t it make sense to bring machine learning capabilities straight to the database itself? To do so, we have developed a concept called AITables. In this article we want to present to you what AITables are, how you can use them in MariaDB. Invite you to try it out, get involved and to join us in the journey of ML meets MariaDB. AiTables AITables differ from normal tables in that they can generate predictions upon being queried and returning such predictions like if it was data that existed on the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this; SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > To really sink in this idea, let us expand the concept through an example. The used car price example Imagine that you want to solve the problem of estimating the right price for a car on your website that has been selling used cars over the past 2 years. Also, you use MariaDB and in your database there is a table called used_cars_data where you keep records of every car you have sold so far, storing information such as: price, transmission, mileage, fuel_type, road_tax, mpg (Miles Per Gallon) and engine_size. Since you have historical data, you know that you could use Machine Learning to solve this problem. Wouldn\u2019t it be nice if you could simply tell your MariaDB server to do and manage the Machine Learning parts for you? At MindsDB we think so too! And AI-Tables baked into MariaDB are here to do exactly that. Although further down in this article we will guide you step by step on how to run this example yourself, let us introduce you to what you can do and how it looks in standard SQL. You can for instance with a single INSERT statement, create a machine learning model/predictor trained to predict \u2018price\u2019 using the data that lives in the table sold_cars and publish it as an AI-Table called \u2018used_cars_model\u2019. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); After that you can get price predictions by querying the generated \u2018used_cars_model\u2019 AI-Table, as follows: SELECT price , confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; As you can see with AI-Tables, we are aiming to simplify Machine Learning mechanics to simple SQL queries, so that you can focus on the important part; which is to think about what predictions are key for your business and what data you want your ML to learn from to make such predictions. By now, you might be thinking, how do I get started?, what do I need? How can I run this example in my environment? As promised, in the rest of this article we will show you step-by-step instructions on how to integrate MindsDB into your MariaDB server, how to build, test and use Machine Learning Models as AI-Tables all without the need for specific machine learning skills and how to evaluate prediction results in an \u201cExplainable AI\u201d way. If you want to preview this tutorial visually go ahead to our youtube channel and follow up the Machine Learning in MariaDB with AI Tables video. MindsDB AITables in MariaDB With MindsDB any MariaDB user can train and test neural-networks based Machine Learning models with the same knowledge they have of SQL. MindsDB is an open-source ML framework that enables running Machine Learning Models as AI-Tables. On top of that it has an exciting \u201cExplainable AI\u201d feature that allows users to get insights into their Machine Prediction accuracy score and evaluate its dependencies. For example, users can estimate how adding or removing certain data would impact on the effectiveness of the prediction. The whole integration consists of two important parts: * The Machine Learning models are exposed as database tables (AI-Tables) that can be queried with the SELECT statements. * The ML model generation and training is done through a simple INSERT statement. This is possible thanks to MariaDBs CONNECT engine, which enables us to publish tables that live outside MariaDB. Since MindsDB supports the MySQL tcp-ip protocol, AI-Tables can be plugged as if they are external MariaDB tables. The following diagram illustrates this process. The resource intensive Machine Learning tasks like model training happen on a separate MindsDB server instance or in the cloud, so that the Database performance is not affected. In the following step-by-step tutorial you will learn how to install a MindsDB server with MariaDB and connect to the data, how to train the model and get predictions using an example dataset. So let\u2019s get started. How to install MariaDB? If you already have MariaDB installed you can skip this section. MariaDB is one of the most popular database servers in the world and works on the most widely used operating systems. You can find the installation binaries and packages on the mariadb download site. To check the full list of distributions which include MariaDB head over to list of distributions . How to install MindsDB? Before you install MindsDB you need a Python version greater than 3.6 and pip >=19.3 which comes pre-installed with newer Python versions. Also, you will need to have around 1GB free space on your machine for installing the MindsDB\u2019s dependencies. Other than that the installation is quite simple. Inside your virtual environment just run: pip install mindsdb To check if the installation was successful run: pip show mindsdb And you should be able to see the MindsDB information as name, version, summary, license etc: That\u2019s all. Let\u2019s set up the required configuration and start MindsDB. Example Dataset If you are following this tutorial with your own data, you can skip to the next section. For this example we will use the Used Car Price dataset from the 100k used cars scraped data. The dataset contains information on price, transmission, mileage, fuel type, road tax, miles per gallon (mpg), and engine size of the used cars in the UK. The idea is to predict the price depending on the above features. Add data to MariaDB The first thing we need to do is to create the table. Execute the below query: CREATE TABLE ` used_cars_data ` ( ` model ` VARCHAR ( 100 ) DEFAULT NULL , ` year ` INT ( 11 ) DEFAULT NULL , ` price ` INT ( 11 ) DEFAULT NULL , ` transmission ` VARCHAR ( 100 ) DEFAULT NULL , ` mileage ` INT ( 11 ) DEFAULT NULL , ` fueltype ` VARCHAR ( 100 ) DEFAULT NULL , ` tax ` INT ( 11 ) DEFAULT NULL , ` mpg ` FLOAT DEFAULT NULL , ` enginesize ` FLOAT DEFAULT NULL ) engine = innodb DEFAULT charset = latin1 The InnoDB is a general storage engine and the one offered as the best choice in most cases from the MariaDB team.To see the list with all available ENGINEs and for advice on which one to choose check out the MariaDB engine docs . After creating the table there are a few options that you can do to add the data inside MariaDB. If you are using graphical clients such as dbForge, DBeaver or another SQL client use the import option from the menu. Use the LOAD DATA statement that reads the local file from the location provided and sends the content to the MariaDB Server: LOAD DATA LOCAL INFILE 'data.csv' INTO used_cars_data FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\r\\n'; The TERMINATED BY specifies the separator in the data. The escape characters and new lines are manageable with the ENCLOSED BY and LINES TERMINATED BY clause. Using mysqlimport . This is the cli for the above, LOAD DATA statement. The arguments sent here correspond to the clauses of the LOAD DATA example: mysqlimport --local --fields-terminated-by=\",\" used_cars_data data.csv Let\u2019s select the data from used_cars_data table to make sure it is successfully imported: SELECT * FROM test . used_cars_data LIMIT 5 ; The data is inside MariaDB so the next step is to add the required configuration. Required Configuration The first thing we need to set up is the required configuration for MindsDB Server. Let\u2019s create a new file called config.json and add the following example: { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" } , \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } } , \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mariadb\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"password\" , \"port\" : 3306 , \"type\" : \"mariadb\" , \"user\" : \"root\" } } , \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } } , \"storage_dir\" : \"/storage\" } This looks like a big configuration file but it is quite simple. What we have included are: api \u2013 The keys here are specifying the host and port for MindsDB REST Apis. mysql \u2013 All of the information for connecting to mysql and using the mysql protocol as host, user, password and port. Also the log level information and the path to your local SSL certificate. If certificate_path is left empty, MindsDB will automatically create one. integrations \u2013 Here, we are specifying the type of integration that we will use, default_mariadb. Also the required parameters for connecting to it as host, port, user, password. Other supported databases for integrations are ClickHouse, PostgreSQL, MySQL and Microsoft SQL Server. interface \u2013 The required keys added here are datastore and mindsdb_native, that contains the path to the storage location, which will be used by MindsDB to save some of the configuration files. And the last thing we need to do is to install the CONNECT storage engine plugin that we have added in the plugin-load-add variable. To check how to download it for your OS check installing docs. It should be quite simple as using the package manager, e.g: sudo apt - get install mariadb - plugin - connect That\u2019s pretty much everything related to the configuration required for successful integration with MariaDB. Let\u2019s jump to the interesting part where we will train the machine learning model and query it. AutoML inside MariaDB First, we need to start MindsDB: python3 -m mindsdb \u2013api=mysql \u2013config=config.json The flags added here are: \u2013api \u2013 This specifies the type of API we will use with MindsDB, in this case mysql. \u2013config \u2013 The path to the config.json file we have created before. If MindsDB was successfully started there should be a new database automatically created in MariaDB called mindsdb with two tables (commands and predictors). In these tables, MindsDB will keep information about the models, model accuracy, training status, target variable that we will predict and additional options used for model training. Create new predictor The main motto when we first started MindsDB was with one line of code, so now we will try to stick to it and present that in the databases with just one query. The Predictor in MindsDB\u2019s words means Machine Learning model, so creating one could be done with an INSERT statement inside mindsdb.predictors table. Execute the following query: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); What this query does is it creates a new model called \u2018used_cars_model\u2019 , specifies the column that we will try to predict as \u2018price\u2019 from the used_cars_data table. The required columns(parameters) for training the predictor are: name (string) \u2013 the name of the predictor. predict (string) \u2013 the feature you want to predict, in this example it will be price. select_data_query (string) \u2013 the SELECT query that will get the data from MariaDB. training_options (dictionary) \u2013 optional value that contains additional training parameters. For a full list of the parameters check the mindsdb.docs. You should see the message about the successful execution of the query and if you open up the console, the MindsDB logger shall display messages while training the model. Training could take some time depending on the data used, columns types, size etc. In our example not more than 2-3 min. To check that model was successfully trained run: SELECT * FROM mindsdb . predictors WHERE name = 'used_cars_model' The column status shall be complete and the model accuracy will be saved when the training finishes. The model has been trained successfully. It was quite simple because we didn\u2019t do any hyperparameters tuning or features engineering and leave that out to MindsDB as an AutoML framework to try and fit the best model. With the INSERT query, we just provided labeled data as an input. The next step is to query the trained model with SELECT and get the output from it (predict the price of the car). Query the predictor To get the prediction from the model is as easy as executing the SELECT statement where we will select the price and price confidence. The main idea is to predict the used car\u2019s price depending on the different features. The first thing that comes to mind when looking for the used car is the car model, fuel type, mileage, the year when the car was produced etc. We should include in the WHERE clause all of this informations and leave it to MindsDB to make the predictions for them e.g: SELECT price AS predicted , price_confidence AS confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; You should see that MindsDB is quite confident that the car with all of the above characteristics as included in the WHERE clause shall cost around 13,111. To get additional information about the prediction include the explain column in the SELECT e.g: SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; Note that to beautify the resultFormat you can add command line option format for particular session. Now MindsDB will display additional information in the info column as prediction quality, confidence interval, missing information for improving the prediction etc. { \"predicted_value\" : 13772 , \"confidence\" : 0.9922 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10795 , 31666 ], \"important_missing_information\" : [], \"confidence_composition\" : { \"Model\" : 0.008 , \"year\" : 0.018 , \"transmission\" : 0.001 , \"mpg\" : 0.001 }, \"extra_insights\" : { \"if_missing\" : [{ \"Model\" : 13661 }, { \"year\" : 17136 }, { \"transmission\" : 3405 }, { \"mileage\" : 15281 }, { \"fuelType\" : 7877 }, { \"tax\" : 13908 }, { \"mpg\" : 38858 }, { \"engineSize\" : 13772 }] } } The confidence_interval specifies the probability that the value of a price lies within the range of 10k to 30k. The important_missing_information in this case is empty, but if we omit some of the important values in the WHERE clause e.g price, year or mpg, MindsDB shall warn us that that column is important for the model. The if_missing in the extra_insights shows the price value if some of the mentioned columns are missing. Now, let\u2019s try and get the price prediction for different car models with different fuel type, mileage, engine size, transmission: SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"a1\" AND mileage = 122946 AND transmission = \"manual\" AND fueltype = \"petrol\" AND mpg = \"35.4\" AND enginesize = 1 . 4 AND year = 2014 AND tax = 30 ; Now, MindsDB thinks that this type of car would cost around 12k and price ranges to 23k. Let\u2019s sum up what we did and the simple steps we take to get the predictions: Install MariaDB and MindsDB. Setup the configuration. Train the model with an INSERT query. Get predictions from the model with a SELECT query. Quite simple right? This is a brand new feature that we have developed so we are happy to hear your opinions on it. You can play around and query the model with different values or train and query the model with different datasets. If you have some interesting results or you found some issues we are happy to help and talk with you. Join our community forum or reach out to us on GitHub .","title":"AI Tables in MariaDB"},{"location":"tutorials/mariadb/#ai-tables-in-mariadb","text":"Database users are the best to know what data is relevant for ML models. Virtual AI tables in MariaDB allows users to run Automated Machine Learning models directly from inside the database. This tutorial is an overview of this integration capability. Anyone that has dealt with Machine Learning (ML) understands that data is a fundamental ingredient to it. Given that a great deal of the world\u2019s organized data already exists inside databases, doesn\u2019t it make sense to bring machine learning capabilities straight to the database itself? To do so, we have developed a concept called AITables. In this article we want to present to you what AITables are, how you can use them in MariaDB. Invite you to try it out, get involved and to join us in the journey of ML meets MariaDB.","title":"AI-Tables in MariaDB"},{"location":"tutorials/mariadb/#aitables","text":"AITables differ from normal tables in that they can generate predictions upon being queried and returning such predictions like if it was data that existed on the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this; SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > To really sink in this idea, let us expand the concept through an example.","title":"AiTables"},{"location":"tutorials/mariadb/#the-used-car-price-example","text":"Imagine that you want to solve the problem of estimating the right price for a car on your website that has been selling used cars over the past 2 years. Also, you use MariaDB and in your database there is a table called used_cars_data where you keep records of every car you have sold so far, storing information such as: price, transmission, mileage, fuel_type, road_tax, mpg (Miles Per Gallon) and engine_size. Since you have historical data, you know that you could use Machine Learning to solve this problem. Wouldn\u2019t it be nice if you could simply tell your MariaDB server to do and manage the Machine Learning parts for you? At MindsDB we think so too! And AI-Tables baked into MariaDB are here to do exactly that. Although further down in this article we will guide you step by step on how to run this example yourself, let us introduce you to what you can do and how it looks in standard SQL. You can for instance with a single INSERT statement, create a machine learning model/predictor trained to predict \u2018price\u2019 using the data that lives in the table sold_cars and publish it as an AI-Table called \u2018used_cars_model\u2019. INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); After that you can get price predictions by querying the generated \u2018used_cars_model\u2019 AI-Table, as follows: SELECT price , confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; As you can see with AI-Tables, we are aiming to simplify Machine Learning mechanics to simple SQL queries, so that you can focus on the important part; which is to think about what predictions are key for your business and what data you want your ML to learn from to make such predictions. By now, you might be thinking, how do I get started?, what do I need? How can I run this example in my environment? As promised, in the rest of this article we will show you step-by-step instructions on how to integrate MindsDB into your MariaDB server, how to build, test and use Machine Learning Models as AI-Tables all without the need for specific machine learning skills and how to evaluate prediction results in an \u201cExplainable AI\u201d way. If you want to preview this tutorial visually go ahead to our youtube channel and follow up the Machine Learning in MariaDB with AI Tables video.","title":"The used car price example"},{"location":"tutorials/mariadb/#mindsdb-aitables-in-mariadb","text":"With MindsDB any MariaDB user can train and test neural-networks based Machine Learning models with the same knowledge they have of SQL. MindsDB is an open-source ML framework that enables running Machine Learning Models as AI-Tables. On top of that it has an exciting \u201cExplainable AI\u201d feature that allows users to get insights into their Machine Prediction accuracy score and evaluate its dependencies. For example, users can estimate how adding or removing certain data would impact on the effectiveness of the prediction. The whole integration consists of two important parts: * The Machine Learning models are exposed as database tables (AI-Tables) that can be queried with the SELECT statements. * The ML model generation and training is done through a simple INSERT statement. This is possible thanks to MariaDBs CONNECT engine, which enables us to publish tables that live outside MariaDB. Since MindsDB supports the MySQL tcp-ip protocol, AI-Tables can be plugged as if they are external MariaDB tables. The following diagram illustrates this process. The resource intensive Machine Learning tasks like model training happen on a separate MindsDB server instance or in the cloud, so that the Database performance is not affected. In the following step-by-step tutorial you will learn how to install a MindsDB server with MariaDB and connect to the data, how to train the model and get predictions using an example dataset. So let\u2019s get started.","title":"MindsDB AITables in MariaDB"},{"location":"tutorials/mariadb/#how-to-install-mariadb","text":"If you already have MariaDB installed you can skip this section. MariaDB is one of the most popular database servers in the world and works on the most widely used operating systems. You can find the installation binaries and packages on the mariadb download site. To check the full list of distributions which include MariaDB head over to list of distributions .","title":"How to install MariaDB?"},{"location":"tutorials/mariadb/#how-to-install-mindsdb","text":"Before you install MindsDB you need a Python version greater than 3.6 and pip >=19.3 which comes pre-installed with newer Python versions. Also, you will need to have around 1GB free space on your machine for installing the MindsDB\u2019s dependencies. Other than that the installation is quite simple. Inside your virtual environment just run: pip install mindsdb To check if the installation was successful run: pip show mindsdb And you should be able to see the MindsDB information as name, version, summary, license etc: That\u2019s all. Let\u2019s set up the required configuration and start MindsDB.","title":"How to install MindsDB?"},{"location":"tutorials/mariadb/#example-dataset","text":"If you are following this tutorial with your own data, you can skip to the next section. For this example we will use the Used Car Price dataset from the 100k used cars scraped data. The dataset contains information on price, transmission, mileage, fuel type, road tax, miles per gallon (mpg), and engine size of the used cars in the UK. The idea is to predict the price depending on the above features.","title":"Example Dataset"},{"location":"tutorials/mariadb/#add-data-to-mariadb","text":"The first thing we need to do is to create the table. Execute the below query: CREATE TABLE ` used_cars_data ` ( ` model ` VARCHAR ( 100 ) DEFAULT NULL , ` year ` INT ( 11 ) DEFAULT NULL , ` price ` INT ( 11 ) DEFAULT NULL , ` transmission ` VARCHAR ( 100 ) DEFAULT NULL , ` mileage ` INT ( 11 ) DEFAULT NULL , ` fueltype ` VARCHAR ( 100 ) DEFAULT NULL , ` tax ` INT ( 11 ) DEFAULT NULL , ` mpg ` FLOAT DEFAULT NULL , ` enginesize ` FLOAT DEFAULT NULL ) engine = innodb DEFAULT charset = latin1 The InnoDB is a general storage engine and the one offered as the best choice in most cases from the MariaDB team.To see the list with all available ENGINEs and for advice on which one to choose check out the MariaDB engine docs . After creating the table there are a few options that you can do to add the data inside MariaDB. If you are using graphical clients such as dbForge, DBeaver or another SQL client use the import option from the menu. Use the LOAD DATA statement that reads the local file from the location provided and sends the content to the MariaDB Server: LOAD DATA LOCAL INFILE 'data.csv' INTO used_cars_data FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\r\\n'; The TERMINATED BY specifies the separator in the data. The escape characters and new lines are manageable with the ENCLOSED BY and LINES TERMINATED BY clause. Using mysqlimport . This is the cli for the above, LOAD DATA statement. The arguments sent here correspond to the clauses of the LOAD DATA example: mysqlimport --local --fields-terminated-by=\",\" used_cars_data data.csv Let\u2019s select the data from used_cars_data table to make sure it is successfully imported: SELECT * FROM test . used_cars_data LIMIT 5 ; The data is inside MariaDB so the next step is to add the required configuration.","title":"Add data to MariaDB"},{"location":"tutorials/mariadb/#required-configuration","text":"The first thing we need to set up is the required configuration for MindsDB Server. Let\u2019s create a new file called config.json and add the following example: { \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" } , \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } } , \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mariadb\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"password\" , \"port\" : 3306 , \"type\" : \"mariadb\" , \"user\" : \"root\" } } , \"log\" : { \"level\" : { \"console\" : \"DEBUG\" , \"file\" : \"INFO\" } } , \"storage_dir\" : \"/storage\" } This looks like a big configuration file but it is quite simple. What we have included are: api \u2013 The keys here are specifying the host and port for MindsDB REST Apis. mysql \u2013 All of the information for connecting to mysql and using the mysql protocol as host, user, password and port. Also the log level information and the path to your local SSL certificate. If certificate_path is left empty, MindsDB will automatically create one. integrations \u2013 Here, we are specifying the type of integration that we will use, default_mariadb. Also the required parameters for connecting to it as host, port, user, password. Other supported databases for integrations are ClickHouse, PostgreSQL, MySQL and Microsoft SQL Server. interface \u2013 The required keys added here are datastore and mindsdb_native, that contains the path to the storage location, which will be used by MindsDB to save some of the configuration files. And the last thing we need to do is to install the CONNECT storage engine plugin that we have added in the plugin-load-add variable. To check how to download it for your OS check installing docs. It should be quite simple as using the package manager, e.g: sudo apt - get install mariadb - plugin - connect That\u2019s pretty much everything related to the configuration required for successful integration with MariaDB. Let\u2019s jump to the interesting part where we will train the machine learning model and query it.","title":"Required Configuration"},{"location":"tutorials/mariadb/#automl-inside-mariadb","text":"First, we need to start MindsDB: python3 -m mindsdb \u2013api=mysql \u2013config=config.json The flags added here are: \u2013api \u2013 This specifies the type of API we will use with MindsDB, in this case mysql. \u2013config \u2013 The path to the config.json file we have created before. If MindsDB was successfully started there should be a new database automatically created in MariaDB called mindsdb with two tables (commands and predictors). In these tables, MindsDB will keep information about the models, model accuracy, training status, target variable that we will predict and additional options used for model training.","title":"AutoML inside MariaDB"},{"location":"tutorials/mariadb/#create-new-predictor","text":"The main motto when we first started MindsDB was with one line of code, so now we will try to stick to it and present that in the databases with just one query. The Predictor in MindsDB\u2019s words means Machine Learning model, so creating one could be done with an INSERT statement inside mindsdb.predictors table. Execute the following query: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'used_cars_model' , 'price' , ' SELECT * FROM used_cars_data ); What this query does is it creates a new model called \u2018used_cars_model\u2019 , specifies the column that we will try to predict as \u2018price\u2019 from the used_cars_data table. The required columns(parameters) for training the predictor are: name (string) \u2013 the name of the predictor. predict (string) \u2013 the feature you want to predict, in this example it will be price. select_data_query (string) \u2013 the SELECT query that will get the data from MariaDB. training_options (dictionary) \u2013 optional value that contains additional training parameters. For a full list of the parameters check the mindsdb.docs. You should see the message about the successful execution of the query and if you open up the console, the MindsDB logger shall display messages while training the model. Training could take some time depending on the data used, columns types, size etc. In our example not more than 2-3 min. To check that model was successfully trained run: SELECT * FROM mindsdb . predictors WHERE name = 'used_cars_model' The column status shall be complete and the model accuracy will be saved when the training finishes. The model has been trained successfully. It was quite simple because we didn\u2019t do any hyperparameters tuning or features engineering and leave that out to MindsDB as an AutoML framework to try and fit the best model. With the INSERT query, we just provided labeled data as an input. The next step is to query the trained model with SELECT and get the output from it (predict the price of the car).","title":"Create new predictor"},{"location":"tutorials/mariadb/#query-the-predictor","text":"To get the prediction from the model is as easy as executing the SELECT statement where we will select the price and price confidence. The main idea is to predict the used car\u2019s price depending on the different features. The first thing that comes to mind when looking for the used car is the car model, fuel type, mileage, the year when the car was produced etc. We should include in the WHERE clause all of this informations and leave it to MindsDB to make the predictions for them e.g: SELECT price AS predicted , price_confidence AS confidence FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; You should see that MindsDB is quite confident that the car with all of the above characteristics as included in the WHERE clause shall cost around 13,111. To get additional information about the prediction include the explain column in the SELECT e.g: SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"a6\" AND mileage = 36203 AND transmission = \"automatic\" AND fueltype = \"diesel\" AND mpg = \"64.2\" AND enginesize = 2 AND year = 2016 AND tax = 20 ; Note that to beautify the resultFormat you can add command line option format for particular session. Now MindsDB will display additional information in the info column as prediction quality, confidence interval, missing information for improving the prediction etc. { \"predicted_value\" : 13772 , \"confidence\" : 0.9922 , \"prediction_quality\" : \"very confident\" , \"confidence_interval\" : [ 10795 , 31666 ], \"important_missing_information\" : [], \"confidence_composition\" : { \"Model\" : 0.008 , \"year\" : 0.018 , \"transmission\" : 0.001 , \"mpg\" : 0.001 }, \"extra_insights\" : { \"if_missing\" : [{ \"Model\" : 13661 }, { \"year\" : 17136 }, { \"transmission\" : 3405 }, { \"mileage\" : 15281 }, { \"fuelType\" : 7877 }, { \"tax\" : 13908 }, { \"mpg\" : 38858 }, { \"engineSize\" : 13772 }] } } The confidence_interval specifies the probability that the value of a price lies within the range of 10k to 30k. The important_missing_information in this case is empty, but if we omit some of the important values in the WHERE clause e.g price, year or mpg, MindsDB shall warn us that that column is important for the model. The if_missing in the extra_insights shows the price value if some of the mentioned columns are missing. Now, let\u2019s try and get the price prediction for different car models with different fuel type, mileage, engine size, transmission: SELECT price AS predicted , price_confidence AS confidence , price_explain AS info FROM mindsdb . used_cars_model WHERE model = \"a1\" AND mileage = 122946 AND transmission = \"manual\" AND fueltype = \"petrol\" AND mpg = \"35.4\" AND enginesize = 1 . 4 AND year = 2014 AND tax = 30 ; Now, MindsDB thinks that this type of car would cost around 12k and price ranges to 23k. Let\u2019s sum up what we did and the simple steps we take to get the predictions: Install MariaDB and MindsDB. Setup the configuration. Train the model with an INSERT query. Get predictions from the model with a SELECT query. Quite simple right? This is a brand new feature that we have developed so we are happy to hear your opinions on it. You can play around and query the model with different values or train and query the model with different datasets. If you have some interesting results or you found some issues we are happy to help and talk with you. Join our community forum or reach out to us on GitHub .","title":"Query the predictor"},{"location":"tutorials/mysql/","text":"How to enable Automated Machine Learning in MySQL Database is surely the best place for Machine Learning - because data is the main ingredient of it. And now you can build, train, test & query Machine Learning models using standard SQL queries within MySQL database! This doesn't require hardcore data science knowledge - the whole Machine Learning workflow is automated. This solution is called AI-Tables and is available in MySQL thanks to integration with an open-source predictive engine from MindsDB. AI-Tables look like normal database tables and return predictions upon being queried as if it is data that exists in the table. In plain SQL it looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > This video explains how it works: In this tutorial below, you will get step-by-step instructions on how to enable AI-Tables in MySQL database. Based on the churn prediction example, you see how to build, train and query Machine Learning models only by using SQL statements with MindsDB! How to install MySQL? If you don\u2019t have MySQL installed you can download the installers for various platforms from the official documentation . Example dataset In this tutorial, we will use the Churn Modelling Data Set . If you have other datasets in your MySQL database please skip this section. This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer. Import dataset to MySQL The first thing we need to do is to import the dataset in MySQL. Create a new table called bank_churn: -- test.bank_churn definition CREATE TABLE test . bank_churn ( CreditScore NUMERIC NULL , Geography varchar ( 100 ) NULL , Gender varchar ( 100 ) NULL , Age NUMERIC NULL , Tenure NUMERIC NULL , Balance NUMERIC NULL , NumOfProducts NUMERIC NULL , HasCrCard NUMERIC NULL , IsActiveMember NUMERIC NULL , EstimatedSalary NUMERIC NULL , Exited NUMERIC NULL ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ; Next, we need to import the data inside the table. There are a few options to do that: Using the Load Data statement: LOAD DATA LOCAL INFILE 'data.csv' INTO bank_churn FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\r\\n'; Using MySQL import: mysqlimport --local --fields-terminated-by=\",\" bank_churn data.csv Using pgAdmin, DBeaver or another SQL client just use the import from CSV file option from the navigation menu. Let\u2019s Select some data from bank_churn table to check that the data was successfully imported to MySQL: SELECT * FROM bank_churn LIMIT 1 ; Add Configuration As a prerequisite for using MySQL we need to enable the Federated Storage engine. Check out the official MySQL documentation to see how you can do that. The last step is to create the MindsDB\u2019s configuration file. MindsDB will try to use the default configuration options like host, port or username for MySQL. In case you want to extend them or change the default values you need to add a config.json file. Create a new file config.json and include the following information: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"storage_dir\" : \"storage/\" } The values provided in the configuration file are: api['http\u2019] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations[default_mysql] -- This key specifies the integration type in this case default_mysql. The required keys are: user(default root) - The MySQL user name. host(default localhost) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. storage_dir -- The directory where mindsdb will store models and configuration files. Now, we have successfully set up all of the requirements for AI Tables in MySQL. AutoML with AI Tables in MySQL If you don't have MindsDB installed, check out our Installation guide and find an option that works for you. After that start the MindsDB server: python3 -m mindsdb --api=mysql --config=config.json The arguments sent to MindsDB are: --api - That tells MindsDB which API should be started (HTTP or MySQL). --config - The path to the configuration file that we have created. If everything works as expected you should see the following message: Upon successful setup, MindsDB should create a new database called mindsdb. In the mindsdb database, two new tables should be created called commands and predictors. The mindsdb.predictors table is the table where MindsDB will keep information about trained models. Train new Machine Learning Model Training the machine learning model using MindsDB is quite simple. It can be done by executing the INSERT query inside the mindsdb.predictors table. In our example, we want to predict if the bank's customer left the bank from the bank_churn table, so let\u2019s run the INSERT query as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'churn_model' , 'Exited' , 'SELECT * FROM test.bank_churn' ); This query will create a new model called 'churn_model', and a new table 'churn_model' inside mindsdb database. The required columns(parameters) added in the INSERT for training the predictor are: * name (string) - the name of the predictor. * predict (string) - the feature you want to predict, in this example it will be Exited. * select_data_query (string) - the SELECT query that will get the data from MySQL. To check that the training successfully finished we can SELECT from mindsdb.predictors table and get the status: SELECT * FROM mindsdb . predictors WHERE name = 'churn_model' ; The status complete means that training successfully finished. Now, let\u2019s query the model. The trained model behaves like an AI Table and can be queried as it is a standard database table. To get the prediction we need to execute a SELECT query and in the WHERE clause include the when_data as a JSON string that includes features values such as CreditScore, EstimatedSalary, Gender, Balance etc. SELECT * FROM mindsdb . churn_model WHERE when_data = '{\"CreditScore\": \"619\",\"Geography\": \"France\",\"Gender\": \"Female\", \"EstimatedSalary\": 100000, \"Balance\": 0.0, \"Age\":42, \"Tenure\": 2}' ; In a second we should get the prediction back from MindsDB. So, MindsDB thinks that the above customer closed the account in the bank (predicted_value 1) with around 98% confidence. Information in JSON format in the explain column: { \"predicted_value\" : \"1.0\" , \"confidence\" : 0.98 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"NumOfProducts\" ] } The important_missing_information shows the list of features that MindsDB things are quite important for better prediction, in this case, the \"NumOfProducts\". Congratulations, you have successfully trained and queried the Machine Learning Model only by using SQL Statements. Note that even if we used MySQL, you can still query the same model from the other databases too.","title":"AI Tables in MySQL"},{"location":"tutorials/mysql/#how-to-enable-automated-machine-learning-in-mysql","text":"Database is surely the best place for Machine Learning - because data is the main ingredient of it. And now you can build, train, test & query Machine Learning models using standard SQL queries within MySQL database! This doesn't require hardcore data science knowledge - the whole Machine Learning workflow is automated. This solution is called AI-Tables and is available in MySQL thanks to integration with an open-source predictive engine from MindsDB. AI-Tables look like normal database tables and return predictions upon being queried as if it is data that exists in the table. In plain SQL it looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > This video explains how it works: In this tutorial below, you will get step-by-step instructions on how to enable AI-Tables in MySQL database. Based on the churn prediction example, you see how to build, train and query Machine Learning models only by using SQL statements with MindsDB!","title":"How to enable Automated Machine Learning in MySQL"},{"location":"tutorials/mysql/#how-to-install-mysql","text":"If you don\u2019t have MySQL installed you can download the installers for various platforms from the official documentation .","title":"How to install MySQL?"},{"location":"tutorials/mysql/#example-dataset","text":"In this tutorial, we will use the Churn Modelling Data Set . If you have other datasets in your MySQL database please skip this section. This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer.","title":"Example dataset"},{"location":"tutorials/mysql/#import-dataset-to-mysql","text":"The first thing we need to do is to import the dataset in MySQL. Create a new table called bank_churn: -- test.bank_churn definition CREATE TABLE test . bank_churn ( CreditScore NUMERIC NULL , Geography varchar ( 100 ) NULL , Gender varchar ( 100 ) NULL , Age NUMERIC NULL , Tenure NUMERIC NULL , Balance NUMERIC NULL , NumOfProducts NUMERIC NULL , HasCrCard NUMERIC NULL , IsActiveMember NUMERIC NULL , EstimatedSalary NUMERIC NULL , Exited NUMERIC NULL ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ; Next, we need to import the data inside the table. There are a few options to do that: Using the Load Data statement: LOAD DATA LOCAL INFILE 'data.csv' INTO bank_churn FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\r\\n'; Using MySQL import: mysqlimport --local --fields-terminated-by=\",\" bank_churn data.csv Using pgAdmin, DBeaver or another SQL client just use the import from CSV file option from the navigation menu. Let\u2019s Select some data from bank_churn table to check that the data was successfully imported to MySQL: SELECT * FROM bank_churn LIMIT 1 ;","title":"Import dataset to MySQL"},{"location":"tutorials/mysql/#add-configuration","text":"As a prerequisite for using MySQL we need to enable the Federated Storage engine. Check out the official MySQL documentation to see how you can do that. The last step is to create the MindsDB\u2019s configuration file. MindsDB will try to use the default configuration options like host, port or username for MySQL. In case you want to extend them or change the default values you need to add a config.json file. Create a new file config.json and include the following information: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_mysql\" : { \"enabled\" : true , \"host\" : \"localhost\" , \"password\" : \"root\" , \"port\" : 3307 , \"type\" : \"mysql\" , \"user\" : \"root\" } }, \"storage_dir\" : \"storage/\" } The values provided in the configuration file are: api['http\u2019] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations[default_mysql] -- This key specifies the integration type in this case default_mysql. The required keys are: user(default root) - The MySQL user name. host(default localhost) - Connect to the MySQL server on the given host. password - The password of the MySQL account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. storage_dir -- The directory where mindsdb will store models and configuration files. Now, we have successfully set up all of the requirements for AI Tables in MySQL.","title":"Add Configuration"},{"location":"tutorials/mysql/#automl-with-ai-tables-in-mysql","text":"If you don't have MindsDB installed, check out our Installation guide and find an option that works for you. After that start the MindsDB server: python3 -m mindsdb --api=mysql --config=config.json The arguments sent to MindsDB are: --api - That tells MindsDB which API should be started (HTTP or MySQL). --config - The path to the configuration file that we have created. If everything works as expected you should see the following message: Upon successful setup, MindsDB should create a new database called mindsdb. In the mindsdb database, two new tables should be created called commands and predictors. The mindsdb.predictors table is the table where MindsDB will keep information about trained models.","title":"AutoML with AI Tables in MySQL"},{"location":"tutorials/mysql/#train-new-machine-learning-model","text":"Training the machine learning model using MindsDB is quite simple. It can be done by executing the INSERT query inside the mindsdb.predictors table. In our example, we want to predict if the bank's customer left the bank from the bank_churn table, so let\u2019s run the INSERT query as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'churn_model' , 'Exited' , 'SELECT * FROM test.bank_churn' ); This query will create a new model called 'churn_model', and a new table 'churn_model' inside mindsdb database. The required columns(parameters) added in the INSERT for training the predictor are: * name (string) - the name of the predictor. * predict (string) - the feature you want to predict, in this example it will be Exited. * select_data_query (string) - the SELECT query that will get the data from MySQL. To check that the training successfully finished we can SELECT from mindsdb.predictors table and get the status: SELECT * FROM mindsdb . predictors WHERE name = 'churn_model' ; The status complete means that training successfully finished. Now, let\u2019s query the model. The trained model behaves like an AI Table and can be queried as it is a standard database table. To get the prediction we need to execute a SELECT query and in the WHERE clause include the when_data as a JSON string that includes features values such as CreditScore, EstimatedSalary, Gender, Balance etc. SELECT * FROM mindsdb . churn_model WHERE when_data = '{\"CreditScore\": \"619\",\"Geography\": \"France\",\"Gender\": \"Female\", \"EstimatedSalary\": 100000, \"Balance\": 0.0, \"Age\":42, \"Tenure\": 2}' ; In a second we should get the prediction back from MindsDB. So, MindsDB thinks that the above customer closed the account in the bank (predicted_value 1) with around 98% confidence. Information in JSON format in the explain column: { \"predicted_value\" : \"1.0\" , \"confidence\" : 0.98 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"NumOfProducts\" ] } The important_missing_information shows the list of features that MindsDB things are quite important for better prediction, in this case, the \"NumOfProducts\". Congratulations, you have successfully trained and queried the Machine Learning Model only by using SQL Statements. Note that even if we used MySQL, you can still query the same model from the other databases too.","title":"Train new Machine Learning Model"},{"location":"tutorials/postgresql/","text":"AI-Tables in PostgreSQL - get neural-network-based predictions using simple SQL queries Anyone that has dealt with Machine Learning understands that data is a fundamental ingredient to it. Given that a great deal of the world\u2019s organized data already exists inside databases, doesn't it make sense to bring machine learning capabilities straight to the database itself? Bringing Machine Learning to those who know their data best can significantly augment the capacity to solve important problems. To do so, we have developed a concept called AI-Tables. What is AI Tables AI-Tables differ from normal tables in that they can generate predictions upon being queried and returning such predictions as if it was data that existed in the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > Now, in this tutorial, you will get step-by-step instructions on how to enable AI-Tables in your database and how to build, train and query a Machine Learning model only by using SQL statements! How to install PostgreSQL? If you don\u2019t have PostgreSQL installed you can download the installers for various platforms from the official documentation. Example dataset In this tutorial, we will use the Airline Passenger Satisfaction dataset . If you have other datasets in your PostgreSQL database please skip this section. This dataset contains airline passenger satisfaction survey data and we will try to predict passenger satisfaction based on the other factors in the data. Import dataset to PostgreSQL First, let's create a us_consumption table. -- public.airline_passenger_satisfaction definition CREATE TABLE public . airline_passenger_satisfaction ( id numeric NULL , gender varchar NULL , \"Customer Type\" varchar NULL , age numeric NULL , \"Type of Travel\" varchar NULL , \"Class\" varchar NULL , \"Flight Distance\" numeric NULL , \"Inflight wifi service\" numeric NULL , \"Departure/Arrival time convenient\" numeric NULL , \"Ease of Online booking\" numeric NULL , \"Gate location\" numeric NULL , \"Food and drink\" numeric NULL , \"Online boarding\" numeric NULL , \"Seat comfort\" numeric NULL , \"Inflight entertainment\" numeric NULL , \"On-board service\" numeric NULL , \"Leg room service\" numeric NULL , \"Baggage handling\" numeric NULL , \"Checkin service\" numeric NULL , \"Inflight service\" numeric NULL , cleanliness numeric NULL , \"Departure Delay in Minutes\" numeric NULL , \"Arrival Delay in Minutes\" numeric NULL , satisfaction varchar NULL ); After you create the table, you can use the \\copy command to import the data from the CSV file to PostgreSQL: \\copy usconsumption FROM '/path/to/csv/usconsumption.csv' DELIMITER ',' CSV Or, if you are using pgAdmin, DBeaver or another SQL client just use the import from CSV file option from the navigation menu. To check if the data was successfully imported execute SELECT query: SELECT * FROM airline_passenger_satisfaction LIMIT 10 ; Add Configuration We have the data inside PostgreSQL, so the next step is to install PostgreSQL foreign data wrapper for MySQL. Please check the EnterpriseDB documentation on how to do that. The last step is to create the MindsDB\u2019s configuration file. MindsDB will try to use the default configuration options like host, port, username for the PostgreSQL integration. In case you want to extend them or change the default values you need to add a config.json file. Create a new file config.json and include the following information: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"enabled\" : true , \"host\" : \"127.0.0.1\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"storage_dir\" : \"storage/\" } The values provided in the configuration file are: api['http\u2019] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type in this case default_postgres. The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. storage_dir -- The directory where mindsdb will store models and configuration. That\u2019s all for setting up the AI Tables in PostgreSQL. AutoML with AI Tables in PostgreSQL If you don't have MindsDB installed, check out our Installation guide and find an option that works for you. After that start the MindsDB server: python3 -m mindsdb --api=http,mysql --config=config.json The arguments sent to MindsDB are: --api - That tells MindsDB which API should be started (HTTP, MySQL or both). --config - The path to the configuration file that we have created. If everything works as expected you should see the following message: Upon successful setup, MindsDB should create a new schema called mindsdb. In the mindsdb schema, two new tables should be created called commands and predictors. The mindsdb.predictors table is the table where MindsDB will keep information about trained and in training models. Train new Machine Learning Model Training the machine learning model using MindsDB is quite simple. It can be done by executing the INSERT query inside the mindsdb.predictors table. In our example we want to predict the consumption from the us_consumption table, so let\u2019s run the INSERT query as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'passenger_satisfaction_model' , 'satisfaction' , 'SELECT * FROM airline_passenger_satisfaction' ); This query will create a new model called 'passenger_satisfaction_model', and a new table 'airline_passenger_satisfaction' inside mindsdb schema. The required columns(parameters) added in the INSERT for training the predictor are: name (string) - the name of the predictor. predict (string) - the feature you want to predict, in this example it will be satisfaction. select_data_query (string) - the SELECT query that will get the data from PostgreSQL. training_options (dictionary) - optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . To check that the training successfully finished we can SELECT from mindsdb.predictors table and get the status: SELECT * FROM mindsdb . predictors WHERE name = 'passenger_satisfaction_model' ; The status complete means that training successfully finished. Now, let\u2019s query the model. The trained model behaves like an AI Table and can be queried as it is a standard database table. To get the prediction we need to execute SELECT query and in the WHERE clause include the other features values as Customer Type, Type of Travel, Seat comfort etc. SELECT satisfaction AS predicted , satisfaction_confidence AS confidence FROM mindsdb . passenger_satisfaction_model WHERE \"Customer Type\" = 'Loyal Customer' AND \"Inflight wifi service\" = 5 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; In a second we should get the prediction back from MindsDB. So, MindsDB thinks that the value for consumption rate is around 0.87 with pretty much big confidence 97%. There is additional information that we can get back from MindsDB by selecting the explain column from the model as: SELECT satisfaction AS predicted , satisfaction_confidence AS confidence , satisfaction_explain AS info FROM mindsdb . passenger_satisfaction_model WHERE \"Customer Type\" = 'Loyal Customer' AND age = 52 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; Now, apart from the predicted and confidence values, MindsDB will return additional Information in JSON format in the explain column: { \"predicted_value\" : \"satisfied\" , \"confidence\" : 0.94 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"Flight Distance\" , \"Inflight wifi service\" , \"Online boarding\" , \"Baggage handling\" ], \"confidence_composition\" : { \"age\" : 0.935 }, \"extra_insights\" : { \"if_missing\" : [{ \"Customer Type\" : \"satisfied\" }, { \"Type of Travel\" : \"satisfied\" }, { \"Class\" : \"satisfied\" }] } } The confidence_interva l shows a possible range of values where consumption lies in. The important_missing_information shows the list of features that MindsDB things are quite important for better prediction, in this case, the \u201cFlight Distance\u201d, \"Inflight wifi service\", \"Online boarding\" or \"Baggage handling\". The extra_insights shows a list of rows that we have included in the WHERE clause and show the consumption value if some of those were missing. Congratulations, you have successfully trained and queried the Machine Learning Model only by using SQL Statements. Note that even if we used PostgreSQL, you can still query the same model from the other databases too.","title":"AI Tables in PostgreSQL"},{"location":"tutorials/postgresql/#ai-tables-in-postgresql-get-neural-network-based-predictions-using-simple-sql-queries","text":"Anyone that has dealt with Machine Learning understands that data is a fundamental ingredient to it. Given that a great deal of the world\u2019s organized data already exists inside databases, doesn't it make sense to bring machine learning capabilities straight to the database itself? Bringing Machine Learning to those who know their data best can significantly augment the capacity to solve important problems. To do so, we have developed a concept called AI-Tables.","title":"AI-Tables in PostgreSQL - get neural-network-based predictions using simple SQL queries"},{"location":"tutorials/postgresql/#what-is-ai-tables","text":"AI-Tables differ from normal tables in that they can generate predictions upon being queried and returning such predictions as if it was data that existed in the table. Simply put, an AI-Table allows you to use machine learning models as if they were normal database tables, in something that in plain SQL looks like this: SELECT < predicted_variable > FROM < ML_model > WHERE < conditions > Now, in this tutorial, you will get step-by-step instructions on how to enable AI-Tables in your database and how to build, train and query a Machine Learning model only by using SQL statements!","title":"What is AI Tables"},{"location":"tutorials/postgresql/#how-to-install-postgresql","text":"If you don\u2019t have PostgreSQL installed you can download the installers for various platforms from the official documentation.","title":"How to install PostgreSQL?"},{"location":"tutorials/postgresql/#example-dataset","text":"In this tutorial, we will use the Airline Passenger Satisfaction dataset . If you have other datasets in your PostgreSQL database please skip this section. This dataset contains airline passenger satisfaction survey data and we will try to predict passenger satisfaction based on the other factors in the data.","title":"Example dataset"},{"location":"tutorials/postgresql/#import-dataset-to-postgresql","text":"First, let's create a us_consumption table. -- public.airline_passenger_satisfaction definition CREATE TABLE public . airline_passenger_satisfaction ( id numeric NULL , gender varchar NULL , \"Customer Type\" varchar NULL , age numeric NULL , \"Type of Travel\" varchar NULL , \"Class\" varchar NULL , \"Flight Distance\" numeric NULL , \"Inflight wifi service\" numeric NULL , \"Departure/Arrival time convenient\" numeric NULL , \"Ease of Online booking\" numeric NULL , \"Gate location\" numeric NULL , \"Food and drink\" numeric NULL , \"Online boarding\" numeric NULL , \"Seat comfort\" numeric NULL , \"Inflight entertainment\" numeric NULL , \"On-board service\" numeric NULL , \"Leg room service\" numeric NULL , \"Baggage handling\" numeric NULL , \"Checkin service\" numeric NULL , \"Inflight service\" numeric NULL , cleanliness numeric NULL , \"Departure Delay in Minutes\" numeric NULL , \"Arrival Delay in Minutes\" numeric NULL , satisfaction varchar NULL ); After you create the table, you can use the \\copy command to import the data from the CSV file to PostgreSQL: \\copy usconsumption FROM '/path/to/csv/usconsumption.csv' DELIMITER ',' CSV Or, if you are using pgAdmin, DBeaver or another SQL client just use the import from CSV file option from the navigation menu. To check if the data was successfully imported execute SELECT query: SELECT * FROM airline_passenger_satisfaction LIMIT 10 ;","title":"Import dataset to PostgreSQL"},{"location":"tutorials/postgresql/#add-configuration","text":"We have the data inside PostgreSQL, so the next step is to install PostgreSQL foreign data wrapper for MySQL. Please check the EnterpriseDB documentation on how to do that. The last step is to create the MindsDB\u2019s configuration file. MindsDB will try to use the default configuration options like host, port, username for the PostgreSQL integration. In case you want to extend them or change the default values you need to add a config.json file. Create a new file config.json and include the following information: { \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" }, \"mysql\" : { \"host\" : \"127.0.0.1\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"root\" } }, \"config_version\" : \"1.3\" , \"debug\" : true , \"integrations\" : { \"default_postgres\" : { \"database\" : \"postgres\" , \"enabled\" : true , \"host\" : \"127.0.0.1\" , \"password\" : \"postgres\" , \"port\" : 5432 , \"type\" : \"postgres\" , \"user\" : \"postgres\" } }, \"storage_dir\" : \"storage/\" } The values provided in the configuration file are: api['http\u2019] -- This key is used for starting the MindsDB HTTP server by providing: host(default 127.0.0.1) - The mindsdb server address. port(default 47334) - The mindsdb server port. api['mysql'] -- This key is used for database integrations that work through MySQL protocol. The required keys are: user(default root). password(default empty). host(default 127.0.0.1). port(default 47335). integrations['default_postgres'] -- This key specifies the integration type in this case default_postgres. The required keys are: user(default postgres) - The Postgres user name. host(default 127.0.0.1) - Connect to the PostgreSQL server on the given host. password - The password of the Postgres account. type - Integration type(mariadb, postgresql, mysql, clickhouse, mongodb). port(default 5432) - The TCP/IP port number to use for the connection. storage_dir -- The directory where mindsdb will store models and configuration. That\u2019s all for setting up the AI Tables in PostgreSQL.","title":"Add Configuration"},{"location":"tutorials/postgresql/#automl-with-ai-tables-in-postgresql","text":"If you don't have MindsDB installed, check out our Installation guide and find an option that works for you. After that start the MindsDB server: python3 -m mindsdb --api=http,mysql --config=config.json The arguments sent to MindsDB are: --api - That tells MindsDB which API should be started (HTTP, MySQL or both). --config - The path to the configuration file that we have created. If everything works as expected you should see the following message: Upon successful setup, MindsDB should create a new schema called mindsdb. In the mindsdb schema, two new tables should be created called commands and predictors. The mindsdb.predictors table is the table where MindsDB will keep information about trained and in training models.","title":"AutoML with AI Tables in PostgreSQL"},{"location":"tutorials/postgresql/#train-new-machine-learning-model","text":"Training the machine learning model using MindsDB is quite simple. It can be done by executing the INSERT query inside the mindsdb.predictors table. In our example we want to predict the consumption from the us_consumption table, so let\u2019s run the INSERT query as: INSERT INTO mindsdb . predictors ( name , predict , select_data_query ) VALUES ( 'passenger_satisfaction_model' , 'satisfaction' , 'SELECT * FROM airline_passenger_satisfaction' ); This query will create a new model called 'passenger_satisfaction_model', and a new table 'airline_passenger_satisfaction' inside mindsdb schema. The required columns(parameters) added in the INSERT for training the predictor are: name (string) - the name of the predictor. predict (string) - the feature you want to predict, in this example it will be satisfaction. select_data_query (string) - the SELECT query that will get the data from PostgreSQL. training_options (dictionary) - optional value that contains additional training parameters. For a full list of the parameters check the PredictorInterface . To check that the training successfully finished we can SELECT from mindsdb.predictors table and get the status: SELECT * FROM mindsdb . predictors WHERE name = 'passenger_satisfaction_model' ; The status complete means that training successfully finished. Now, let\u2019s query the model. The trained model behaves like an AI Table and can be queried as it is a standard database table. To get the prediction we need to execute SELECT query and in the WHERE clause include the other features values as Customer Type, Type of Travel, Seat comfort etc. SELECT satisfaction AS predicted , satisfaction_confidence AS confidence FROM mindsdb . passenger_satisfaction_model WHERE \"Customer Type\" = 'Loyal Customer' AND \"Inflight wifi service\" = 5 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; In a second we should get the prediction back from MindsDB. So, MindsDB thinks that the value for consumption rate is around 0.87 with pretty much big confidence 97%. There is additional information that we can get back from MindsDB by selecting the explain column from the model as: SELECT satisfaction AS predicted , satisfaction_confidence AS confidence , satisfaction_explain AS info FROM mindsdb . passenger_satisfaction_model WHERE \"Customer Type\" = 'Loyal Customer' AND age = 52 AND \"Type of Travel\" = 'Business travel' AND \"Class\" = 'Eco' ; Now, apart from the predicted and confidence values, MindsDB will return additional Information in JSON format in the explain column: { \"predicted_value\" : \"satisfied\" , \"confidence\" : 0.94 , \"prediction_quality\" : \"very confident\" , \"important_missing_information\" : [ \"Flight Distance\" , \"Inflight wifi service\" , \"Online boarding\" , \"Baggage handling\" ], \"confidence_composition\" : { \"age\" : 0.935 }, \"extra_insights\" : { \"if_missing\" : [{ \"Customer Type\" : \"satisfied\" }, { \"Type of Travel\" : \"satisfied\" }, { \"Class\" : \"satisfied\" }] } } The confidence_interva l shows a possible range of values where consumption lies in. The important_missing_information shows the list of features that MindsDB things are quite important for better prediction, in this case, the \u201cFlight Distance\u201d, \"Inflight wifi service\", \"Online boarding\" or \"Baggage handling\". The extra_insights shows a list of rows that we have included in the WHERE clause and show the consumption value if some of those were missing. Congratulations, you have successfully trained and queried the Machine Learning Model only by using SQL Statements. Note that even if we used PostgreSQL, you can still query the same model from the other databases too.","title":"Train new Machine Learning Model"}]}